{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is there a Pet waiting?\n",
    "In this notebook I will use transfer learning to train a classifier\n",
    "which classifies if there is a pet present on a cctv camera.\n",
    "RESNET 18 will be use for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.025\n",
    "EPOCHS = 5\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for transfer learning\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "for parameters in model.parameters():\n",
    "    parameters.requires_grad = False\n",
    "\n",
    "last_layer_shape = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(last_layer_shape, 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model is correctly initialized\n",
    "# summary(model,input_size=(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transforms\n",
    "\n",
    "# general_transforms = tfs.Compose([\n",
    "#     tfs.Resize((224,224)),\n",
    "#     tfs.ToTensor()\n",
    "#     ])\n",
    "\n",
    "train_transforms = tfs.Compose([\n",
    "    tfs.Resize((224,224)),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    tfs.RandomPosterize(bits=2,p=0.25),\n",
    "    tfs.RandomAdjustSharpness(sharpness_factor=2,p=0.5),\n",
    "    tfs.RandomAutocontrast()\n",
    "])\n",
    "\n",
    "test_val_transforms = tfs.Compose([\n",
    "    tfs.Resize((224,224)),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall dataset size: 12521, train: 9390, validation: 1565, test: 1566\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and create dataloaders\n",
    "all_datasets = ImageFolder(\"./data\")\n",
    "dataset_len = len(all_datasets)\n",
    "# train, validation, test = random_split(all_datasets, [int(dataset_len*0.75),int(dataset_len*0.125),int(dataset_len*0.125+2)])\n",
    "train, validation, test = random_split(all_datasets, [int(dataset_len*0.75),int(dataset_len*0.125),int(dataset_len*0.125+1)])\n",
    "print(f\"Overall dataset size: {dataset_len}, train: {len(train)}, validation: {len(validation)}, test: {len(test)}\")\n",
    "\n",
    "train.dataset.transform = train_transforms\n",
    "validation.dataset.transform = test_val_transforms\n",
    "test.dataset.transform = test_val_transforms\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train,shuffle=True, batch_size=BATCH_SIZE)\n",
    "validation_loader = DataLoader(validation, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# data = next(iter(train_loader))[0]\n",
    "# data = make_grid(data)\n",
    "\n",
    "\n",
    "# plt.imshow(data.permute(1,2,0).numpy())\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr= LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training | EPOCH: 1 | Average Loss: 0.8663 | Current Mini-batch loss: 0.8663\n",
      "Training | EPOCH: 1 | Average Loss: 0.4332 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 1 | Average Loss: 1.0033 | Current Mini-batch loss: 1.5734\n",
      "Training | EPOCH: 1 | Average Loss: 0.7984 | Current Mini-batch loss: 0.5935\n",
      "Training | EPOCH: 1 | Average Loss: 0.5063 | Current Mini-batch loss: 0.2143\n",
      "Training | EPOCH: 1 | Average Loss: 0.4602 | Current Mini-batch loss: 0.4140\n",
      "Training | EPOCH: 1 | Average Loss: 0.5275 | Current Mini-batch loss: 0.5948\n",
      "Training | EPOCH: 1 | Average Loss: 0.5610 | Current Mini-batch loss: 0.5945\n",
      "Training | EPOCH: 1 | Average Loss: 0.3449 | Current Mini-batch loss: 0.1287\n",
      "Training | EPOCH: 1 | Average Loss: 0.2577 | Current Mini-batch loss: 0.1704\n",
      "Training | EPOCH: 1 | Average Loss: 0.3225 | Current Mini-batch loss: 0.3873\n",
      "Training | EPOCH: 1 | Average Loss: 0.8896 | Current Mini-batch loss: 1.4567\n",
      "Training | EPOCH: 1 | Average Loss: 0.4448 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 1 | Average Loss: 1.4323 | Current Mini-batch loss: 2.4198\n",
      "Training | EPOCH: 1 | Average Loss: 0.7241 | Current Mini-batch loss: 0.0158\n",
      "Training | EPOCH: 1 | Average Loss: 0.4723 | Current Mini-batch loss: 0.2206\n",
      "Training | EPOCH: 1 | Average Loss: 0.4074 | Current Mini-batch loss: 0.3426\n",
      "Training | EPOCH: 1 | Average Loss: 0.5058 | Current Mini-batch loss: 0.6041\n",
      "Training | EPOCH: 1 | Average Loss: 0.7616 | Current Mini-batch loss: 1.0174\n",
      "Training | EPOCH: 1 | Average Loss: 0.6499 | Current Mini-batch loss: 0.5381\n",
      "Training | EPOCH: 1 | Average Loss: 0.5049 | Current Mini-batch loss: 0.3600\n",
      "Training | EPOCH: 1 | Average Loss: 0.3012 | Current Mini-batch loss: 0.0975\n",
      "Training | EPOCH: 1 | Average Loss: 0.1971 | Current Mini-batch loss: 0.0930\n",
      "Training | EPOCH: 1 | Average Loss: 0.1901 | Current Mini-batch loss: 0.1831\n",
      "Training | EPOCH: 1 | Average Loss: 0.3773 | Current Mini-batch loss: 0.5645\n",
      "Training | EPOCH: 1 | Average Loss: 0.3844 | Current Mini-batch loss: 0.3916\n",
      "Training | EPOCH: 1 | Average Loss: 0.2185 | Current Mini-batch loss: 0.0525\n",
      "Training | EPOCH: 1 | Average Loss: 0.1520 | Current Mini-batch loss: 0.0855\n",
      "Training | EPOCH: 1 | Average Loss: 0.8301 | Current Mini-batch loss: 1.5082\n",
      "Training | EPOCH: 1 | Average Loss: 1.8414 | Current Mini-batch loss: 2.8526\n",
      "Training | EPOCH: 1 | Average Loss: 2.7122 | Current Mini-batch loss: 3.5830\n",
      "Training | EPOCH: 1 | Average Loss: 1.4527 | Current Mini-batch loss: 0.1931\n",
      "Training | EPOCH: 1 | Average Loss: 0.7308 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 1 | Average Loss: 0.3786 | Current Mini-batch loss: 0.0264\n",
      "Training | EPOCH: 1 | Average Loss: 0.2107 | Current Mini-batch loss: 0.0428\n",
      "Training | EPOCH: 1 | Average Loss: 0.4245 | Current Mini-batch loss: 0.6382\n",
      "Training | EPOCH: 1 | Average Loss: 0.5147 | Current Mini-batch loss: 0.6049\n",
      "Training | EPOCH: 1 | Average Loss: 0.2586 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 1 | Average Loss: 0.2622 | Current Mini-batch loss: 0.2659\n",
      "Training | EPOCH: 1 | Average Loss: 0.2693 | Current Mini-batch loss: 0.2764\n",
      "Training | EPOCH: 1 | Average Loss: 0.2156 | Current Mini-batch loss: 0.1619\n",
      "Training | EPOCH: 1 | Average Loss: 0.1516 | Current Mini-batch loss: 0.0876\n",
      "Training | EPOCH: 1 | Average Loss: 0.0908 | Current Mini-batch loss: 0.0300\n",
      "Training | EPOCH: 1 | Average Loss: 0.0577 | Current Mini-batch loss: 0.0246\n",
      "Training | EPOCH: 1 | Average Loss: 0.1129 | Current Mini-batch loss: 0.1680\n",
      "Training | EPOCH: 1 | Average Loss: 0.1039 | Current Mini-batch loss: 0.0950\n",
      "Training | EPOCH: 1 | Average Loss: 0.1572 | Current Mini-batch loss: 0.2104\n",
      "Training | EPOCH: 1 | Average Loss: 0.4215 | Current Mini-batch loss: 0.6859\n",
      "Training | EPOCH: 1 | Average Loss: 0.5041 | Current Mini-batch loss: 0.5866\n",
      "Training | EPOCH: 1 | Average Loss: 0.3135 | Current Mini-batch loss: 0.1228\n",
      "Training | EPOCH: 1 | Average Loss: 0.1975 | Current Mini-batch loss: 0.0816\n",
      "Training | EPOCH: 1 | Average Loss: 0.1706 | Current Mini-batch loss: 0.1437\n",
      "Training | EPOCH: 1 | Average Loss: 0.1110 | Current Mini-batch loss: 0.0514\n",
      "Training | EPOCH: 1 | Average Loss: 0.2116 | Current Mini-batch loss: 0.3122\n",
      "Training | EPOCH: 1 | Average Loss: 0.2897 | Current Mini-batch loss: 0.3678\n",
      "Training | EPOCH: 1 | Average Loss: 0.3543 | Current Mini-batch loss: 0.4190\n",
      "Training | EPOCH: 1 | Average Loss: 0.2892 | Current Mini-batch loss: 0.2242\n",
      "Training | EPOCH: 1 | Average Loss: 0.3967 | Current Mini-batch loss: 0.5041\n",
      "Training | EPOCH: 1 | Average Loss: 0.4435 | Current Mini-batch loss: 0.4903\n",
      "Training | EPOCH: 1 | Average Loss: 0.2281 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 1 | Average Loss: 0.2672 | Current Mini-batch loss: 0.3063\n",
      "Training | EPOCH: 1 | Average Loss: 0.4716 | Current Mini-batch loss: 0.6761\n",
      "Training | EPOCH: 1 | Average Loss: 0.3298 | Current Mini-batch loss: 0.1880\n",
      "Training | EPOCH: 1 | Average Loss: 0.2045 | Current Mini-batch loss: 0.0791\n",
      "Training | EPOCH: 1 | Average Loss: 0.1327 | Current Mini-batch loss: 0.0609\n",
      "Training | EPOCH: 1 | Average Loss: 0.1441 | Current Mini-batch loss: 0.1554\n",
      "Training | EPOCH: 1 | Average Loss: 0.0729 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 1.3115 | Current Mini-batch loss: 2.5501\n",
      "Training | EPOCH: 1 | Average Loss: 4.1446 | Current Mini-batch loss: 6.9777\n",
      "Training | EPOCH: 1 | Average Loss: 3.1931 | Current Mini-batch loss: 2.2415\n",
      "Training | EPOCH: 1 | Average Loss: 1.9213 | Current Mini-batch loss: 0.6494\n",
      "Training | EPOCH: 1 | Average Loss: 0.9673 | Current Mini-batch loss: 0.0133\n",
      "Training | EPOCH: 1 | Average Loss: 0.5498 | Current Mini-batch loss: 0.1323\n",
      "Training | EPOCH: 1 | Average Loss: 0.2966 | Current Mini-batch loss: 0.0433\n",
      "Training | EPOCH: 1 | Average Loss: 0.1762 | Current Mini-batch loss: 0.0558\n",
      "Training | EPOCH: 1 | Average Loss: 0.0999 | Current Mini-batch loss: 0.0236\n",
      "Training | EPOCH: 1 | Average Loss: 0.0543 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 1 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 1 | Average Loss: 0.0214 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 1 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 1 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 1 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0269\n",
      "Training | EPOCH: 1 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 1 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 1 | Average Loss: 0.2650 | Current Mini-batch loss: 0.5173\n",
      "Training | EPOCH: 1 | Average Loss: 0.5597 | Current Mini-batch loss: 0.8543\n",
      "Training | EPOCH: 1 | Average Loss: 0.7911 | Current Mini-batch loss: 1.0226\n",
      "Training | EPOCH: 1 | Average Loss: 0.6019 | Current Mini-batch loss: 0.4128\n",
      "Training | EPOCH: 1 | Average Loss: 0.4501 | Current Mini-batch loss: 0.2982\n",
      "Training | EPOCH: 1 | Average Loss: 0.2263 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.1222 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 1 | Average Loss: 0.1258 | Current Mini-batch loss: 0.1293\n",
      "Training | EPOCH: 1 | Average Loss: 0.1304 | Current Mini-batch loss: 0.1351\n",
      "Training | EPOCH: 1 | Average Loss: 0.0704 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 1 | Average Loss: 0.0396 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 1 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 1 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 1 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0380\n",
      "Training | EPOCH: 1 | Average Loss: 0.1018 | Current Mini-batch loss: 0.1777\n",
      "Training | EPOCH: 1 | Average Loss: 0.0582 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 1 | Average Loss: 0.0311 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 1 | Average Loss: 0.4212 | Current Mini-batch loss: 0.8114\n",
      "Training | EPOCH: 1 | Average Loss: 0.3495 | Current Mini-batch loss: 0.2777\n",
      "Training | EPOCH: 1 | Average Loss: 0.1818 | Current Mini-batch loss: 0.0141\n",
      "Training | EPOCH: 1 | Average Loss: 0.2330 | Current Mini-batch loss: 0.2842\n",
      "Training | EPOCH: 1 | Average Loss: 0.1544 | Current Mini-batch loss: 0.0758\n",
      "Training | EPOCH: 1 | Average Loss: 0.1679 | Current Mini-batch loss: 0.1813\n",
      "Training | EPOCH: 1 | Average Loss: 0.0895 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 1 | Average Loss: 0.2275 | Current Mini-batch loss: 0.3655\n",
      "Training | EPOCH: 1 | Average Loss: 0.5099 | Current Mini-batch loss: 0.7923\n",
      "Training | EPOCH: 1 | Average Loss: 0.2654 | Current Mini-batch loss: 0.0208\n",
      "Training | EPOCH: 1 | Average Loss: 0.5383 | Current Mini-batch loss: 0.8113\n",
      "Training | EPOCH: 1 | Average Loss: 0.3385 | Current Mini-batch loss: 0.1387\n",
      "Training | EPOCH: 1 | Average Loss: 0.1955 | Current Mini-batch loss: 0.0525\n",
      "Training | EPOCH: 1 | Average Loss: 0.2718 | Current Mini-batch loss: 0.3480\n",
      "Training | EPOCH: 1 | Average Loss: 0.3990 | Current Mini-batch loss: 0.5263\n",
      "Training | EPOCH: 1 | Average Loss: 0.2429 | Current Mini-batch loss: 0.0867\n",
      "Training | EPOCH: 1 | Average Loss: 0.1232 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0894 | Current Mini-batch loss: 0.0555\n",
      "Training | EPOCH: 1 | Average Loss: 0.2564 | Current Mini-batch loss: 0.4234\n",
      "Training | EPOCH: 1 | Average Loss: 0.1298 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0575\n",
      "Training | EPOCH: 1 | Average Loss: 0.0818 | Current Mini-batch loss: 0.0699\n",
      "Training | EPOCH: 1 | Average Loss: 0.0423 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0431 | Current Mini-batch loss: 0.0439\n",
      "Training | EPOCH: 1 | Average Loss: 0.2803 | Current Mini-batch loss: 0.5175\n",
      "Training | EPOCH: 1 | Average Loss: 0.1442 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 1 | Average Loss: 0.1694 | Current Mini-batch loss: 0.1946\n",
      "Training | EPOCH: 1 | Average Loss: 0.0877 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 1 | Average Loss: 0.0715 | Current Mini-batch loss: 0.0553\n",
      "Training | EPOCH: 1 | Average Loss: 0.0461 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 1 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 1 | Average Loss: 0.1537 | Current Mini-batch loss: 0.2834\n",
      "Training | EPOCH: 1 | Average Loss: 0.3194 | Current Mini-batch loss: 0.4851\n",
      "Training | EPOCH: 1 | Average Loss: 0.4218 | Current Mini-batch loss: 0.5241\n",
      "Training | EPOCH: 1 | Average Loss: 0.5779 | Current Mini-batch loss: 0.7341\n",
      "Training | EPOCH: 1 | Average Loss: 0.2961 | Current Mini-batch loss: 0.0142\n",
      "Training | EPOCH: 1 | Average Loss: 0.1547 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 1 | Average Loss: 0.1166 | Current Mini-batch loss: 0.0785\n",
      "Training | EPOCH: 1 | Average Loss: 0.0700 | Current Mini-batch loss: 0.0235\n",
      "Training | EPOCH: 1 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 1 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0153\n",
      "Training | EPOCH: 1 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0307\n",
      "Training | EPOCH: 1 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 1 | Average Loss: 0.2280 | Current Mini-batch loss: 0.4398\n",
      "Training | EPOCH: 1 | Average Loss: 0.1808 | Current Mini-batch loss: 0.1336\n",
      "Training | EPOCH: 1 | Average Loss: 0.1374 | Current Mini-batch loss: 0.0939\n",
      "Training | EPOCH: 1 | Average Loss: 0.0722 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 1 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0094\n",
      "Training | EPOCH: 1 | Average Loss: 0.3909 | Current Mini-batch loss: 0.7411\n",
      "Training | EPOCH: 1 | Average Loss: 0.1965 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 1 | Average Loss: 0.0996 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.1138 | Current Mini-batch loss: 0.1279\n",
      "Training | EPOCH: 1 | Average Loss: 0.0581 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0619 | Current Mini-batch loss: 0.0658\n",
      "Training | EPOCH: 1 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.1435 | Current Mini-batch loss: 0.2546\n",
      "Training | EPOCH: 1 | Average Loss: 1.7740 | Current Mini-batch loss: 3.4046\n",
      "Training | EPOCH: 1 | Average Loss: 0.9251 | Current Mini-batch loss: 0.0763\n",
      "Training | EPOCH: 1 | Average Loss: 1.2331 | Current Mini-batch loss: 1.5410\n",
      "Training | EPOCH: 1 | Average Loss: 0.6187 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.3095 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.1799 | Current Mini-batch loss: 0.0502\n",
      "Training | EPOCH: 1 | Average Loss: 0.0941 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 1 | Average Loss: 0.2580 | Current Mini-batch loss: 0.4218\n",
      "Training | EPOCH: 1 | Average Loss: 0.1834 | Current Mini-batch loss: 0.1089\n",
      "Training | EPOCH: 1 | Average Loss: 0.1449 | Current Mini-batch loss: 0.1063\n",
      "Training | EPOCH: 1 | Average Loss: 0.0985 | Current Mini-batch loss: 0.0522\n",
      "Training | EPOCH: 1 | Average Loss: 0.0587 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 1 | Average Loss: 0.1234 | Current Mini-batch loss: 0.1882\n",
      "Training | EPOCH: 1 | Average Loss: 0.3783 | Current Mini-batch loss: 0.6331\n",
      "Training | EPOCH: 1 | Average Loss: 0.3090 | Current Mini-batch loss: 0.2397\n",
      "Training | EPOCH: 1 | Average Loss: 0.1572 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 1 | Average Loss: 0.0828 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 1 | Average Loss: 0.0494 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 1 | Average Loss: 0.0932 | Current Mini-batch loss: 0.1370\n",
      "Training | EPOCH: 1 | Average Loss: 0.0877 | Current Mini-batch loss: 0.0822\n",
      "Training | EPOCH: 1 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.1720 | Current Mini-batch loss: 0.2995\n",
      "Training | EPOCH: 1 | Average Loss: 0.2278 | Current Mini-batch loss: 0.2836\n",
      "Training | EPOCH: 1 | Average Loss: 0.6382 | Current Mini-batch loss: 1.0486\n",
      "Training | EPOCH: 1 | Average Loss: 0.4360 | Current Mini-batch loss: 0.2337\n",
      "Training | EPOCH: 1 | Average Loss: 0.2183 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.1193 | Current Mini-batch loss: 0.0203\n",
      "Training | EPOCH: 1 | Average Loss: 0.0648 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 1 | Average Loss: 0.0355 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 1 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0186\n",
      "Training | EPOCH: 1 | Average Loss: 0.0664 | Current Mini-batch loss: 0.1059\n",
      "Training | EPOCH: 1 | Average Loss: 0.0628 | Current Mini-batch loss: 0.0591\n",
      "Training | EPOCH: 1 | Average Loss: 0.0388 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 1 | Average Loss: 0.1652 | Current Mini-batch loss: 0.2915\n",
      "Training | EPOCH: 1 | Average Loss: 0.6798 | Current Mini-batch loss: 1.1943\n",
      "Training | EPOCH: 1 | Average Loss: 0.6562 | Current Mini-batch loss: 0.6326\n",
      "Training | EPOCH: 1 | Average Loss: 0.3777 | Current Mini-batch loss: 0.0993\n",
      "Training | EPOCH: 1 | Average Loss: 0.4677 | Current Mini-batch loss: 0.5578\n",
      "Training | EPOCH: 1 | Average Loss: 0.3032 | Current Mini-batch loss: 0.1387\n",
      "Training | EPOCH: 1 | Average Loss: 0.1522 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0821 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 1 | Average Loss: 0.0433 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0692 | Current Mini-batch loss: 0.0951\n",
      "Training | EPOCH: 1 | Average Loss: 0.0926 | Current Mini-batch loss: 0.1160\n",
      "Training | EPOCH: 1 | Average Loss: 0.0479 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 1 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0115\n",
      "Training | EPOCH: 1 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0255\n",
      "Training | EPOCH: 1 | Average Loss: 0.0367 | Current Mini-batch loss: 0.0505\n",
      "Training | EPOCH: 1 | Average Loss: 0.4886 | Current Mini-batch loss: 0.9404\n",
      "Training | EPOCH: 1 | Average Loss: 0.2779 | Current Mini-batch loss: 0.0672\n",
      "Training | EPOCH: 1 | Average Loss: 0.1507 | Current Mini-batch loss: 0.0236\n",
      "Training | EPOCH: 1 | Average Loss: 0.1589 | Current Mini-batch loss: 0.1671\n",
      "Training | EPOCH: 1 | Average Loss: 0.0861 | Current Mini-batch loss: 0.0133\n",
      "Training | EPOCH: 1 | Average Loss: 0.0943 | Current Mini-batch loss: 0.1026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0476 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0833 | Current Mini-batch loss: 0.1191\n",
      "Training | EPOCH: 1 | Average Loss: 0.0428 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.1907 | Current Mini-batch loss: 0.3386\n",
      "Training | EPOCH: 1 | Average Loss: 0.1259 | Current Mini-batch loss: 0.0611\n",
      "Training | EPOCH: 1 | Average Loss: 0.0960 | Current Mini-batch loss: 0.0660\n",
      "Training | EPOCH: 1 | Average Loss: 0.0492 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 1 | Average Loss: 0.0172 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 1 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0616\n",
      "Training | EPOCH: 1 | Average Loss: 0.1375 | Current Mini-batch loss: 0.2356\n",
      "Training | EPOCH: 1 | Average Loss: 0.0952 | Current Mini-batch loss: 0.0529\n",
      "Training | EPOCH: 1 | Average Loss: 0.1123 | Current Mini-batch loss: 0.1294\n",
      "Training | EPOCH: 1 | Average Loss: 0.0603 | Current Mini-batch loss: 0.0083\n",
      "Training | EPOCH: 1 | Average Loss: 0.0404 | Current Mini-batch loss: 0.0205\n",
      "Training | EPOCH: 1 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 1 | Average Loss: 0.1460 | Current Mini-batch loss: 0.2657\n",
      "Training | EPOCH: 1 | Average Loss: 0.0851 | Current Mini-batch loss: 0.0243\n",
      "Training | EPOCH: 1 | Average Loss: 0.0670 | Current Mini-batch loss: 0.0489\n",
      "Training | EPOCH: 1 | Average Loss: 0.1655 | Current Mini-batch loss: 0.2640\n",
      "Training | EPOCH: 1 | Average Loss: 0.1054 | Current Mini-batch loss: 0.0452\n",
      "Training | EPOCH: 1 | Average Loss: 0.0630 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 1 | Average Loss: 0.0373 | Current Mini-batch loss: 0.0117\n",
      "Training | EPOCH: 1 | Average Loss: 0.0391 | Current Mini-batch loss: 0.0409\n",
      "Training | EPOCH: 1 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0229\n",
      "Training | EPOCH: 1 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 1 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0264\n",
      "Training | EPOCH: 1 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 1 | Average Loss: 0.0611 | Current Mini-batch loss: 0.1025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0391 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 1 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0115\n",
      "Training | EPOCH: 1 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0519\n",
      "Training | EPOCH: 1 | Average Loss: 0.0453 | Current Mini-batch loss: 0.0582\n",
      "Training | EPOCH: 1 | Average Loss: 0.0301 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 1 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0229\n",
      "Training | EPOCH: 1 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 1 | Average Loss: 0.1600 | Current Mini-batch loss: 0.3041\n",
      "Training | EPOCH: 1 | Average Loss: 0.1395 | Current Mini-batch loss: 0.1190\n",
      "Training | EPOCH: 1 | Average Loss: 0.0781 | Current Mini-batch loss: 0.0166\n",
      "Training | EPOCH: 1 | Average Loss: 0.0494 | Current Mini-batch loss: 0.0207\n",
      "Training | EPOCH: 1 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0188 | Current Mini-batch loss: 0.0125\n",
      "Training | EPOCH: 1 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 1 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 1 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0414\n",
      "Training | EPOCH: 1 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 1 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 1 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.1917 | Current Mini-batch loss: 0.3721\n",
      "Training | EPOCH: 1 | Average Loss: 0.1156 | Current Mini-batch loss: 0.0396\n",
      "Training | EPOCH: 1 | Average Loss: 0.2221 | Current Mini-batch loss: 0.3285\n",
      "Training | EPOCH: 1 | Average Loss: 0.3394 | Current Mini-batch loss: 0.4567\n",
      "Training | EPOCH: 1 | Average Loss: 0.1763 | Current Mini-batch loss: 0.0131\n",
      "Training | EPOCH: 1 | Average Loss: 0.0914 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 1 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 1 | Average Loss: 0.0726 | Current Mini-batch loss: 0.0967\n",
      "Training | EPOCH: 1 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0573\n",
      "Training | EPOCH: 1 | Average Loss: 0.1121 | Current Mini-batch loss: 0.1592\n",
      "Training | EPOCH: 1 | Average Loss: 0.0905 | Current Mini-batch loss: 0.0690\n",
      "Training | EPOCH: 1 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0211\n",
      "Training | EPOCH: 1 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0198\n",
      "Training | EPOCH: 1 | Average Loss: 0.0320 | Current Mini-batch loss: 0.0427\n",
      "Training | EPOCH: 1 | Average Loss: 0.1194 | Current Mini-batch loss: 0.2069\n",
      "Training | EPOCH: 1 | Average Loss: 0.1442 | Current Mini-batch loss: 0.1689\n",
      "Training | EPOCH: 1 | Average Loss: 0.1052 | Current Mini-batch loss: 0.0662\n",
      "Training | EPOCH: 1 | Average Loss: 0.0637 | Current Mini-batch loss: 0.0223\n",
      "Training | EPOCH: 1 | Average Loss: 0.0440 | Current Mini-batch loss: 0.0243\n",
      "Training | EPOCH: 1 | Average Loss: 0.0415 | Current Mini-batch loss: 0.0391\n",
      "Training | EPOCH: 1 | Average Loss: 0.1126 | Current Mini-batch loss: 0.1836\n",
      "Training | EPOCH: 1 | Average Loss: 0.0608 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 1 | Average Loss: 0.3137 | Current Mini-batch loss: 0.5667\n",
      "Training | EPOCH: 1 | Average Loss: 0.1968 | Current Mini-batch loss: 0.0798\n",
      "Training | EPOCH: 1 | Average Loss: 0.1096 | Current Mini-batch loss: 0.0225\n",
      "Training | EPOCH: 1 | Average Loss: 0.0561 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 1 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 1 | Average Loss: 0.2260 | Current Mini-batch loss: 0.4408\n",
      "Training | EPOCH: 1 | Average Loss: 0.1313 | Current Mini-batch loss: 0.0367\n",
      "Training | EPOCH: 1 | Average Loss: 0.1030 | Current Mini-batch loss: 0.0746\n",
      "Training | EPOCH: 1 | Average Loss: 0.1695 | Current Mini-batch loss: 0.2359\n",
      "Training | EPOCH: 1 | Average Loss: 0.0851 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 1 | Average Loss: 0.0442 | Current Mini-batch loss: 0.0448\n",
      "Training | EPOCH: 1 | Average Loss: 0.2344 | Current Mini-batch loss: 0.4247\n",
      "Training | EPOCH: 1 | Average Loss: 0.1312 | Current Mini-batch loss: 0.0280\n",
      "Training | EPOCH: 1 | Average Loss: 0.0732 | Current Mini-batch loss: 0.0152\n",
      "Training | EPOCH: 1 | Average Loss: 0.5960 | Current Mini-batch loss: 1.1188\n",
      "Training | EPOCH: 1 | Average Loss: 0.3100 | Current Mini-batch loss: 0.0240\n",
      "Training | EPOCH: 1 | Average Loss: 0.1814 | Current Mini-batch loss: 0.0527\n",
      "Training | EPOCH: 1 | Average Loss: 0.7843 | Current Mini-batch loss: 1.3871\n",
      "Training | EPOCH: 1 | Average Loss: 0.6886 | Current Mini-batch loss: 0.5929\n",
      "Training | EPOCH: 1 | Average Loss: 0.5154 | Current Mini-batch loss: 0.3422\n",
      "Training | EPOCH: 1 | Average Loss: 0.4055 | Current Mini-batch loss: 0.2956\n",
      "Training | EPOCH: 1 | Average Loss: 0.5065 | Current Mini-batch loss: 0.6076\n",
      "Training | EPOCH: 1 | Average Loss: 0.2533 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 1 | Average Loss: 0.2890 | Current Mini-batch loss: 0.3246\n",
      "Training | EPOCH: 1 | Average Loss: 0.1821 | Current Mini-batch loss: 0.0752\n",
      "Training | EPOCH: 1 | Average Loss: 0.1087 | Current Mini-batch loss: 0.0354\n",
      "Training | EPOCH: 1 | Average Loss: 0.1935 | Current Mini-batch loss: 0.2783\n",
      "Training | EPOCH: 1 | Average Loss: 0.1801 | Current Mini-batch loss: 0.1666\n",
      "Training | EPOCH: 1 | Average Loss: 0.0910 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0946 | Current Mini-batch loss: 0.0982\n",
      "Training | EPOCH: 1 | Average Loss: 0.1002 | Current Mini-batch loss: 0.1059\n",
      "Training | EPOCH: 1 | Average Loss: 0.0607 | Current Mini-batch loss: 0.0211\n",
      "Training | EPOCH: 1 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0889 | Current Mini-batch loss: 0.1452\n",
      "Training | EPOCH: 1 | Average Loss: 0.0583 | Current Mini-batch loss: 0.0277\n",
      "Training | EPOCH: 1 | Average Loss: 0.0309 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 1 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 1 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0954 | Current Mini-batch loss: 0.1877\n",
      "Training | EPOCH: 1 | Average Loss: 0.0653 | Current Mini-batch loss: 0.0351\n",
      "Training | EPOCH: 1 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0190 | Current Mini-batch loss: 0.0277\n",
      "Training | EPOCH: 1 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.0423 | Current Mini-batch loss: 0.0740\n",
      "Training | EPOCH: 1 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 1 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 1 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 1 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0339\n",
      "Training | EPOCH: 1 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 1 | Average Loss: 0.0188 | Current Mini-batch loss: 0.0268\n",
      "Training | EPOCH: 1 | Average Loss: 0.0488 | Current Mini-batch loss: 0.0788\n",
      "Training | EPOCH: 1 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0164\n",
      "Training | EPOCH: 1 | Average Loss: 0.2460 | Current Mini-batch loss: 0.4594\n",
      "Training | EPOCH: 1 | Average Loss: 0.1901 | Current Mini-batch loss: 0.1342\n",
      "Training | EPOCH: 1 | Average Loss: 0.1821 | Current Mini-batch loss: 0.1741\n",
      "Training | EPOCH: 1 | Average Loss: 0.0913 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 1 | Average Loss: 0.0802 | Current Mini-batch loss: 0.1105\n",
      "Training | EPOCH: 1 | Average Loss: 0.0533 | Current Mini-batch loss: 0.0263\n",
      "Training | EPOCH: 1 | Average Loss: 0.0513 | Current Mini-batch loss: 0.0494\n",
      "Training | EPOCH: 1 | Average Loss: 0.3700 | Current Mini-batch loss: 0.6886\n",
      "Training | EPOCH: 1 | Average Loss: 0.1855 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0931 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0635 | Current Mini-batch loss: 0.0338\n",
      "Training | EPOCH: 1 | Average Loss: 0.0328 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 1 | Average Loss: 0.2591 | Current Mini-batch loss: 0.5006\n",
      "Training | EPOCH: 1 | Average Loss: 0.2722 | Current Mini-batch loss: 0.2854\n",
      "Training | EPOCH: 1 | Average Loss: 0.1453 | Current Mini-batch loss: 0.0183\n",
      "Training | EPOCH: 1 | Average Loss: 0.2560 | Current Mini-batch loss: 0.3666\n",
      "Training | EPOCH: 1 | Average Loss: 0.1581 | Current Mini-batch loss: 0.0603\n",
      "Training | EPOCH: 1 | Average Loss: 0.0811 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 1 | Average Loss: 0.0558 | Current Mini-batch loss: 0.0305\n",
      "Training | EPOCH: 1 | Average Loss: 0.0347 | Current Mini-batch loss: 0.0135\n",
      "Training | EPOCH: 1 | Average Loss: 0.4478 | Current Mini-batch loss: 0.8609\n",
      "Training | EPOCH: 1 | Average Loss: 0.3875 | Current Mini-batch loss: 0.3273\n",
      "Training | EPOCH: 1 | Average Loss: 0.1952 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.1012 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 1 | Average Loss: 0.0529 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0278 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 1 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0116\n",
      "Training | EPOCH: 1 | Average Loss: 0.0542 | Current Mini-batch loss: 0.0985\n",
      "Training | EPOCH: 1 | Average Loss: 0.0589 | Current Mini-batch loss: 0.0635\n",
      "Training | EPOCH: 1 | Average Loss: 0.0357 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 1 | Average Loss: 0.2556 | Current Mini-batch loss: 0.4754\n",
      "Training | EPOCH: 1 | Average Loss: 0.1279 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0734 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 1 | Average Loss: 0.2479 | Current Mini-batch loss: 0.4225\n",
      "Training | EPOCH: 1 | Average Loss: 0.1244 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0636 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0398 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 1 | Average Loss: 0.2823 | Current Mini-batch loss: 0.5247\n",
      "Training | EPOCH: 1 | Average Loss: 1.6800 | Current Mini-batch loss: 3.0777\n",
      "Training | EPOCH: 1 | Average Loss: 1.0996 | Current Mini-batch loss: 0.5192\n",
      "Training | EPOCH: 1 | Average Loss: 1.3759 | Current Mini-batch loss: 1.6522\n",
      "Training | EPOCH: 1 | Average Loss: 0.6880 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.3442 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.1727 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0886 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 1 | Average Loss: 0.0602 | Current Mini-batch loss: 0.0319\n",
      "Training | EPOCH: 1 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 1 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 1 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0072\n",
      "Training | EPOCH: 1 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 1 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 1 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 1 | Average Loss: 0.7930 | Current Mini-batch loss: 1.5768\n",
      "Training | EPOCH: 1 | Average Loss: 0.5140 | Current Mini-batch loss: 0.2349\n",
      "Training | EPOCH: 1 | Average Loss: 0.5729 | Current Mini-batch loss: 0.6318\n",
      "Training | EPOCH: 1 | Average Loss: 0.2865 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.3455 | Current Mini-batch loss: 0.4044\n",
      "Training | EPOCH: 1 | Average Loss: 0.2000 | Current Mini-batch loss: 0.0545\n",
      "Training | EPOCH: 1 | Average Loss: 0.1005 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0563 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 1 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0177\n",
      "Training | EPOCH: 1 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0097\n",
      "Training | EPOCH: 1 | Average Loss: 0.4143 | Current Mini-batch loss: 0.8122\n",
      "Training | EPOCH: 1 | Average Loss: 0.2098 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 1 | Average Loss: 0.1068 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 1 | Average Loss: 0.0881 | Current Mini-batch loss: 0.0693\n",
      "Training | EPOCH: 1 | Average Loss: 0.0540 | Current Mini-batch loss: 0.0198\n",
      "Training | EPOCH: 1 | Average Loss: 0.0989 | Current Mini-batch loss: 0.1439\n",
      "Training | EPOCH: 1 | Average Loss: 0.0508 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.1710 | Current Mini-batch loss: 0.2912\n",
      "Training | EPOCH: 1 | Average Loss: 0.0871 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 1 | Average Loss: 0.2547 | Current Mini-batch loss: 0.4224\n",
      "Training | EPOCH: 1 | Average Loss: 0.2507 | Current Mini-batch loss: 0.2466\n",
      "Training | EPOCH: 1 | Average Loss: 0.1257 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0689 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 1 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 1 | Average Loss: 0.2634 | Current Mini-batch loss: 0.4887\n",
      "Training | EPOCH: 1 | Average Loss: 0.1319 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.1947 | Current Mini-batch loss: 0.2576\n",
      "Training | EPOCH: 1 | Average Loss: 0.0977 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.1414 | Current Mini-batch loss: 0.1851\n",
      "Training | EPOCH: 1 | Average Loss: 0.0739 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 1 | Average Loss: 0.0958 | Current Mini-batch loss: 0.1177\n",
      "Training | EPOCH: 1 | Average Loss: 0.0486 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0117\n",
      "Training | EPOCH: 1 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 1 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0462\n",
      "Training | EPOCH: 1 | Average Loss: 0.0756 | Current Mini-batch loss: 0.1234\n",
      "Training | EPOCH: 1 | Average Loss: 0.2004 | Current Mini-batch loss: 0.3253\n",
      "Training | EPOCH: 1 | Average Loss: 0.1011 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0584 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 1 | Average Loss: 0.1151 | Current Mini-batch loss: 0.1718\n",
      "Training | EPOCH: 1 | Average Loss: 0.0856 | Current Mini-batch loss: 0.0561\n",
      "Training | EPOCH: 1 | Average Loss: 0.0440 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.2587 | Current Mini-batch loss: 0.4733\n",
      "Training | EPOCH: 1 | Average Loss: 0.2116 | Current Mini-batch loss: 0.1645\n",
      "Training | EPOCH: 1 | Average Loss: 0.1081 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0596 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 1 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 1 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 1 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.1266 | Current Mini-batch loss: 0.2459\n",
      "Training | EPOCH: 1 | Average Loss: 0.0665 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 1 | Average Loss: 0.3577 | Current Mini-batch loss: 0.6488\n",
      "Training | EPOCH: 1 | Average Loss: 0.1789 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0912 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0535 | Current Mini-batch loss: 0.0158\n",
      "Training | EPOCH: 1 | Average Loss: 0.1055 | Current Mini-batch loss: 0.1575\n",
      "Training | EPOCH: 1 | Average Loss: 0.0551 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 1 | Average Loss: 0.0404 | Current Mini-batch loss: 0.0258\n",
      "Training | EPOCH: 1 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0187\n",
      "Training | EPOCH: 1 | Average Loss: 0.0537 | Current Mini-batch loss: 0.0779\n",
      "Training | EPOCH: 1 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 1 | Average Loss: 0.1609 | Current Mini-batch loss: 0.2915\n",
      "Training | EPOCH: 1 | Average Loss: 0.0866 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 1 | Average Loss: 0.2110 | Current Mini-batch loss: 0.3354\n",
      "Training | EPOCH: 1 | Average Loss: 0.1056 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0593 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 1 | Average Loss: 0.1742 | Current Mini-batch loss: 0.2892\n",
      "Training | EPOCH: 1 | Average Loss: 0.2507 | Current Mini-batch loss: 0.3273\n",
      "Training | EPOCH: 1 | Average Loss: 0.1300 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 1 | Average Loss: 0.3057 | Current Mini-batch loss: 0.4814\n",
      "Training | EPOCH: 1 | Average Loss: 0.1601 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 1 | Average Loss: 0.0823 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 1 | Average Loss: 0.0576 | Current Mini-batch loss: 0.0329\n",
      "Training | EPOCH: 1 | Average Loss: 0.0545 | Current Mini-batch loss: 0.0513\n",
      "Training | EPOCH: 1 | Average Loss: 0.3987 | Current Mini-batch loss: 0.7429\n",
      "Training | EPOCH: 1 | Average Loss: 0.2150 | Current Mini-batch loss: 0.0312\n",
      "Training | EPOCH: 1 | Average Loss: 0.1354 | Current Mini-batch loss: 0.0559\n",
      "Training | EPOCH: 1 | Average Loss: 0.8611 | Current Mini-batch loss: 1.5867\n",
      "Training | EPOCH: 1 | Average Loss: 0.8708 | Current Mini-batch loss: 0.8805\n",
      "Training | EPOCH: 1 | Average Loss: 0.4483 | Current Mini-batch loss: 0.0258\n",
      "Training | EPOCH: 1 | Average Loss: 0.5886 | Current Mini-batch loss: 0.7290\n",
      "Training | EPOCH: 1 | Average Loss: 0.3212 | Current Mini-batch loss: 0.0539\n",
      "Training | EPOCH: 1 | Average Loss: 0.3595 | Current Mini-batch loss: 0.3978\n",
      "Training | EPOCH: 1 | Average Loss: 0.1798 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.4784 | Current Mini-batch loss: 0.7771\n",
      "Training | EPOCH: 1 | Average Loss: 0.3020 | Current Mini-batch loss: 0.1256\n",
      "Training | EPOCH: 1 | Average Loss: 0.1585 | Current Mini-batch loss: 0.0150\n",
      "Training | EPOCH: 1 | Average Loss: 0.0808 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 1 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 1 | Average Loss: 0.8072 | Current Mini-batch loss: 1.5907\n",
      "Training | EPOCH: 1 | Average Loss: 0.5497 | Current Mini-batch loss: 0.2923\n",
      "Training | EPOCH: 1 | Average Loss: 0.2871 | Current Mini-batch loss: 0.0245\n",
      "Training | EPOCH: 1 | Average Loss: 0.1449 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0908 | Current Mini-batch loss: 0.0367\n",
      "Training | EPOCH: 1 | Average Loss: 0.1148 | Current Mini-batch loss: 0.1388\n",
      "Training | EPOCH: 1 | Average Loss: 0.0627 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 1 | Average Loss: 0.0707 | Current Mini-batch loss: 0.0788\n",
      "Training | EPOCH: 1 | Average Loss: 0.1046 | Current Mini-batch loss: 0.1384\n",
      "Training | EPOCH: 1 | Average Loss: 0.0538 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0821 | Current Mini-batch loss: 0.1104\n",
      "Training | EPOCH: 1 | Average Loss: 0.0419 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.3328 | Current Mini-batch loss: 0.6236\n",
      "Training | EPOCH: 1 | Average Loss: 0.1678 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.2067 | Current Mini-batch loss: 0.2456\n",
      "Training | EPOCH: 1 | Average Loss: 0.1070 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 1 | Average Loss: 0.1019 | Current Mini-batch loss: 0.0967\n",
      "Training | EPOCH: 1 | Average Loss: 0.1836 | Current Mini-batch loss: 0.2653\n",
      "Training | EPOCH: 1 | Average Loss: 0.1748 | Current Mini-batch loss: 0.1660\n",
      "Training | EPOCH: 1 | Average Loss: 0.1087 | Current Mini-batch loss: 0.0426\n",
      "Training | EPOCH: 1 | Average Loss: 0.0667 | Current Mini-batch loss: 0.0246\n",
      "Training | EPOCH: 1 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 1 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0687 | Current Mini-batch loss: 0.1308\n",
      "Training | EPOCH: 1 | Average Loss: 0.1332 | Current Mini-batch loss: 0.1977\n",
      "Training | EPOCH: 1 | Average Loss: 0.0794 | Current Mini-batch loss: 0.0256\n",
      "Training | EPOCH: 1 | Average Loss: 0.0418 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0962 | Current Mini-batch loss: 0.1507\n",
      "Training | EPOCH: 1 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0702 | Current Mini-batch loss: 0.0905\n",
      "Training | EPOCH: 1 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 1 | Average Loss: 0.0583 | Current Mini-batch loss: 0.0702\n",
      "Training | EPOCH: 1 | Average Loss: 0.0596 | Current Mini-batch loss: 0.0610\n",
      "Training | EPOCH: 1 | Average Loss: 0.0314 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 1 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 1 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0413\n",
      "Training | EPOCH: 1 | Average Loss: 0.0286 | Current Mini-batch loss: 0.0356\n",
      "Training | EPOCH: 1 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 1 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 1 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0682 | Current Mini-batch loss: 0.1310\n",
      "Training | EPOCH: 1 | Average Loss: 0.0346 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 1 | Average Loss: 0.0507 | Current Mini-batch loss: 0.0833\n",
      "Training | EPOCH: 1 | Average Loss: 0.0999 | Current Mini-batch loss: 0.1491\n",
      "Training | EPOCH: 1 | Average Loss: 0.0502 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0527 | Current Mini-batch loss: 0.0551\n",
      "Training | EPOCH: 1 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 1 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 1 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0115\n",
      "Training | EPOCH: 1 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 1 | Average Loss: 0.1634 | Current Mini-batch loss: 0.3158\n",
      "Training | EPOCH: 1 | Average Loss: 0.0839 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0493 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 1 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 1 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 1 | Average Loss: 0.1780 | Current Mini-batch loss: 0.3467\n",
      "Training | EPOCH: 1 | Average Loss: 0.0892 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.1697 | Current Mini-batch loss: 0.2502\n",
      "Training | EPOCH: 1 | Average Loss: 0.0868 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 1 | Average Loss: 0.1836 | Current Mini-batch loss: 0.2805\n",
      "Training | EPOCH: 1 | Average Loss: 0.0929 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.0502 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 1 | Average Loss: 0.0633 | Current Mini-batch loss: 0.0763\n",
      "Training | EPOCH: 1 | Average Loss: 0.0822 | Current Mini-batch loss: 0.1012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0094\n",
      "Training | EPOCH: 1 | Average Loss: 0.0428 | Current Mini-batch loss: 0.0598\n",
      "Training | EPOCH: 1 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 1 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0139\n",
      "Training | EPOCH: 1 | Average Loss: 0.3737 | Current Mini-batch loss: 0.7368\n",
      "Training | EPOCH: 1 | Average Loss: 0.1909 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 1 | Average Loss: 0.1040 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 1 | Average Loss: 0.0565 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 1 | Average Loss: 0.0291 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 1 | Average Loss: 0.2901 | Current Mini-batch loss: 0.5511\n",
      "Training | EPOCH: 1 | Average Loss: 0.1455 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 1 | Average Loss: 0.2345 | Current Mini-batch loss: 0.3235\n",
      "Training | EPOCH: 1 | Average Loss: 0.1199 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 1 | Average Loss: 0.1067 | Current Mini-batch loss: 0.0936\n",
      "Training | EPOCH: 1 | Average Loss: 0.0555 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0261\n",
      "Training | EPOCH: 1 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 1 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0159\n",
      "Training | EPOCH: 1 | Average Loss: 0.2576 | Current Mini-batch loss: 0.5051\n",
      "Training | EPOCH: 1 | Average Loss: 0.1495 | Current Mini-batch loss: 0.0415\n",
      "Training | EPOCH: 1 | Average Loss: 0.1030 | Current Mini-batch loss: 0.0565\n",
      "Training | EPOCH: 1 | Average Loss: 0.0629 | Current Mini-batch loss: 0.0229\n",
      "Training | EPOCH: 1 | Average Loss: 0.1872 | Current Mini-batch loss: 0.3115\n",
      "Training | EPOCH: 1 | Average Loss: 0.1062 | Current Mini-batch loss: 0.0252\n",
      "Training | EPOCH: 1 | Average Loss: 0.0872 | Current Mini-batch loss: 0.0681\n",
      "Training | EPOCH: 1 | Average Loss: 0.0453 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 1 | Average Loss: 0.0590 | Current Mini-batch loss: 0.0908\n",
      "Training | EPOCH: 1 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.3852 | Current Mini-batch loss: 0.7554\n",
      "Training | EPOCH: 1 | Average Loss: 0.2007 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 1 | Average Loss: 0.1073 | Current Mini-batch loss: 0.0138\n",
      "Training | EPOCH: 1 | Average Loss: 0.0762 | Current Mini-batch loss: 0.0451\n",
      "Training | EPOCH: 1 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 1 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0824\n",
      "Training | EPOCH: 1 | Average Loss: 0.1370 | Current Mini-batch loss: 0.2218\n",
      "Training | EPOCH: 1 | Average Loss: 0.1038 | Current Mini-batch loss: 0.0707\n",
      "Training | EPOCH: 1 | Average Loss: 0.1210 | Current Mini-batch loss: 0.1382\n",
      "Training | EPOCH: 1 | Average Loss: 0.1874 | Current Mini-batch loss: 0.2537\n",
      "Training | EPOCH: 1 | Average Loss: 0.1859 | Current Mini-batch loss: 0.1844\n",
      "Training | EPOCH: 1 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 1 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0156\n",
      "Training | EPOCH: 1 | Average Loss: 0.0195 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 1 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 1 | Average Loss: 0.4144 | Current Mini-batch loss: 0.8181\n",
      "Training | EPOCH: 1 | Average Loss: 0.2077 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 1 | Average Loss: 0.1066 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 1 | Average Loss: 0.0983 | Current Mini-batch loss: 0.0900\n",
      "Training | EPOCH: 1 | Average Loss: 0.0670 | Current Mini-batch loss: 0.0356\n",
      "Training | EPOCH: 1 | Average Loss: 0.1292 | Current Mini-batch loss: 0.1914\n",
      "Training | EPOCH: 1 | Average Loss: 0.3090 | Current Mini-batch loss: 0.4888\n",
      "Training | EPOCH: 1 | Average Loss: 0.8816 | Current Mini-batch loss: 1.4541\n",
      "Training | EPOCH: 1 | Average Loss: 0.4419 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.4747 | Current Mini-batch loss: 0.5075\n",
      "Training | EPOCH: 1 | Average Loss: 0.2555 | Current Mini-batch loss: 0.0364\n",
      "Training | EPOCH: 1 | Average Loss: 0.1283 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0672 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 1 | Average Loss: 0.2441 | Current Mini-batch loss: 0.4210\n",
      "Training | EPOCH: 1 | Average Loss: 0.1241 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 1 | Average Loss: 0.0629 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 1 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 1 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 1 | Average Loss: 0.0507 | Current Mini-batch loss: 0.0924\n",
      "Training | EPOCH: 1 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0409\n",
      "Training | EPOCH: 1 | Average Loss: 0.0271 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 1 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 1 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0907 | Current Mini-batch loss: 0.1734\n",
      "Training | EPOCH: 1 | Average Loss: 0.1558 | Current Mini-batch loss: 0.2210\n",
      "Training | EPOCH: 1 | Average Loss: 0.1000 | Current Mini-batch loss: 0.0442\n",
      "Training | EPOCH: 1 | Average Loss: 0.1910 | Current Mini-batch loss: 0.2821\n",
      "Training | EPOCH: 1 | Average Loss: 0.0967 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.0492 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 1 | Average Loss: 0.0472 | Current Mini-batch loss: 0.0453\n",
      "Training | EPOCH: 1 | Average Loss: 0.0311 | Current Mini-batch loss: 0.0150\n",
      "Training | EPOCH: 1 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 1 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 1 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0285\n",
      "Training | EPOCH: 1 | Average Loss: 0.0500 | Current Mini-batch loss: 0.0801\n",
      "Training | EPOCH: 1 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0754 | Current Mini-batch loss: 0.1255\n",
      "Training | EPOCH: 1 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 1 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 1 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 1 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 1 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0789\n",
      "Training | EPOCH: 1 | Average Loss: 0.0299 | Current Mini-batch loss: 0.0174\n",
      "Training | EPOCH: 1 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 1 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 1 | Average Loss: 0.1019 | Current Mini-batch loss: 0.2016\n",
      "Training | EPOCH: 1 | Average Loss: 0.0743 | Current Mini-batch loss: 0.0468\n",
      "Training | EPOCH: 1 | Average Loss: 0.0463 | Current Mini-batch loss: 0.0183\n",
      "Training | EPOCH: 1 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0323\n",
      "Training | EPOCH: 1 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.1518 | Current Mini-batch loss: 0.2833\n",
      "Training | EPOCH: 1 | Average Loss: 0.1814 | Current Mini-batch loss: 0.2110\n",
      "Training | EPOCH: 1 | Average Loss: 0.0921 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0576 | Current Mini-batch loss: 0.0232\n",
      "Training | EPOCH: 1 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 1 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0429 | Current Mini-batch loss: 0.0750\n",
      "Training | EPOCH: 1 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 1 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0097\n",
      "Training | EPOCH: 1 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 1 | Average Loss: 0.2675 | Current Mini-batch loss: 0.5281\n",
      "Training | EPOCH: 1 | Average Loss: 0.2575 | Current Mini-batch loss: 0.2475\n",
      "Training | EPOCH: 1 | Average Loss: 0.1327 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 1 | Average Loss: 0.0669 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.1022 | Current Mini-batch loss: 0.1374\n",
      "Training | EPOCH: 1 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.7550 | Current Mini-batch loss: 1.4838\n",
      "Training | EPOCH: 1 | Average Loss: 0.3776 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.1899 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.2918 | Current Mini-batch loss: 0.3937\n",
      "Training | EPOCH: 1 | Average Loss: 0.1474 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.1466 | Current Mini-batch loss: 0.1458\n",
      "Training | EPOCH: 1 | Average Loss: 0.0750 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0287\n",
      "Training | EPOCH: 1 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 1 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 1 | Average Loss: 0.3810 | Current Mini-batch loss: 0.7454\n",
      "Training | EPOCH: 1 | Average Loss: 1.5247 | Current Mini-batch loss: 2.6685\n",
      "Training | EPOCH: 1 | Average Loss: 1.0415 | Current Mini-batch loss: 0.5582\n",
      "Training | EPOCH: 1 | Average Loss: 1.0112 | Current Mini-batch loss: 0.9810\n",
      "Training | EPOCH: 1 | Average Loss: 0.5069 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.2610 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 1 | Average Loss: 0.1307 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.2225 | Current Mini-batch loss: 0.3143\n",
      "Training | EPOCH: 1 | Average Loss: 0.1141 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0580 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0612 | Current Mini-batch loss: 0.0644\n",
      "Training | EPOCH: 1 | Average Loss: 0.0334 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 1 | Average Loss: 0.0195 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 1 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0531\n",
      "Training | EPOCH: 1 | Average Loss: 0.2972 | Current Mini-batch loss: 0.5581\n",
      "Training | EPOCH: 1 | Average Loss: 0.4410 | Current Mini-batch loss: 0.5847\n",
      "Training | EPOCH: 1 | Average Loss: 0.7007 | Current Mini-batch loss: 0.9604\n",
      "Training | EPOCH: 1 | Average Loss: 0.3521 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.1768 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 1 | Average Loss: 0.1770 | Current Mini-batch loss: 0.1772\n",
      "Training | EPOCH: 1 | Average Loss: 0.0975 | Current Mini-batch loss: 0.0179\n",
      "Training | EPOCH: 1 | Average Loss: 0.0490 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 1 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0213\n",
      "Training | EPOCH: 1 | Average Loss: 0.0330 | Current Mini-batch loss: 0.0469\n",
      "Training | EPOCH: 1 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 1 | Average Loss: 0.0648 | Current Mini-batch loss: 0.1183\n",
      "Training | EPOCH: 1 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 1 | Average Loss: 0.2198 | Current Mini-batch loss: 0.4166\n",
      "Training | EPOCH: 1 | Average Loss: 0.1123 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 1 | Average Loss: 0.0965 | Current Mini-batch loss: 0.0808\n",
      "Training | EPOCH: 1 | Average Loss: 0.0506 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0969 | Current Mini-batch loss: 0.1431\n",
      "Training | EPOCH: 1 | Average Loss: 0.0494 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0144\n",
      "Training | EPOCH: 1 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0139\n",
      "Training | EPOCH: 1 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0344\n",
      "Training | EPOCH: 1 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 1 | Average Loss: 0.3389 | Current Mini-batch loss: 0.6620\n",
      "Training | EPOCH: 1 | Average Loss: 0.2617 | Current Mini-batch loss: 0.1846\n",
      "Training | EPOCH: 1 | Average Loss: 0.1309 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 1 | Average Loss: 0.0663 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 1 | Average Loss: 0.0358 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 1 | Average Loss: 0.1328 | Current Mini-batch loss: 0.2299\n",
      "Training | EPOCH: 1 | Average Loss: 0.0670 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.1246 | Current Mini-batch loss: 0.1823\n",
      "Training | EPOCH: 1 | Average Loss: 0.0641 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0327 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0305\n",
      "Training | EPOCH: 1 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 1 | Average Loss: 0.2318 | Current Mini-batch loss: 0.4399\n",
      "Training | EPOCH: 1 | Average Loss: 0.1185 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 1 | Average Loss: 0.0793 | Current Mini-batch loss: 0.0400\n",
      "Training | EPOCH: 1 | Average Loss: 0.0405 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.1519 | Current Mini-batch loss: 0.2633\n",
      "Training | EPOCH: 1 | Average Loss: 0.0850 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 1 | Average Loss: 0.0436 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 1 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0208\n",
      "Training | EPOCH: 1 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0166\n",
      "Training | EPOCH: 1 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0214 | Current Mini-batch loss: 0.0316\n",
      "Training | EPOCH: 1 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0447\n",
      "Training | EPOCH: 1 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0117\n",
      "Training | EPOCH: 1 | Average Loss: 0.0488 | Current Mini-batch loss: 0.0775\n",
      "Training | EPOCH: 1 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 1 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0391\n",
      "Training | EPOCH: 1 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 1 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 1 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 1 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0264\n",
      "Training | EPOCH: 1 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 1 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 1 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0657 | Current Mini-batch loss: 0.1282\n",
      "Training | EPOCH: 1 | Average Loss: 0.0334 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0398\n",
      "Training | EPOCH: 1 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0351\n",
      "Training | EPOCH: 1 | Average Loss: 0.0234 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 1 | Average Loss: 0.0234 | Current Mini-batch loss: 0.0234\n",
      "Training | EPOCH: 1 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0288\n",
      "Training | EPOCH: 1 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0245\n",
      "Training | EPOCH: 1 | Average Loss: 0.1282 | Current Mini-batch loss: 0.2337\n",
      "Training | EPOCH: 1 | Average Loss: 0.0975 | Current Mini-batch loss: 0.0667\n",
      "Training | EPOCH: 1 | Average Loss: 0.2785 | Current Mini-batch loss: 0.4596\n",
      "Training | EPOCH: 1 | Average Loss: 0.1574 | Current Mini-batch loss: 0.0362\n",
      "Training | EPOCH: 1 | Average Loss: 0.1061 | Current Mini-batch loss: 0.0549\n",
      "Training | EPOCH: 1 | Average Loss: 0.2099 | Current Mini-batch loss: 0.3137\n",
      "Training | EPOCH: 1 | Average Loss: 0.1083 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 1 | Average Loss: 0.1018 | Current Mini-batch loss: 0.0952\n",
      "Training | EPOCH: 1 | Average Loss: 0.1011 | Current Mini-batch loss: 0.1005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0602 | Current Mini-batch loss: 0.0194\n",
      "Training | EPOCH: 1 | Average Loss: 0.0308 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0607 | Current Mini-batch loss: 0.0906\n",
      "Training | EPOCH: 1 | Average Loss: 0.0320 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 1 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0506\n",
      "Training | EPOCH: 1 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 1 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0215\n",
      "Training | EPOCH: 1 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0147\n",
      "Training | EPOCH: 1 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.6051 | Current Mini-batch loss: 1.2062\n",
      "Training | EPOCH: 1 | Average Loss: 0.3261 | Current Mini-batch loss: 0.0471\n",
      "Training | EPOCH: 1 | Average Loss: 0.1883 | Current Mini-batch loss: 0.0505\n",
      "Training | EPOCH: 1 | Average Loss: 0.0945 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0481 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 1 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0252\n",
      "Training | EPOCH: 1 | Average Loss: 0.0561 | Current Mini-batch loss: 0.0824\n",
      "Training | EPOCH: 1 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 1 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 1 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0244\n",
      "Training | EPOCH: 1 | Average Loss: 0.3514 | Current Mini-batch loss: 0.6869\n",
      "Training | EPOCH: 1 | Average Loss: 0.1824 | Current Mini-batch loss: 0.0134\n",
      "Training | EPOCH: 1 | Average Loss: 0.3179 | Current Mini-batch loss: 0.4533\n",
      "Training | EPOCH: 1 | Average Loss: 0.1601 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.0815 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0388 | Current Mini-batch loss: 0.0363\n",
      "Training | EPOCH: 1 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0262\n",
      "Training | EPOCH: 1 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 1 | Average Loss: 0.5082 | Current Mini-batch loss: 1.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.3455 | Current Mini-batch loss: 0.1828\n",
      "Training | EPOCH: 1 | Average Loss: 0.3062 | Current Mini-batch loss: 0.2669\n",
      "Training | EPOCH: 1 | Average Loss: 0.1750 | Current Mini-batch loss: 0.0437\n",
      "Training | EPOCH: 1 | Average Loss: 0.0891 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 1 | Average Loss: 0.0471 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 1 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0255\n",
      "Training | EPOCH: 1 | Average Loss: 0.0798 | Current Mini-batch loss: 0.1233\n",
      "Training | EPOCH: 1 | Average Loss: 0.0479 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 1 | Average Loss: 0.1114 | Current Mini-batch loss: 0.1749\n",
      "Training | EPOCH: 1 | Average Loss: 0.0826 | Current Mini-batch loss: 0.0538\n",
      "Training | EPOCH: 1 | Average Loss: 0.0494 | Current Mini-batch loss: 0.0162\n",
      "Training | EPOCH: 1 | Average Loss: 0.0313 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 1 | Average Loss: 0.2160 | Current Mini-batch loss: 0.4007\n",
      "Training | EPOCH: 1 | Average Loss: 0.2150 | Current Mini-batch loss: 0.2141\n",
      "Training | EPOCH: 1 | Average Loss: 0.1098 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0600 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 1 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 1 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0196 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 1 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.3353 | Current Mini-batch loss: 0.6649\n",
      "Training | EPOCH: 1 | Average Loss: 0.2246 | Current Mini-batch loss: 0.1140\n",
      "Training | EPOCH: 1 | Average Loss: 0.3694 | Current Mini-batch loss: 0.5141\n",
      "Training | EPOCH: 1 | Average Loss: 0.2695 | Current Mini-batch loss: 0.1696\n",
      "Training | EPOCH: 1 | Average Loss: 0.1827 | Current Mini-batch loss: 0.0960\n",
      "Training | EPOCH: 1 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 1 | Average Loss: 0.0495 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 1 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 1 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0313\n",
      "Training | EPOCH: 1 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 1 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 1 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0735 | Current Mini-batch loss: 0.1388\n",
      "Training | EPOCH: 1 | Average Loss: 0.0386 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 1 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 1 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0205\n",
      "Training | EPOCH: 1 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0153\n",
      "Training | EPOCH: 1 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.0431 | Current Mini-batch loss: 0.0793\n",
      "Training | EPOCH: 1 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0183\n",
      "Training | EPOCH: 1 | Average Loss: 0.0575 | Current Mini-batch loss: 0.0843\n",
      "Training | EPOCH: 1 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 1 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0564\n",
      "Training | EPOCH: 1 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.3908 | Current Mini-batch loss: 0.7597\n",
      "Training | EPOCH: 1 | Average Loss: 0.1959 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0986 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0846 | Current Mini-batch loss: 0.0705\n",
      "Training | EPOCH: 1 | Average Loss: 0.1839 | Current Mini-batch loss: 0.2832\n",
      "Training | EPOCH: 1 | Average Loss: 0.1030 | Current Mini-batch loss: 0.0222\n",
      "Training | EPOCH: 1 | Average Loss: 0.0539 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 1 | Average Loss: 0.0276 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0137\n",
      "Training | EPOCH: 1 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 1 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 1 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 1 | Average Loss: 0.0923 | Current Mini-batch loss: 0.1732\n",
      "Training | EPOCH: 1 | Average Loss: 0.0741 | Current Mini-batch loss: 0.0560\n",
      "Training | EPOCH: 1 | Average Loss: 0.0813 | Current Mini-batch loss: 0.0886\n",
      "Training | EPOCH: 1 | Average Loss: 0.2236 | Current Mini-batch loss: 0.3659\n",
      "Training | EPOCH: 1 | Average Loss: 0.1130 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.2067 | Current Mini-batch loss: 0.3004\n",
      "Training | EPOCH: 1 | Average Loss: 0.2460 | Current Mini-batch loss: 0.2853\n",
      "Training | EPOCH: 1 | Average Loss: 0.3453 | Current Mini-batch loss: 0.4445\n",
      "Training | EPOCH: 1 | Average Loss: 0.1785 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 1 | Average Loss: 0.0896 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0502 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 1 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0161\n",
      "Training | EPOCH: 1 | Average Loss: 1.0822 | Current Mini-batch loss: 2.1437\n",
      "Training | EPOCH: 1 | Average Loss: 0.5441 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 1 | Average Loss: 0.2770 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 1 | Average Loss: 0.1400 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0724 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 1 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0384 | Current Mini-batch loss: 0.0406\n",
      "Training | EPOCH: 1 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0184\n",
      "Training | EPOCH: 1 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 1 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0275\n",
      "Training | EPOCH: 1 | Average Loss: 0.0912 | Current Mini-batch loss: 0.1609\n",
      "Training | EPOCH: 1 | Average Loss: 0.0593 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 1 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0203\n",
      "Training | EPOCH: 1 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0142\n",
      "Training | EPOCH: 1 | Average Loss: 0.3035 | Current Mini-batch loss: 0.5908\n",
      "Training | EPOCH: 1 | Average Loss: 0.1522 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0894 | Current Mini-batch loss: 0.0267\n",
      "Training | EPOCH: 1 | Average Loss: 0.2137 | Current Mini-batch loss: 0.3379\n",
      "Training | EPOCH: 1 | Average Loss: 0.2356 | Current Mini-batch loss: 0.2575\n",
      "Training | EPOCH: 1 | Average Loss: 0.1193 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0608 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.0389 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 1 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 1 | Average Loss: 0.1939 | Current Mini-batch loss: 0.3662\n",
      "Training | EPOCH: 1 | Average Loss: 0.1079 | Current Mini-batch loss: 0.0219\n",
      "Training | EPOCH: 1 | Average Loss: 0.0545 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0295 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 1 | Average Loss: 0.0367 | Current Mini-batch loss: 0.0636\n",
      "Training | EPOCH: 1 | Average Loss: 0.1964 | Current Mini-batch loss: 0.3560\n",
      "Training | EPOCH: 1 | Average Loss: 0.0985 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0524 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 1 | Average Loss: 0.0735 | Current Mini-batch loss: 0.0946\n",
      "Training | EPOCH: 1 | Average Loss: 0.0410 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 1 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 1 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 1 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 1 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0730 | Current Mini-batch loss: 0.1416\n",
      "Training | EPOCH: 1 | Average Loss: 0.4318 | Current Mini-batch loss: 0.7907\n",
      "Training | EPOCH: 1 | Average Loss: 0.2208 | Current Mini-batch loss: 0.0097\n",
      "Training | EPOCH: 1 | Average Loss: 0.3791 | Current Mini-batch loss: 0.5374\n",
      "Training | EPOCH: 1 | Average Loss: 0.1913 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 1 | Average Loss: 0.0968 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.1803 | Current Mini-batch loss: 0.2637\n",
      "Training | EPOCH: 1 | Average Loss: 0.0913 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.1960 | Current Mini-batch loss: 0.3008\n",
      "Training | EPOCH: 1 | Average Loss: 0.1645 | Current Mini-batch loss: 0.1330\n",
      "Training | EPOCH: 1 | Average Loss: 0.0850 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 1 | Average Loss: 0.1709 | Current Mini-batch loss: 0.2568\n",
      "Training | EPOCH: 1 | Average Loss: 0.0864 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 1 | Average Loss: 0.0527 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 1 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 1 | Average Loss: 0.0282 | Current Mini-batch loss: 0.0533\n",
      "Training | EPOCH: 1 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0102\n",
      "Training | EPOCH: 1 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.1162 | Current Mini-batch loss: 0.2227\n",
      "Training | EPOCH: 1 | Average Loss: 0.0970 | Current Mini-batch loss: 0.0777\n",
      "Training | EPOCH: 1 | Average Loss: 0.1362 | Current Mini-batch loss: 0.1755\n",
      "Training | EPOCH: 1 | Average Loss: 0.0801 | Current Mini-batch loss: 0.0240\n",
      "Training | EPOCH: 1 | Average Loss: 0.0432 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 1 | Average Loss: 0.0289 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 1 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 1 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0599\n",
      "Training | EPOCH: 1 | Average Loss: 0.0332 | Current Mini-batch loss: 0.0340\n",
      "Training | EPOCH: 1 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0189\n",
      "Training | EPOCH: 1 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0354\n",
      "Training | EPOCH: 1 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 1 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0156\n",
      "Training | EPOCH: 1 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0776\n",
      "Training | EPOCH: 1 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 1 | Average Loss: 0.2545 | Current Mini-batch loss: 0.4956\n",
      "Training | EPOCH: 1 | Average Loss: 0.1363 | Current Mini-batch loss: 0.0182\n",
      "Training | EPOCH: 1 | Average Loss: 0.0689 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0468 | Current Mini-batch loss: 0.0248\n",
      "Training | EPOCH: 1 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.4048 | Current Mini-batch loss: 0.7859\n",
      "Training | EPOCH: 1 | Average Loss: 0.3226 | Current Mini-batch loss: 0.2404\n",
      "Training | EPOCH: 1 | Average Loss: 0.1627 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0870 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 1 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 1 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0175\n",
      "Training | EPOCH: 1 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 1 | Average Loss: 0.1040 | Current Mini-batch loss: 0.1873\n",
      "Training | EPOCH: 1 | Average Loss: 0.1708 | Current Mini-batch loss: 0.2377\n",
      "Training | EPOCH: 1 | Average Loss: 0.0977 | Current Mini-batch loss: 0.0245\n",
      "Training | EPOCH: 1 | Average Loss: 0.0855 | Current Mini-batch loss: 0.0733\n",
      "Training | EPOCH: 1 | Average Loss: 0.0442 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0245 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 1 | Average Loss: 0.0753 | Current Mini-batch loss: 0.1260\n",
      "Training | EPOCH: 1 | Average Loss: 0.0784 | Current Mini-batch loss: 0.0814\n",
      "Training | EPOCH: 1 | Average Loss: 0.0728 | Current Mini-batch loss: 0.0673\n",
      "Training | EPOCH: 1 | Average Loss: 0.0691 | Current Mini-batch loss: 0.0655\n",
      "Training | EPOCH: 1 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.4599 | Current Mini-batch loss: 0.8849\n",
      "Training | EPOCH: 1 | Average Loss: 0.2305 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.3975 | Current Mini-batch loss: 0.5645\n",
      "Training | EPOCH: 1 | Average Loss: 0.1999 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 1 | Average Loss: 0.1007 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 1 | Average Loss: 0.0516 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 1 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0231\n",
      "Training | EPOCH: 1 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 1 | Average Loss: 0.2787 | Current Mini-batch loss: 0.5423\n",
      "Training | EPOCH: 1 | Average Loss: 0.1656 | Current Mini-batch loss: 0.0526\n",
      "Training | EPOCH: 1 | Average Loss: 0.1050 | Current Mini-batch loss: 0.0445\n",
      "Training | EPOCH: 1 | Average Loss: 0.0578 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 1 | Average Loss: 0.1122 | Current Mini-batch loss: 0.1665\n",
      "Training | EPOCH: 1 | Average Loss: 0.0733 | Current Mini-batch loss: 0.0344\n",
      "Training | EPOCH: 1 | Average Loss: 0.0698 | Current Mini-batch loss: 0.0663\n",
      "Training | EPOCH: 1 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 1 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 1 | Average Loss: 0.0443 | Current Mini-batch loss: 0.0702\n",
      "Training | EPOCH: 1 | Average Loss: 0.0226 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0135 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 1 | Average Loss: 0.1759 | Current Mini-batch loss: 0.3382\n",
      "Training | EPOCH: 1 | Average Loss: 0.0894 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 1 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 1 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 1.4091 | Current Mini-batch loss: 2.8117\n",
      "Training | EPOCH: 1 | Average Loss: 2.8546 | Current Mini-batch loss: 4.3000\n",
      "Training | EPOCH: 1 | Average Loss: 1.4273 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 1 | Average Loss: 1.0024 | Current Mini-batch loss: 0.5775\n",
      "Training | EPOCH: 1 | Average Loss: 0.5075 | Current Mini-batch loss: 0.0127\n",
      "Training | EPOCH: 1 | Average Loss: 0.4030 | Current Mini-batch loss: 0.2985\n",
      "Training | EPOCH: 1 | Average Loss: 0.2021 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.1055 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 1 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 1 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 1 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 1 | Average Loss: 0.1045 | Current Mini-batch loss: 0.1934\n",
      "Training | EPOCH: 1 | Average Loss: 0.0530 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 1 | Average Loss: 0.0699 | Current Mini-batch loss: 0.0868\n",
      "Training | EPOCH: 1 | Average Loss: 0.0357 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 1 | Average Loss: 0.0226 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 1 | Average Loss: 0.0503 | Current Mini-batch loss: 0.0780\n",
      "Training | EPOCH: 1 | Average Loss: 0.0266 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 1 | Average Loss: 0.0891 | Current Mini-batch loss: 0.1516\n",
      "Training | EPOCH: 1 | Average Loss: 0.0586 | Current Mini-batch loss: 0.0281\n",
      "Training | EPOCH: 1 | Average Loss: 0.2007 | Current Mini-batch loss: 0.3429\n",
      "Training | EPOCH: 1 | Average Loss: 0.1389 | Current Mini-batch loss: 0.0770\n",
      "Training | EPOCH: 1 | Average Loss: 0.0695 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 1 | Average Loss: 0.3541 | Current Mini-batch loss: 0.6387\n",
      "Training | EPOCH: 1 | Average Loss: 0.2396 | Current Mini-batch loss: 0.1251\n",
      "Training | EPOCH: 1 | Average Loss: 0.1257 | Current Mini-batch loss: 0.0117\n",
      "Training | EPOCH: 1 | Average Loss: 0.0631 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0548 | Current Mini-batch loss: 0.0925\n",
      "Training | EPOCH: 1 | Average Loss: 0.1093 | Current Mini-batch loss: 0.1639\n",
      "Training | EPOCH: 1 | Average Loss: 0.0548 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 1 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0098\n",
      "Training | EPOCH: 1 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0477 | Current Mini-batch loss: 0.0858\n",
      "Training | EPOCH: 1 | Average Loss: 0.1012 | Current Mini-batch loss: 0.1546\n",
      "Training | EPOCH: 1 | Average Loss: 0.0508 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0255 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 1 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 1 | Average Loss: 0.0135 | Current Mini-batch loss: 0.0193\n",
      "Training | EPOCH: 1 | Average Loss: 0.0453 | Current Mini-batch loss: 0.0771\n",
      "Training | EPOCH: 1 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 1 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0582\n",
      "Training | EPOCH: 1 | Average Loss: 0.0357 | Current Mini-batch loss: 0.0364\n",
      "Training | EPOCH: 1 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 1 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 1 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.1606 | Current Mini-batch loss: 0.3129\n",
      "Training | EPOCH: 1 | Average Loss: 0.0804 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 1 | Average Loss: 0.3209 | Current Mini-batch loss: 0.5614\n",
      "Training | EPOCH: 1 | Average Loss: 0.3721 | Current Mini-batch loss: 0.4233\n",
      "Training | EPOCH: 1 | Average Loss: 0.1862 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.1766 | Current Mini-batch loss: 0.1670\n",
      "Training | EPOCH: 1 | Average Loss: 0.0887 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 1 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 1 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 1 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0340\n",
      "Training | EPOCH: 1 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 1 | Average Loss: 0.0299 | Current Mini-batch loss: 0.0547\n",
      "Training | EPOCH: 1 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 1 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0222\n",
      "Training | EPOCH: 1 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 1 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 1 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 1 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0283\n",
      "Training | EPOCH: 1 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 1 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0822\n",
      "Training | EPOCH: 1 | Average Loss: 0.0548 | Current Mini-batch loss: 0.0611\n",
      "Training | EPOCH: 1 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 1 | Average Loss: 0.0381 | Current Mini-batch loss: 0.0486\n",
      "Training | EPOCH: 1 | Average Loss: 0.0193 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 1 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0436\n",
      "Training | EPOCH: 1 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 1 | Average Loss: 0.1132 | Current Mini-batch loss: 0.2100\n",
      "Training | EPOCH: 1 | Average Loss: 0.0640 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 1 | Average Loss: 0.0371 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 1 | Average Loss: 0.0188 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 1 | Average Loss: 0.0466 | Current Mini-batch loss: 0.0745\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0644 | Current Mini-batch loss: 0.0644\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0030\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0150\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0021\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0054\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0028\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0150\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0024\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0056\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0961 | Current Mini-batch loss: 0.1886\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0490 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0330\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0170 | Current Mini-batch loss: 0.0110\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0114\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0135\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0349 | Current Mini-batch loss: 0.0595\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0049\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1263 | Current Mini-batch loss: 0.2479\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0633 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1374 | Current Mini-batch loss: 0.2114\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0716 | Current Mini-batch loss: 0.0059\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0481 | Current Mini-batch loss: 0.0600\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0019\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0040\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0351 | Current Mini-batch loss: 0.0688\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0319\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0387\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 1 | Average Loss: 0.4342 | Current Mini-batch loss: 0.8531\n",
      "Validation | EPOCH: 1 | Average Loss: 0.4841 | Current Mini-batch loss: 0.5341\n",
      "Validation | EPOCH: 1 | Average Loss: 0.2425 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1218 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0080\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0471 | Current Mini-batch loss: 0.0293\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0056\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0052\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0026\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0432 | Current Mini-batch loss: 0.0849\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0021\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1481 | Current Mini-batch loss: 0.2896\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0745 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0376 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1225 | Current Mini-batch loss: 0.2075\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0825 | Current Mini-batch loss: 0.0425\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0415 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0047\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0107\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0239 | Current Mini-batch loss: 0.0402\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0058\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0083\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0035\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0040\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0897 | Current Mini-batch loss: 0.1770\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0666 | Current Mini-batch loss: 0.0435\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0153\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0101\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1094 | Current Mini-batch loss: 0.2077\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0551 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0282 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0421 | Current Mini-batch loss: 0.0767\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0060\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0189\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0198\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0053\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0190\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1224 | Current Mini-batch loss: 0.2386\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0707 | Current Mini-batch loss: 0.0190\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0356 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0078\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.2487 | Current Mini-batch loss: 0.4964\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1252 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0629 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0096\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0363\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0038\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0713\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0448\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0164 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0140\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0222\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0066\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0317 | Current Mini-batch loss: 0.0596\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0019\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0859 | Current Mini-batch loss: 0.1551\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0448 | Current Mini-batch loss: 0.0036\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0023\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0046\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0126\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0200\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1112 | Current Mini-batch loss: 0.2162\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0556 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0039\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0186\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0060\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0292\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0022\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0061\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0452\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0186\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 1 | Average Loss: 0.2042 | Current Mini-batch loss: 0.3977\n",
      "Validation | EPOCH: 1 | Average Loss: 0.1025 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0559 | Current Mini-batch loss: 0.0093\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0950 | Current Mini-batch loss: 0.1340\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0480 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 1 | Average Loss: 0.0489 | Current Mini-batch loss: 0.0498\n",
      "Training | EPOCH: 2 | Average Loss: 0.0368 | Current Mini-batch loss: 0.0368\n",
      "Training | EPOCH: 2 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 2 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0144\n",
      "Training | EPOCH: 2 | Average Loss: 0.0705 | Current Mini-batch loss: 0.1231\n",
      "Training | EPOCH: 2 | Average Loss: 0.0421 | Current Mini-batch loss: 0.0138\n",
      "Training | EPOCH: 2 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0509 | Current Mini-batch loss: 0.0807\n",
      "Training | EPOCH: 2 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0281\n",
      "Training | EPOCH: 2 | Average Loss: 0.0311 | Current Mini-batch loss: 0.0227\n",
      "Training | EPOCH: 2 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 2 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0654\n",
      "Training | EPOCH: 2 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.1168 | Current Mini-batch loss: 0.2122\n",
      "Training | EPOCH: 2 | Average Loss: 0.0595 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 2 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0438 | Current Mini-batch loss: 0.0857\n",
      "Training | EPOCH: 2 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0231\n",
      "Training | EPOCH: 2 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0255\n",
      "Training | EPOCH: 2 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0177\n",
      "Training | EPOCH: 2 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0137\n",
      "Training | EPOCH: 2 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0475 | Current Mini-batch loss: 0.0916\n",
      "Training | EPOCH: 2 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0494\n",
      "Training | EPOCH: 2 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.1005 | Current Mini-batch loss: 0.1759\n",
      "Training | EPOCH: 2 | Average Loss: 0.4143 | Current Mini-batch loss: 0.7281\n",
      "Training | EPOCH: 2 | Average Loss: 0.2078 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.1180 | Current Mini-batch loss: 0.0282\n",
      "Training | EPOCH: 2 | Average Loss: 0.0591 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 1.1597 | Current Mini-batch loss: 2.3154\n",
      "Training | EPOCH: 2 | Average Loss: 1.2210 | Current Mini-batch loss: 1.2824\n",
      "Training | EPOCH: 2 | Average Loss: 0.6111 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.3086 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 2 | Average Loss: 0.3410 | Current Mini-batch loss: 0.3734\n",
      "Training | EPOCH: 2 | Average Loss: 0.1712 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.1497 | Current Mini-batch loss: 0.1282\n",
      "Training | EPOCH: 2 | Average Loss: 0.0758 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0386 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0478 | Current Mini-batch loss: 0.0570\n",
      "Training | EPOCH: 2 | Average Loss: 0.6160 | Current Mini-batch loss: 1.1842\n",
      "Training | EPOCH: 2 | Average Loss: 0.3104 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.1561 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0784 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.2335 | Current Mini-batch loss: 0.3886\n",
      "Training | EPOCH: 2 | Average Loss: 0.1216 | Current Mini-batch loss: 0.0098\n",
      "Training | EPOCH: 2 | Average Loss: 0.0802 | Current Mini-batch loss: 0.0388\n",
      "Training | EPOCH: 2 | Average Loss: 0.0402 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.2023 | Current Mini-batch loss: 0.3644\n",
      "Training | EPOCH: 2 | Average Loss: 0.7498 | Current Mini-batch loss: 1.2973\n",
      "Training | EPOCH: 2 | Average Loss: 0.3839 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 2 | Average Loss: 0.2561 | Current Mini-batch loss: 0.1283\n",
      "Training | EPOCH: 2 | Average Loss: 0.1282 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.2533 | Current Mini-batch loss: 0.3783\n",
      "Training | EPOCH: 2 | Average Loss: 0.2438 | Current Mini-batch loss: 0.2344\n",
      "Training | EPOCH: 2 | Average Loss: 0.1343 | Current Mini-batch loss: 0.0249\n",
      "Training | EPOCH: 2 | Average Loss: 0.0690 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 2 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 2 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0078\n",
      "Training | EPOCH: 2 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0276 | Current Mini-batch loss: 0.0524\n",
      "Training | EPOCH: 2 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 2 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 2 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 2 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 2 | Average Loss: 0.0464 | Current Mini-batch loss: 0.0877\n",
      "Training | EPOCH: 2 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 2 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0271\n",
      "Training | EPOCH: 2 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0806\n",
      "Training | EPOCH: 2 | Average Loss: 0.0251 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0598 | Current Mini-batch loss: 0.1068\n",
      "Training | EPOCH: 2 | Average Loss: 0.0823 | Current Mini-batch loss: 0.1049\n",
      "Training | EPOCH: 2 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 2 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0159\n",
      "Training | EPOCH: 2 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0339\n",
      "Training | EPOCH: 2 | Average Loss: 0.2051 | Current Mini-batch loss: 0.3889\n",
      "Training | EPOCH: 2 | Average Loss: 0.2112 | Current Mini-batch loss: 0.2173\n",
      "Training | EPOCH: 2 | Average Loss: 0.1264 | Current Mini-batch loss: 0.0416\n",
      "Training | EPOCH: 2 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 2 | Average Loss: 0.0351 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 2 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 2 | Average Loss: 0.0345 | Current Mini-batch loss: 0.0470\n",
      "Training | EPOCH: 2 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0136\n",
      "Training | EPOCH: 2 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 2 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0083\n",
      "Training | EPOCH: 2 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 2 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 2 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0209\n",
      "Training | EPOCH: 2 | Average Loss: 0.0798 | Current Mini-batch loss: 0.1464\n",
      "Training | EPOCH: 2 | Average Loss: 0.0449 | Current Mini-batch loss: 0.0101\n",
      "Training | EPOCH: 2 | Average Loss: 0.0286 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 2 | Average Loss: 0.0383 | Current Mini-batch loss: 0.0480\n",
      "Training | EPOCH: 2 | Average Loss: 0.0617 | Current Mini-batch loss: 0.0852\n",
      "Training | EPOCH: 2 | Average Loss: 0.0314 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0216\n",
      "Training | EPOCH: 2 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 2 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0199\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0274\n",
      "Training | EPOCH: 2 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0253\n",
      "Training | EPOCH: 2 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0537\n",
      "Training | EPOCH: 2 | Average Loss: 0.0313 | Current Mini-batch loss: 0.0342\n",
      "Training | EPOCH: 2 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 2 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 2 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0204\n",
      "Training | EPOCH: 2 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0152\n",
      "Training | EPOCH: 2 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.6780 | Current Mini-batch loss: 1.3528\n",
      "Training | EPOCH: 2 | Average Loss: 0.3394 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.4662 | Current Mini-batch loss: 0.5930\n",
      "Training | EPOCH: 2 | Average Loss: 0.5869 | Current Mini-batch loss: 0.7076\n",
      "Training | EPOCH: 2 | Average Loss: 0.3263 | Current Mini-batch loss: 0.0657\n",
      "Training | EPOCH: 2 | Average Loss: 0.2183 | Current Mini-batch loss: 0.1102\n",
      "Training | EPOCH: 2 | Average Loss: 0.1581 | Current Mini-batch loss: 0.0979\n",
      "Training | EPOCH: 2 | Average Loss: 0.0796 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.9068 | Current Mini-batch loss: 1.7339\n",
      "Training | EPOCH: 2 | Average Loss: 0.7497 | Current Mini-batch loss: 0.5926\n",
      "Training | EPOCH: 2 | Average Loss: 0.8620 | Current Mini-batch loss: 0.9742\n",
      "Training | EPOCH: 2 | Average Loss: 0.4412 | Current Mini-batch loss: 0.0205\n",
      "Training | EPOCH: 2 | Average Loss: 0.2212 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.3122 | Current Mini-batch loss: 0.4033\n",
      "Training | EPOCH: 2 | Average Loss: 0.1746 | Current Mini-batch loss: 0.0369\n",
      "Training | EPOCH: 2 | Average Loss: 0.0934 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 2 | Average Loss: 0.0468 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.2677 | Current Mini-batch loss: 0.4885\n",
      "Training | EPOCH: 2 | Average Loss: 0.1364 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 2 | Average Loss: 0.0707 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 2 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.2240 | Current Mini-batch loss: 0.4121\n",
      "Training | EPOCH: 2 | Average Loss: 0.1798 | Current Mini-batch loss: 0.1355\n",
      "Training | EPOCH: 2 | Average Loss: 0.0903 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.5513 | Current Mini-batch loss: 1.0123\n",
      "Training | EPOCH: 2 | Average Loss: 0.3926 | Current Mini-batch loss: 0.2338\n",
      "Training | EPOCH: 2 | Average Loss: 0.1978 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.1581 | Current Mini-batch loss: 0.1185\n",
      "Training | EPOCH: 2 | Average Loss: 0.1373 | Current Mini-batch loss: 0.1165\n",
      "Training | EPOCH: 2 | Average Loss: 0.0696 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0369 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0520 | Current Mini-batch loss: 0.0670\n",
      "Training | EPOCH: 2 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0721 | Current Mini-batch loss: 0.1310\n",
      "Training | EPOCH: 2 | Average Loss: 0.0759 | Current Mini-batch loss: 0.0796\n",
      "Training | EPOCH: 2 | Average Loss: 0.0665 | Current Mini-batch loss: 0.0571\n",
      "Training | EPOCH: 2 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 2 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0137\n",
      "Training | EPOCH: 2 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 2 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0223\n",
      "Training | EPOCH: 2 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0576\n",
      "Training | EPOCH: 2 | Average Loss: 0.3085 | Current Mini-batch loss: 0.5805\n",
      "Training | EPOCH: 2 | Average Loss: 0.1733 | Current Mini-batch loss: 0.0381\n",
      "Training | EPOCH: 2 | Average Loss: 0.0867 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0447 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.1781 | Current Mini-batch loss: 0.3114\n",
      "Training | EPOCH: 2 | Average Loss: 0.0896 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0466 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0439 | Current Mini-batch loss: 0.0412\n",
      "Training | EPOCH: 2 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.2282 | Current Mini-batch loss: 0.4340\n",
      "Training | EPOCH: 2 | Average Loss: 0.1146 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0668 | Current Mini-batch loss: 0.0189\n",
      "Training | EPOCH: 2 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.2794 | Current Mini-batch loss: 0.5238\n",
      "Training | EPOCH: 2 | Average Loss: 0.1959 | Current Mini-batch loss: 0.1124\n",
      "Training | EPOCH: 2 | Average Loss: 0.0987 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0497 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 2 | Average Loss: 0.2571 | Current Mini-batch loss: 0.4810\n",
      "Training | EPOCH: 2 | Average Loss: 0.1543 | Current Mini-batch loss: 0.0515\n",
      "Training | EPOCH: 2 | Average Loss: 0.0773 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 2 | Average Loss: 0.0926 | Current Mini-batch loss: 0.1444\n",
      "Training | EPOCH: 2 | Average Loss: 0.0551 | Current Mini-batch loss: 0.0176\n",
      "Training | EPOCH: 2 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.0281 | Current Mini-batch loss: 0.0271\n",
      "Training | EPOCH: 2 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 2 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0442\n",
      "Training | EPOCH: 2 | Average Loss: 0.1456 | Current Mini-batch loss: 0.2660\n",
      "Training | EPOCH: 2 | Average Loss: 0.0731 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0368 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0823 | Current Mini-batch loss: 0.1462\n",
      "Training | EPOCH: 2 | Average Loss: 0.0422 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 2 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0369\n",
      "Training | EPOCH: 2 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0260\n",
      "Training | EPOCH: 2 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 2 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 2 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0317\n",
      "Training | EPOCH: 2 | Average Loss: 0.2798 | Current Mini-batch loss: 0.5428\n",
      "Training | EPOCH: 2 | Average Loss: 0.1546 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 2 | Average Loss: 0.1014 | Current Mini-batch loss: 0.0483\n",
      "Training | EPOCH: 2 | Average Loss: 0.0516 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0196 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 2 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.1039 | Current Mini-batch loss: 0.2067\n",
      "Training | EPOCH: 2 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0833\n",
      "Training | EPOCH: 2 | Average Loss: 0.0868 | Current Mini-batch loss: 0.0800\n",
      "Training | EPOCH: 2 | Average Loss: 0.0440 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 2 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 2 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 2 | Average Loss: 0.1460 | Current Mini-batch loss: 0.2869\n",
      "Training | EPOCH: 2 | Average Loss: 0.0755 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 2 | Average Loss: 0.2206 | Current Mini-batch loss: 0.3658\n",
      "Training | EPOCH: 2 | Average Loss: 0.1188 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 2 | Average Loss: 0.0721 | Current Mini-batch loss: 0.0255\n",
      "Training | EPOCH: 2 | Average Loss: 0.0384 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 2 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 2 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 2 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 2 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 2 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1210 | Current Mini-batch loss: 0.2397\n",
      "Training | EPOCH: 2 | Average Loss: 0.1091 | Current Mini-batch loss: 0.0972\n",
      "Training | EPOCH: 2 | Average Loss: 0.0889 | Current Mini-batch loss: 0.0686\n",
      "Training | EPOCH: 2 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0102\n",
      "Training | EPOCH: 2 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 2 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0141\n",
      "Training | EPOCH: 2 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0303\n",
      "Training | EPOCH: 2 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 2 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 2 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 2 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 2 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0176\n",
      "Training | EPOCH: 2 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0182\n",
      "Training | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0182\n",
      "Training | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 2 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0482 | Current Mini-batch loss: 0.0923\n",
      "Training | EPOCH: 2 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0189\n",
      "Training | EPOCH: 2 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.3342 | Current Mini-batch loss: 0.6652\n",
      "Training | EPOCH: 2 | Average Loss: 0.1675 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.1374 | Current Mini-batch loss: 0.1073\n",
      "Training | EPOCH: 2 | Average Loss: 0.0718 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 2 | Average Loss: 0.0406 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 2 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0312\n",
      "Training | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0559 | Current Mini-batch loss: 0.0935\n",
      "Training | EPOCH: 2 | Average Loss: 0.1040 | Current Mini-batch loss: 0.1521\n",
      "Training | EPOCH: 2 | Average Loss: 0.0922 | Current Mini-batch loss: 0.0804\n",
      "Training | EPOCH: 2 | Average Loss: 0.0470 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0261\n",
      "Training | EPOCH: 2 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0420 | Current Mini-batch loss: 0.0643\n",
      "Training | EPOCH: 2 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 2 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0232\n",
      "Training | EPOCH: 2 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 2 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 2 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0410 | Current Mini-batch loss: 0.0785\n",
      "Training | EPOCH: 2 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 2 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0378\n",
      "Training | EPOCH: 2 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 2 | Average Loss: 0.1704 | Current Mini-batch loss: 0.3346\n",
      "Training | EPOCH: 2 | Average Loss: 0.1329 | Current Mini-batch loss: 0.0954\n",
      "Training | EPOCH: 2 | Average Loss: 0.0666 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 2 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 2 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0287\n",
      "Training | EPOCH: 2 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 2 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0210\n",
      "Training | EPOCH: 2 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0184\n",
      "Training | EPOCH: 2 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.0996 | Current Mini-batch loss: 0.1923\n",
      "Training | EPOCH: 2 | Average Loss: 0.0595 | Current Mini-batch loss: 0.0194\n",
      "Training | EPOCH: 2 | Average Loss: 0.0599 | Current Mini-batch loss: 0.0603\n",
      "Training | EPOCH: 2 | Average Loss: 0.0400 | Current Mini-batch loss: 0.0202\n",
      "Training | EPOCH: 2 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0219\n",
      "Training | EPOCH: 2 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 2 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 2 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 2 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0442\n",
      "Training | EPOCH: 2 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 2 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0265\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0547 | Current Mini-batch loss: 0.1019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0896 | Current Mini-batch loss: 0.1245\n",
      "Training | EPOCH: 2 | Average Loss: 0.0455 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0237 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0159\n",
      "Training | EPOCH: 2 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 2 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0294\n",
      "Training | EPOCH: 2 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0542 | Current Mini-batch loss: 0.0979\n",
      "Training | EPOCH: 2 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0676 | Current Mini-batch loss: 0.1080\n",
      "Training | EPOCH: 2 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0727 | Current Mini-batch loss: 0.1282\n",
      "Training | EPOCH: 2 | Average Loss: 0.0372 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0772 | Current Mini-batch loss: 0.1171\n",
      "Training | EPOCH: 2 | Average Loss: 0.0538 | Current Mini-batch loss: 0.0304\n",
      "Training | EPOCH: 2 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0187\n",
      "Training | EPOCH: 2 | Average Loss: 0.3125 | Current Mini-batch loss: 0.5888\n",
      "Training | EPOCH: 2 | Average Loss: 0.1641 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 2 | Average Loss: 0.5852 | Current Mini-batch loss: 1.0062\n",
      "Training | EPOCH: 2 | Average Loss: 0.2980 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 2 | Average Loss: 0.1491 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0751 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.1473 | Current Mini-batch loss: 0.2196\n",
      "Training | EPOCH: 2 | Average Loss: 0.0740 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0389 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 2 | Average Loss: 0.5351 | Current Mini-batch loss: 1.0312\n",
      "Training | EPOCH: 2 | Average Loss: 0.2843 | Current Mini-batch loss: 0.0335\n",
      "Training | EPOCH: 2 | Average Loss: 0.1503 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 2 | Average Loss: 0.1012 | Current Mini-batch loss: 0.0521\n",
      "Training | EPOCH: 2 | Average Loss: 0.0524 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.0610 | Current Mini-batch loss: 0.0696\n",
      "Training | EPOCH: 2 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0578 | Current Mini-batch loss: 0.1113\n",
      "Training | EPOCH: 2 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0869 | Current Mini-batch loss: 0.1436\n",
      "Training | EPOCH: 2 | Average Loss: 0.0439 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 2 | Average Loss: 0.1264 | Current Mini-batch loss: 0.2391\n",
      "Training | EPOCH: 2 | Average Loss: 0.1542 | Current Mini-batch loss: 0.1820\n",
      "Training | EPOCH: 2 | Average Loss: 0.0906 | Current Mini-batch loss: 0.0270\n",
      "Training | EPOCH: 2 | Average Loss: 0.0460 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0328\n",
      "Training | EPOCH: 2 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0186\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.1227 | Current Mini-batch loss: 0.2380\n",
      "Training | EPOCH: 2 | Average Loss: 0.0615 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0313 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0213\n",
      "Training | EPOCH: 2 | Average Loss: 0.2033 | Current Mini-batch loss: 0.3804\n",
      "Training | EPOCH: 2 | Average Loss: 0.3330 | Current Mini-batch loss: 0.4627\n",
      "Training | EPOCH: 2 | Average Loss: 0.1866 | Current Mini-batch loss: 0.0402\n",
      "Training | EPOCH: 2 | Average Loss: 0.2629 | Current Mini-batch loss: 0.3393\n",
      "Training | EPOCH: 2 | Average Loss: 0.1330 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0723 | Current Mini-batch loss: 0.0116\n",
      "Training | EPOCH: 2 | Average Loss: 0.0551 | Current Mini-batch loss: 0.0380\n",
      "Training | EPOCH: 2 | Average Loss: 0.0330 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 2 | Average Loss: 0.0267 | Current Mini-batch loss: 0.0204\n",
      "Training | EPOCH: 2 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0606\n",
      "Training | EPOCH: 2 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 2 | Average Loss: 0.1632 | Current Mini-batch loss: 0.3056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0847 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.0424 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.2709 | Current Mini-batch loss: 0.4993\n",
      "Training | EPOCH: 2 | Average Loss: 0.1390 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.0699 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0271\n",
      "Training | EPOCH: 2 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0481\n",
      "Training | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 2 | Average Loss: 0.4293 | Current Mini-batch loss: 0.8442\n",
      "Training | EPOCH: 2 | Average Loss: 0.2299 | Current Mini-batch loss: 0.0306\n",
      "Training | EPOCH: 2 | Average Loss: 0.1156 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.2586 | Current Mini-batch loss: 0.4016\n",
      "Training | EPOCH: 2 | Average Loss: 0.2910 | Current Mini-batch loss: 0.3234\n",
      "Training | EPOCH: 2 | Average Loss: 0.3468 | Current Mini-batch loss: 0.4026\n",
      "Training | EPOCH: 2 | Average Loss: 0.1735 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.1158 | Current Mini-batch loss: 0.0581\n",
      "Training | EPOCH: 2 | Average Loss: 0.0598 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 2 | Average Loss: 0.0327 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.1469 | Current Mini-batch loss: 0.2773\n",
      "Training | EPOCH: 2 | Average Loss: 0.0745 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 2 | Average Loss: 0.6136 | Current Mini-batch loss: 1.1527\n",
      "Training | EPOCH: 2 | Average Loss: 0.5233 | Current Mini-batch loss: 0.4330\n",
      "Training | EPOCH: 2 | Average Loss: 0.2807 | Current Mini-batch loss: 0.0380\n",
      "Training | EPOCH: 2 | Average Loss: 0.1469 | Current Mini-batch loss: 0.0131\n",
      "Training | EPOCH: 2 | Average Loss: 0.1132 | Current Mini-batch loss: 0.0795\n",
      "Training | EPOCH: 2 | Average Loss: 0.2799 | Current Mini-batch loss: 0.4465\n",
      "Training | EPOCH: 2 | Average Loss: 0.2071 | Current Mini-batch loss: 0.1344\n",
      "Training | EPOCH: 2 | Average Loss: 0.1039 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0524 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0846 | Current Mini-batch loss: 0.1167\n",
      "Training | EPOCH: 2 | Average Loss: 0.0553 | Current Mini-batch loss: 0.0260\n",
      "Training | EPOCH: 2 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 2 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0895 | Current Mini-batch loss: 0.1698\n",
      "Training | EPOCH: 2 | Average Loss: 0.0452 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0115\n",
      "Training | EPOCH: 2 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0261 | Current Mini-batch loss: 0.0475\n",
      "Training | EPOCH: 2 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0139\n",
      "Training | EPOCH: 2 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.3047 | Current Mini-batch loss: 0.5987\n",
      "Training | EPOCH: 2 | Average Loss: 0.1825 | Current Mini-batch loss: 0.0602\n",
      "Training | EPOCH: 2 | Average Loss: 0.3160 | Current Mini-batch loss: 0.4494\n",
      "Training | EPOCH: 2 | Average Loss: 0.1582 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1349 | Current Mini-batch loss: 0.1116\n",
      "Training | EPOCH: 2 | Average Loss: 0.1849 | Current Mini-batch loss: 0.2348\n",
      "Training | EPOCH: 2 | Average Loss: 0.0926 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0490 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 2 | Average Loss: 0.2839 | Current Mini-batch loss: 0.5188\n",
      "Training | EPOCH: 2 | Average Loss: 0.1491 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 2 | Average Loss: 0.0750 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0379 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.1999 | Current Mini-batch loss: 0.3620\n",
      "Training | EPOCH: 2 | Average Loss: 0.5123 | Current Mini-batch loss: 0.8246\n",
      "Training | EPOCH: 2 | Average Loss: 0.2592 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 2 | Average Loss: 0.1404 | Current Mini-batch loss: 0.0216\n",
      "Training | EPOCH: 2 | Average Loss: 0.1303 | Current Mini-batch loss: 0.1202\n",
      "Training | EPOCH: 2 | Average Loss: 0.0652 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.1947 | Current Mini-batch loss: 0.3565\n",
      "Training | EPOCH: 2 | Average Loss: 0.0976 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.4940 | Current Mini-batch loss: 0.8903\n",
      "Training | EPOCH: 2 | Average Loss: 0.2476 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.1333 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 2 | Average Loss: 0.0669 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 2 | Average Loss: 0.2430 | Current Mini-batch loss: 0.4501\n",
      "Training | EPOCH: 2 | Average Loss: 0.3160 | Current Mini-batch loss: 0.3890\n",
      "Training | EPOCH: 2 | Average Loss: 0.1613 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 2 | Average Loss: 0.1543 | Current Mini-batch loss: 0.1473\n",
      "Training | EPOCH: 2 | Average Loss: 0.0854 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 2 | Average Loss: 0.0445 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0156\n",
      "Training | EPOCH: 2 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.4947 | Current Mini-batch loss: 0.9742\n",
      "Training | EPOCH: 2 | Average Loss: 0.2557 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 2 | Average Loss: 0.1594 | Current Mini-batch loss: 0.0632\n",
      "Training | EPOCH: 2 | Average Loss: 0.0810 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 2 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 2 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 2 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0199\n",
      "Training | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0495\n",
      "Training | EPOCH: 2 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.3341 | Current Mini-batch loss: 0.6596\n",
      "Training | EPOCH: 2 | Average Loss: 0.1676 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0899 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 2 | Average Loss: 0.2166 | Current Mini-batch loss: 0.3433\n",
      "Training | EPOCH: 2 | Average Loss: 0.1208 | Current Mini-batch loss: 0.0251\n",
      "Training | EPOCH: 2 | Average Loss: 0.0609 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0317 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.2103 | Current Mini-batch loss: 0.4146\n",
      "Training | EPOCH: 2 | Average Loss: 0.1076 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 2 | Average Loss: 0.0558 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 2 | Average Loss: 0.0299 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 2 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0323\n",
      "Training | EPOCH: 2 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 2 | Average Loss: 0.1165 | Current Mini-batch loss: 0.2201\n",
      "Training | EPOCH: 2 | Average Loss: 0.0643 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 2 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0268 | Current Mini-batch loss: 0.0212\n",
      "Training | EPOCH: 2 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.5955 | Current Mini-batch loss: 1.1839\n",
      "Training | EPOCH: 2 | Average Loss: 0.3673 | Current Mini-batch loss: 0.1392\n",
      "Training | EPOCH: 2 | Average Loss: 0.1856 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 2 | Average Loss: 0.2696 | Current Mini-batch loss: 0.3535\n",
      "Training | EPOCH: 2 | Average Loss: 0.1367 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 2 | Average Loss: 0.3017 | Current Mini-batch loss: 0.4667\n",
      "Training | EPOCH: 2 | Average Loss: 0.1529 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 2 | Average Loss: 0.0778 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0496 | Current Mini-batch loss: 0.0213\n",
      "Training | EPOCH: 2 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0375\n",
      "Training | EPOCH: 2 | Average Loss: 0.0809 | Current Mini-batch loss: 0.1184\n",
      "Training | EPOCH: 2 | Average Loss: 0.0602 | Current Mini-batch loss: 0.0394\n",
      "Training | EPOCH: 2 | Average Loss: 0.0355 | Current Mini-batch loss: 0.0108\n",
      "Training | EPOCH: 2 | Average Loss: 0.0268 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 2 | Average Loss: 0.1281 | Current Mini-batch loss: 0.2295\n",
      "Training | EPOCH: 2 | Average Loss: 0.0762 | Current Mini-batch loss: 0.0242\n",
      "Training | EPOCH: 2 | Average Loss: 0.0504 | Current Mini-batch loss: 0.0246\n",
      "Training | EPOCH: 2 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 2 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0831\n",
      "Training | EPOCH: 2 | Average Loss: 0.2653 | Current Mini-batch loss: 0.4861\n",
      "Training | EPOCH: 2 | Average Loss: 0.1376 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 2 | Average Loss: 0.0727 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 2 | Average Loss: 0.0399 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.2079 | Current Mini-batch loss: 0.3758\n",
      "Training | EPOCH: 2 | Average Loss: 0.1126 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 2 | Average Loss: 0.0593 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 2 | Average Loss: 0.0301 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0133\n",
      "Training | EPOCH: 2 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0147\n",
      "Training | EPOCH: 2 | Average Loss: 0.0913 | Current Mini-batch loss: 0.1644\n",
      "Training | EPOCH: 2 | Average Loss: 0.0467 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.1047 | Current Mini-batch loss: 0.1626\n",
      "Training | EPOCH: 2 | Average Loss: 0.0704 | Current Mini-batch loss: 0.0361\n",
      "Training | EPOCH: 2 | Average Loss: 0.0355 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 2 | Average Loss: 0.1432 | Current Mini-batch loss: 0.2817\n",
      "Training | EPOCH: 2 | Average Loss: 0.5244 | Current Mini-batch loss: 0.9057\n",
      "Training | EPOCH: 2 | Average Loss: 0.2625 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1313 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0745 | Current Mini-batch loss: 0.0177\n",
      "Training | EPOCH: 2 | Average Loss: 0.0373 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 2 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 2 | Average Loss: 0.0464 | Current Mini-batch loss: 0.0837\n",
      "Training | EPOCH: 2 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0321\n",
      "Training | EPOCH: 2 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 2 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 2 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 2 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 2 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0652\n",
      "Training | EPOCH: 2 | Average Loss: 0.0348 | Current Mini-batch loss: 0.0364\n",
      "Training | EPOCH: 2 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 2 | Average Loss: 0.0669 | Current Mini-batch loss: 0.1111\n",
      "Training | EPOCH: 2 | Average Loss: 0.0997 | Current Mini-batch loss: 0.1325\n",
      "Training | EPOCH: 2 | Average Loss: 0.0501 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0306 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 2 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 2 | Average Loss: 0.0261 | Current Mini-batch loss: 0.0464\n",
      "Training | EPOCH: 2 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 2 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 2 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0325\n",
      "Training | EPOCH: 2 | Average Loss: 0.0172 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 2 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 2 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.1663 | Current Mini-batch loss: 0.3309\n",
      "Training | EPOCH: 2 | Average Loss: 0.2011 | Current Mini-batch loss: 0.2359\n",
      "Training | EPOCH: 2 | Average Loss: 0.1006 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0507 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0432 | Current Mini-batch loss: 0.0357\n",
      "Training | EPOCH: 2 | Average Loss: 0.5382 | Current Mini-batch loss: 1.0331\n",
      "Training | EPOCH: 2 | Average Loss: 0.2698 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.1351 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0789 | Current Mini-batch loss: 0.0227\n",
      "Training | EPOCH: 2 | Average Loss: 0.0940 | Current Mini-batch loss: 0.1092\n",
      "Training | EPOCH: 2 | Average Loss: 0.0713 | Current Mini-batch loss: 0.0485\n",
      "Training | EPOCH: 2 | Average Loss: 0.0358 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 2 | Average Loss: 0.0824 | Current Mini-batch loss: 0.1419\n",
      "Training | EPOCH: 2 | Average Loss: 0.0414 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0404 | Current Mini-batch loss: 0.0394\n",
      "Training | EPOCH: 2 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0156\n",
      "Training | EPOCH: 2 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.3672 | Current Mini-batch loss: 0.7187\n",
      "Training | EPOCH: 2 | Average Loss: 0.1857 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 2 | Average Loss: 0.0934 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0871 | Current Mini-batch loss: 0.0807\n",
      "Training | EPOCH: 2 | Average Loss: 0.0440 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 2 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.7014 | Current Mini-batch loss: 1.4017\n",
      "Training | EPOCH: 2 | Average Loss: 0.3507 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.5146 | Current Mini-batch loss: 0.6784\n",
      "Training | EPOCH: 2 | Average Loss: 0.2679 | Current Mini-batch loss: 0.0212\n",
      "Training | EPOCH: 2 | Average Loss: 0.1434 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 2 | Average Loss: 0.0841 | Current Mini-batch loss: 0.0248\n",
      "Training | EPOCH: 2 | Average Loss: 0.0472 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 2 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0409\n",
      "Training | EPOCH: 2 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0321\n",
      "Training | EPOCH: 2 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 2 | Average Loss: 0.0651 | Current Mini-batch loss: 0.1185\n",
      "Training | EPOCH: 2 | Average Loss: 0.0870 | Current Mini-batch loss: 0.1088\n",
      "Training | EPOCH: 2 | Average Loss: 0.0477 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 2 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0135 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 2 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.3675 | Current Mini-batch loss: 0.7269\n",
      "Training | EPOCH: 2 | Average Loss: 0.1843 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0933 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0469 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.1074 | Current Mini-batch loss: 0.2071\n",
      "Training | EPOCH: 2 | Average Loss: 0.0542 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0897 | Current Mini-batch loss: 0.1252\n",
      "Training | EPOCH: 2 | Average Loss: 0.0460 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0681 | Current Mini-batch loss: 0.0902\n",
      "Training | EPOCH: 2 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0261 | Current Mini-batch loss: 0.0336\n",
      "Training | EPOCH: 2 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 2 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1010 | Current Mini-batch loss: 0.1938\n",
      "Training | EPOCH: 2 | Average Loss: 0.0510 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 2 | Average Loss: 0.0621 | Current Mini-batch loss: 0.0732\n",
      "Training | EPOCH: 2 | Average Loss: 0.1299 | Current Mini-batch loss: 0.1978\n",
      "Training | EPOCH: 2 | Average Loss: 0.0662 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 2 | Average Loss: 0.1095 | Current Mini-batch loss: 0.2110\n",
      "Training | EPOCH: 2 | Average Loss: 0.0725 | Current Mini-batch loss: 0.0355\n",
      "Training | EPOCH: 2 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0155\n",
      "Training | EPOCH: 2 | Average Loss: 0.3423 | Current Mini-batch loss: 0.6586\n",
      "Training | EPOCH: 2 | Average Loss: 0.2543 | Current Mini-batch loss: 0.1663\n",
      "Training | EPOCH: 2 | Average Loss: 0.1538 | Current Mini-batch loss: 0.0532\n",
      "Training | EPOCH: 2 | Average Loss: 0.0781 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.1541 | Current Mini-batch loss: 0.2301\n",
      "Training | EPOCH: 2 | Average Loss: 0.2965 | Current Mini-batch loss: 0.4388\n",
      "Training | EPOCH: 2 | Average Loss: 0.1510 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 2 | Average Loss: 0.1294 | Current Mini-batch loss: 0.1077\n",
      "Training | EPOCH: 2 | Average Loss: 0.0661 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0342 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0895 | Current Mini-batch loss: 0.1447\n",
      "Training | EPOCH: 2 | Average Loss: 0.0736 | Current Mini-batch loss: 0.0577\n",
      "Training | EPOCH: 2 | Average Loss: 0.0370 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.5332 | Current Mini-batch loss: 1.0295\n",
      "Training | EPOCH: 2 | Average Loss: 0.3522 | Current Mini-batch loss: 0.1713\n",
      "Training | EPOCH: 2 | Average Loss: 0.1765 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.1040 | Current Mini-batch loss: 0.0316\n",
      "Training | EPOCH: 2 | Average Loss: 0.0654 | Current Mini-batch loss: 0.0268\n",
      "Training | EPOCH: 2 | Average Loss: 0.0327 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0285\n",
      "Training | EPOCH: 2 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0582\n",
      "Training | EPOCH: 2 | Average Loss: 0.0600 | Current Mini-batch loss: 0.0866\n",
      "Training | EPOCH: 2 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 2 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0102\n",
      "Training | EPOCH: 2 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0215\n",
      "Training | EPOCH: 2 | Average Loss: 0.1070 | Current Mini-batch loss: 0.2002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0558 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0361 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 2 | Average Loss: 0.5159 | Current Mini-batch loss: 0.9957\n",
      "Training | EPOCH: 2 | Average Loss: 0.2606 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 2 | Average Loss: 0.1891 | Current Mini-batch loss: 0.1175\n",
      "Training | EPOCH: 2 | Average Loss: 0.1500 | Current Mini-batch loss: 0.1110\n",
      "Training | EPOCH: 2 | Average Loss: 0.1885 | Current Mini-batch loss: 0.2269\n",
      "Training | EPOCH: 2 | Average Loss: 0.0961 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 2 | Average Loss: 0.0484 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 2 | Average Loss: 0.1158 | Current Mini-batch loss: 0.2024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0944 | Current Mini-batch loss: 0.0730\n",
      "Training | EPOCH: 2 | Average Loss: 0.1196 | Current Mini-batch loss: 0.1448\n",
      "Training | EPOCH: 2 | Average Loss: 0.0602 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0314\n",
      "Training | EPOCH: 2 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 2 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 2 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.1544 | Current Mini-batch loss: 0.3034\n",
      "Training | EPOCH: 2 | Average Loss: 0.0902 | Current Mini-batch loss: 0.0260\n",
      "Training | EPOCH: 2 | Average Loss: 0.0471 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 2 | Average Loss: 0.1588 | Current Mini-batch loss: 0.2706\n",
      "Training | EPOCH: 2 | Average Loss: 0.0916 | Current Mini-batch loss: 0.0244\n",
      "Training | EPOCH: 2 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0421 | Current Mini-batch loss: 0.0383\n",
      "Training | EPOCH: 2 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0356 | Current Mini-batch loss: 0.0656\n",
      "Training | EPOCH: 2 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 2 | Average Loss: 0.0427 | Current Mini-batch loss: 0.0538\n",
      "Training | EPOCH: 2 | Average Loss: 0.4965 | Current Mini-batch loss: 0.9503\n",
      "Training | EPOCH: 2 | Average Loss: 0.3973 | Current Mini-batch loss: 0.2982\n",
      "Training | EPOCH: 2 | Average Loss: 0.2985 | Current Mini-batch loss: 0.1998\n",
      "Training | EPOCH: 2 | Average Loss: 0.2917 | Current Mini-batch loss: 0.2849\n",
      "Training | EPOCH: 2 | Average Loss: 0.1471 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.1027 | Current Mini-batch loss: 0.0584\n",
      "Training | EPOCH: 2 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 2 | Average Loss: 0.0268 | Current Mini-batch loss: 0.0446\n",
      "Training | EPOCH: 2 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.2363 | Current Mini-batch loss: 0.4558\n",
      "Training | EPOCH: 2 | Average Loss: 0.1197 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0610 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0947 | Current Mini-batch loss: 0.1284\n",
      "Training | EPOCH: 2 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0521\n",
      "Training | EPOCH: 2 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0490\n",
      "Training | EPOCH: 2 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 2 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0572\n",
      "Training | EPOCH: 2 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.0485 | Current Mini-batch loss: 0.0833\n",
      "Training | EPOCH: 2 | Average Loss: 0.0247 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0901 | Current Mini-batch loss: 0.1731\n",
      "Training | EPOCH: 2 | Average Loss: 0.3671 | Current Mini-batch loss: 0.6440\n",
      "Training | EPOCH: 2 | Average Loss: 0.2139 | Current Mini-batch loss: 0.0606\n",
      "Training | EPOCH: 2 | Average Loss: 0.2219 | Current Mini-batch loss: 0.2300\n",
      "Training | EPOCH: 2 | Average Loss: 0.4186 | Current Mini-batch loss: 0.6152\n",
      "Training | EPOCH: 2 | Average Loss: 0.2094 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.1554 | Current Mini-batch loss: 0.1013\n",
      "Training | EPOCH: 2 | Average Loss: 0.1214 | Current Mini-batch loss: 0.0874\n",
      "Training | EPOCH: 2 | Average Loss: 0.1291 | Current Mini-batch loss: 0.1369\n",
      "Training | EPOCH: 2 | Average Loss: 0.0684 | Current Mini-batch loss: 0.0077\n",
      "Training | EPOCH: 2 | Average Loss: 0.0943 | Current Mini-batch loss: 0.1202\n",
      "Training | EPOCH: 2 | Average Loss: 0.0483 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0804 | Current Mini-batch loss: 0.1356\n",
      "Training | EPOCH: 2 | Average Loss: 0.0416 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 2 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0212\n",
      "Training | EPOCH: 2 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.1801 | Current Mini-batch loss: 0.3530\n",
      "Training | EPOCH: 2 | Average Loss: 0.4198 | Current Mini-batch loss: 0.6594\n",
      "Training | EPOCH: 2 | Average Loss: 0.2819 | Current Mini-batch loss: 0.1439\n",
      "Training | EPOCH: 2 | Average Loss: 0.1411 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.1343 | Current Mini-batch loss: 0.1275\n",
      "Training | EPOCH: 2 | Average Loss: 0.0674 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.2366 | Current Mini-batch loss: 0.4709\n",
      "Training | EPOCH: 2 | Average Loss: 0.1194 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0625 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0349 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 2 | Average Loss: 0.0183 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 2 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 2 | Average Loss: 0.2322 | Current Mini-batch loss: 0.4581\n",
      "Training | EPOCH: 2 | Average Loss: 0.1163 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.3364 | Current Mini-batch loss: 0.5564\n",
      "Training | EPOCH: 2 | Average Loss: 0.2155 | Current Mini-batch loss: 0.0945\n",
      "Training | EPOCH: 2 | Average Loss: 0.1118 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 2 | Average Loss: 0.0562 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0281 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0161\n",
      "Training | EPOCH: 2 | Average Loss: 0.1297 | Current Mini-batch loss: 0.2472\n",
      "Training | EPOCH: 2 | Average Loss: 0.0659 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 2 | Average Loss: 0.0330 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 2 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 2 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0756\n",
      "Training | EPOCH: 2 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.2037 | Current Mini-batch loss: 0.4030\n",
      "Training | EPOCH: 2 | Average Loss: 0.1355 | Current Mini-batch loss: 0.0673\n",
      "Training | EPOCH: 2 | Average Loss: 0.0740 | Current Mini-batch loss: 0.0125\n",
      "Training | EPOCH: 2 | Average Loss: 0.0386 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 2 | Average Loss: 0.5001 | Current Mini-batch loss: 0.9965\n",
      "Training | EPOCH: 2 | Average Loss: 0.2507 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 2 | Average Loss: 0.1267 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 2 | Average Loss: 0.1095 | Current Mini-batch loss: 0.0924\n",
      "Training | EPOCH: 2 | Average Loss: 0.0670 | Current Mini-batch loss: 0.0244\n",
      "Training | EPOCH: 2 | Average Loss: 0.2134 | Current Mini-batch loss: 0.3599\n",
      "Training | EPOCH: 2 | Average Loss: 0.1380 | Current Mini-batch loss: 0.0627\n",
      "Training | EPOCH: 2 | Average Loss: 0.1200 | Current Mini-batch loss: 0.1020\n",
      "Training | EPOCH: 2 | Average Loss: 0.0632 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 2 | Average Loss: 0.2570 | Current Mini-batch loss: 0.4509\n",
      "Training | EPOCH: 2 | Average Loss: 0.1286 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0667 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 2 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 2 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 2 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0766\n",
      "Training | EPOCH: 2 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.2501 | Current Mini-batch loss: 0.4757\n",
      "Training | EPOCH: 2 | Average Loss: 0.1281 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 2 | Average Loss: 0.0643 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0346 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 2 | Average Loss: 0.1829 | Current Mini-batch loss: 0.3312\n",
      "Training | EPOCH: 2 | Average Loss: 0.0917 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0544 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 2 | Average Loss: 0.4283 | Current Mini-batch loss: 0.8022\n",
      "Training | EPOCH: 2 | Average Loss: 0.6710 | Current Mini-batch loss: 0.9137\n",
      "Training | EPOCH: 2 | Average Loss: 0.3356 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.1678 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0861 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 2 | Average Loss: 0.1201 | Current Mini-batch loss: 0.1542\n",
      "Training | EPOCH: 2 | Average Loss: 0.0616 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0309 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 2 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 2 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0819\n",
      "Training | EPOCH: 2 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0492 | Current Mini-batch loss: 0.0768\n",
      "Training | EPOCH: 2 | Average Loss: 0.0299 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 2 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 2 | Average Loss: 0.0309 | Current Mini-batch loss: 0.0424\n",
      "Training | EPOCH: 2 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 2 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.1200 | Current Mini-batch loss: 0.2369\n",
      "Training | EPOCH: 2 | Average Loss: 0.0637 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 2 | Average Loss: 0.0591 | Current Mini-batch loss: 0.0545\n",
      "Training | EPOCH: 2 | Average Loss: 0.0317 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 2 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0374\n",
      "Training | EPOCH: 2 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 2 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 2 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0108\n",
      "Training | EPOCH: 2 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0144\n",
      "Training | EPOCH: 2 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 2 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0341\n",
      "Training | EPOCH: 2 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0346\n",
      "Training | EPOCH: 2 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0216\n",
      "Training | EPOCH: 2 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0302\n",
      "Training | EPOCH: 2 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0097\n",
      "Training | EPOCH: 2 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 2 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0314 | Current Mini-batch loss: 0.0584\n",
      "Training | EPOCH: 2 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 2 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.1945 | Current Mini-batch loss: 0.3809\n",
      "Training | EPOCH: 2 | Average Loss: 0.0986 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 2 | Average Loss: 0.0861 | Current Mini-batch loss: 0.0736\n",
      "Training | EPOCH: 2 | Average Loss: 0.0610 | Current Mini-batch loss: 0.0359\n",
      "Training | EPOCH: 2 | Average Loss: 0.0488 | Current Mini-batch loss: 0.0365\n",
      "Training | EPOCH: 2 | Average Loss: 0.5099 | Current Mini-batch loss: 0.9711\n",
      "Training | EPOCH: 2 | Average Loss: 0.2557 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.1584 | Current Mini-batch loss: 0.0611\n",
      "Training | EPOCH: 2 | Average Loss: 0.0825 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 2 | Average Loss: 0.3427 | Current Mini-batch loss: 0.6028\n",
      "Training | EPOCH: 2 | Average Loss: 0.1764 | Current Mini-batch loss: 0.0101\n",
      "Training | EPOCH: 2 | Average Loss: 0.1274 | Current Mini-batch loss: 0.0785\n",
      "Training | EPOCH: 2 | Average Loss: 0.0645 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0390 | Current Mini-batch loss: 0.0136\n",
      "Training | EPOCH: 2 | Average Loss: 0.0267 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 2 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0421\n",
      "Training | EPOCH: 2 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 2 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.3145 | Current Mini-batch loss: 0.6194\n",
      "Training | EPOCH: 2 | Average Loss: 0.1575 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0790 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0509 | Current Mini-batch loss: 0.0229\n",
      "Training | EPOCH: 2 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0219\n",
      "Training | EPOCH: 2 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 2 | Average Loss: 0.0845 | Current Mini-batch loss: 0.1481\n",
      "Training | EPOCH: 2 | Average Loss: 0.0431 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 2 | Average Loss: 0.0293 | Current Mini-batch loss: 0.0155\n",
      "Training | EPOCH: 2 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 2 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 2 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 2 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0223\n",
      "Training | EPOCH: 2 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0112\n",
      "Training | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0369\n",
      "Training | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 2 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 2 | Average Loss: 0.0481 | Current Mini-batch loss: 0.0873\n",
      "Training | EPOCH: 2 | Average Loss: 0.0245 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0309\n",
      "Training | EPOCH: 2 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 2 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 2 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 2 | Average Loss: 0.0666 | Current Mini-batch loss: 0.1264\n",
      "Training | EPOCH: 2 | Average Loss: 0.0542 | Current Mini-batch loss: 0.0417\n",
      "Training | EPOCH: 2 | Average Loss: 0.0289 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 2 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 2 | Average Loss: 0.0515 | Current Mini-batch loss: 0.0948\n",
      "Training | EPOCH: 2 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.5805 | Current Mini-batch loss: 1.1350\n",
      "Training | EPOCH: 2 | Average Loss: 0.2905 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1566 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 2 | Average Loss: 0.0851 | Current Mini-batch loss: 0.0135\n",
      "Training | EPOCH: 2 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 2 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0125\n",
      "Training | EPOCH: 2 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 2 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0251\n",
      "Training | EPOCH: 2 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0777\n",
      "Training | EPOCH: 2 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 2 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0249\n",
      "Training | EPOCH: 2 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 2 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0324\n",
      "Training | EPOCH: 2 | Average Loss: 0.0343 | Current Mini-batch loss: 0.0480\n",
      "Training | EPOCH: 2 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 2 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 2 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0427 | Current Mini-batch loss: 0.0838\n",
      "Training | EPOCH: 2 | Average Loss: 0.0257 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 2 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 2 | Average Loss: 0.2885 | Current Mini-batch loss: 0.5675\n",
      "Training | EPOCH: 2 | Average Loss: 0.1494 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 2 | Average Loss: 0.0756 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0379 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 2 | Average Loss: 0.0484 | Current Mini-batch loss: 0.0778\n",
      "Training | EPOCH: 2 | Average Loss: 0.0324 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 2 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 2 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0358\n",
      "Training | EPOCH: 2 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 2 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 2 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0208\n",
      "Training | EPOCH: 2 | Average Loss: 0.0882 | Current Mini-batch loss: 0.1607\n",
      "Training | EPOCH: 2 | Average Loss: 0.1309 | Current Mini-batch loss: 0.1736\n",
      "Training | EPOCH: 2 | Average Loss: 0.0838 | Current Mini-batch loss: 0.0367\n",
      "Training | EPOCH: 2 | Average Loss: 0.0419 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0334 | Current Mini-batch loss: 0.0249\n",
      "Training | EPOCH: 2 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0098\n",
      "Training | EPOCH: 2 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0343\n",
      "Training | EPOCH: 2 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0195\n",
      "Training | EPOCH: 2 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0184\n",
      "Training | EPOCH: 2 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 2 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0446\n",
      "Training | EPOCH: 2 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 2 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0321\n",
      "Training | EPOCH: 2 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 2 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0203\n",
      "Training | EPOCH: 2 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 2 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0237\n",
      "Training | EPOCH: 2 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 2 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 2 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0164 | Current Mini-batch loss: 0.0310\n",
      "Training | EPOCH: 2 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0353\n",
      "Training | EPOCH: 2 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 2 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 2 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 2 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 2 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 2 | Average Loss: 0.1834 | Current Mini-batch loss: 0.3650\n",
      "Training | EPOCH: 2 | Average Loss: 0.0929 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 2 | Average Loss: 0.0526 | Current Mini-batch loss: 0.0123\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1846 | Current Mini-batch loss: 0.1846\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0924 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0021\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0237 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0029\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0040\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0041\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0053\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0027\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0262\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0042\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0017\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1678 | Current Mini-batch loss: 0.3337\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0861 | Current Mini-batch loss: 0.0044\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0442 | Current Mini-batch loss: 0.0024\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0111\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0030\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0060\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0189\n",
      "Validation | EPOCH: 2 | Average Loss: 0.2071 | Current Mini-batch loss: 0.4029\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1039 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0521 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0046\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0167\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0462\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0345 | Current Mini-batch loss: 0.0514\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0019\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0264\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0092\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0128\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0045\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0791 | Current Mini-batch loss: 0.1564\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1860 | Current Mini-batch loss: 0.2929\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1787 | Current Mini-batch loss: 0.2638\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0929 | Current Mini-batch loss: 0.0071\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1425 | Current Mini-batch loss: 0.1920\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1424 | Current Mini-batch loss: 0.1424\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0715 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0773 | Current Mini-batch loss: 0.0831\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0282 | Current Mini-batch loss: 0.0171\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0127\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0094\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0021\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0438 | Current Mini-batch loss: 0.0858\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0295\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0347\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0465\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0027\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0074\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0067\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0108\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0024\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0048\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0045\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0099\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0030\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0272\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0270\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0068\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0032\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 2 | Average Loss: 0.3758 | Current Mini-batch loss: 0.7494\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1881 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0947 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0480 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0033\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0132\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0045\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1136 | Current Mini-batch loss: 0.2247\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0572 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0306 | Current Mini-batch loss: 0.0039\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0017\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0196\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0047\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0071\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0916 | Current Mini-batch loss: 0.1828\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0476 | Current Mini-batch loss: 0.0037\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0024\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0026\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0558 | Current Mini-batch loss: 0.1093\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1803 | Current Mini-batch loss: 0.3464\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0906 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0090\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0045\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0112\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0041\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0051\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0073\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0172\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0026\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0148\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0070\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0235\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1157 | Current Mini-batch loss: 0.2289\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0609 | Current Mini-batch loss: 0.0062\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0036\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0055\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0040\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0187\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0232\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0541 | Current Mini-batch loss: 0.0939\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0274 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 2 | Average Loss: 0.3169 | Current Mini-batch loss: 0.6064\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1592 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 2 | Average Loss: 0.1307 | Current Mini-batch loss: 0.1023\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0809 | Current Mini-batch loss: 0.0311\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0059\n",
      "Validation | EPOCH: 2 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.5014 | Current Mini-batch loss: 1.0019\n",
      "Training | EPOCH: 3 | Average Loss: 1.5952 | Current Mini-batch loss: 2.6890\n",
      "Training | EPOCH: 3 | Average Loss: 0.8145 | Current Mini-batch loss: 0.0337\n",
      "Training | EPOCH: 3 | Average Loss: 0.4073 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.2057 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 3 | Average Loss: 0.2045 | Current Mini-batch loss: 0.2034\n",
      "Training | EPOCH: 3 | Average Loss: 0.1025 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 3 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 3 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0628 | Current Mini-batch loss: 0.1226\n",
      "Training | EPOCH: 3 | Average Loss: 0.0320 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 3 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0331\n",
      "Training | EPOCH: 3 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 3 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 3 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 3 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.2178 | Current Mini-batch loss: 0.4296\n",
      "Training | EPOCH: 3 | Average Loss: 0.1342 | Current Mini-batch loss: 0.0507\n",
      "Training | EPOCH: 3 | Average Loss: 0.0684 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0468 | Current Mini-batch loss: 0.0252\n",
      "Training | EPOCH: 3 | Average Loss: 0.1559 | Current Mini-batch loss: 0.2651\n",
      "Training | EPOCH: 3 | Average Loss: 0.0780 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0518\n",
      "Training | EPOCH: 3 | Average Loss: 0.0325 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0613 | Current Mini-batch loss: 0.0901\n",
      "Training | EPOCH: 3 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.1553 | Current Mini-batch loss: 0.2790\n",
      "Training | EPOCH: 3 | Average Loss: 0.4437 | Current Mini-batch loss: 0.7321\n",
      "Training | EPOCH: 3 | Average Loss: 0.2612 | Current Mini-batch loss: 0.0786\n",
      "Training | EPOCH: 3 | Average Loss: 0.3515 | Current Mini-batch loss: 0.4419\n",
      "Training | EPOCH: 3 | Average Loss: 0.1776 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0907 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 3 | Average Loss: 0.0455 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0777 | Current Mini-batch loss: 0.1327\n",
      "Training | EPOCH: 3 | Average Loss: 0.0390 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0378 | Current Mini-batch loss: 0.0547\n",
      "Training | EPOCH: 3 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 3 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.2048 | Current Mini-batch loss: 0.3994\n",
      "Training | EPOCH: 3 | Average Loss: 0.1324 | Current Mini-batch loss: 0.0600\n",
      "Training | EPOCH: 3 | Average Loss: 0.0662 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0544 | Current Mini-batch loss: 0.0756\n",
      "Training | EPOCH: 3 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0246\n",
      "Training | EPOCH: 3 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0456 | Current Mini-batch loss: 0.0715\n",
      "Training | EPOCH: 3 | Average Loss: 0.0657 | Current Mini-batch loss: 0.0858\n",
      "Training | EPOCH: 3 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.5898 | Current Mini-batch loss: 1.1465\n",
      "Training | EPOCH: 3 | Average Loss: 0.3095 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 3 | Average Loss: 0.1622 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 3 | Average Loss: 0.0814 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0407 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0279 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 3 | Average Loss: 0.0372 | Current Mini-batch loss: 0.0465\n",
      "Training | EPOCH: 3 | Average Loss: 0.1514 | Current Mini-batch loss: 0.2656\n",
      "Training | EPOCH: 3 | Average Loss: 0.0769 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0403 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 3 | Average Loss: 0.0481 | Current Mini-batch loss: 0.0915\n",
      "Training | EPOCH: 3 | Average Loss: 0.0242 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 3 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 3 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0430 | Current Mini-batch loss: 0.0849\n",
      "Training | EPOCH: 3 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.2072 | Current Mini-batch loss: 0.4125\n",
      "Training | EPOCH: 3 | Average Loss: 0.1040 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0597 | Current Mini-batch loss: 0.0154\n",
      "Training | EPOCH: 3 | Average Loss: 0.0521 | Current Mini-batch loss: 0.0445\n",
      "Training | EPOCH: 3 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0257 | Current Mini-batch loss: 0.0379\n",
      "Training | EPOCH: 3 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0112\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 3 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 3 | Average Loss: 0.0269 | Current Mini-batch loss: 0.0492\n",
      "Training | EPOCH: 3 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0311\n",
      "Training | EPOCH: 3 | Average Loss: 0.2334 | Current Mini-batch loss: 0.4476\n",
      "Training | EPOCH: 3 | Average Loss: 0.2465 | Current Mini-batch loss: 0.2595\n",
      "Training | EPOCH: 3 | Average Loss: 0.1234 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 3 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.0409 | Current Mini-batch loss: 0.0480\n",
      "Training | EPOCH: 3 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 3 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0397 | Current Mini-batch loss: 0.0751\n",
      "Training | EPOCH: 3 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0249\n",
      "Training | EPOCH: 3 | Average Loss: 0.0682 | Current Mini-batch loss: 0.1041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0342 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0172 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0663 | Current Mini-batch loss: 0.1155\n",
      "Training | EPOCH: 3 | Average Loss: 0.0396 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 3 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 3 | Average Loss: 0.1493 | Current Mini-batch loss: 0.2774\n",
      "Training | EPOCH: 3 | Average Loss: 0.1125 | Current Mini-batch loss: 0.0757\n",
      "Training | EPOCH: 3 | Average Loss: 0.0566 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0285 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0632\n",
      "Training | EPOCH: 3 | Average Loss: 0.0553 | Current Mini-batch loss: 0.0648\n",
      "Training | EPOCH: 3 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.2136 | Current Mini-batch loss: 0.3995\n",
      "Training | EPOCH: 3 | Average Loss: 0.1068 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0767 | Current Mini-batch loss: 0.0467\n",
      "Training | EPOCH: 3 | Average Loss: 0.0384 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.2409 | Current Mini-batch loss: 0.4433\n",
      "Training | EPOCH: 3 | Average Loss: 0.1335 | Current Mini-batch loss: 0.0261\n",
      "Training | EPOCH: 3 | Average Loss: 0.1170 | Current Mini-batch loss: 0.1005\n",
      "Training | EPOCH: 3 | Average Loss: 0.2197 | Current Mini-batch loss: 0.3224\n",
      "Training | EPOCH: 3 | Average Loss: 0.1194 | Current Mini-batch loss: 0.0191\n",
      "Training | EPOCH: 3 | Average Loss: 0.0598 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1049 | Current Mini-batch loss: 0.1499\n",
      "Training | EPOCH: 3 | Average Loss: 0.1202 | Current Mini-batch loss: 0.1355\n",
      "Training | EPOCH: 3 | Average Loss: 0.1622 | Current Mini-batch loss: 0.2043\n",
      "Training | EPOCH: 3 | Average Loss: 0.1165 | Current Mini-batch loss: 0.0708\n",
      "Training | EPOCH: 3 | Average Loss: 0.0584 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0825 | Current Mini-batch loss: 0.1066\n",
      "Training | EPOCH: 3 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 3 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 3 | Average Loss: 0.7306 | Current Mini-batch loss: 1.4593\n",
      "Training | EPOCH: 3 | Average Loss: 0.5644 | Current Mini-batch loss: 0.3983\n",
      "Training | EPOCH: 3 | Average Loss: 0.4381 | Current Mini-batch loss: 0.3117\n",
      "Training | EPOCH: 3 | Average Loss: 0.3466 | Current Mini-batch loss: 0.2551\n",
      "Training | EPOCH: 3 | Average Loss: 0.1739 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0972 | Current Mini-batch loss: 0.0205\n",
      "Training | EPOCH: 3 | Average Loss: 0.0487 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1769 | Current Mini-batch loss: 0.3051\n",
      "Training | EPOCH: 3 | Average Loss: 0.0888 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0482 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 3 | Average Loss: 0.6059 | Current Mini-batch loss: 1.1636\n",
      "Training | EPOCH: 3 | Average Loss: 0.4894 | Current Mini-batch loss: 0.3729\n",
      "Training | EPOCH: 3 | Average Loss: 0.2449 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.1228 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0622 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0365 | Current Mini-batch loss: 0.0108\n",
      "Training | EPOCH: 3 | Average Loss: 0.0183 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 3 | Average Loss: 0.0898 | Current Mini-batch loss: 0.1745\n",
      "Training | EPOCH: 3 | Average Loss: 0.1216 | Current Mini-batch loss: 0.1533\n",
      "Training | EPOCH: 3 | Average Loss: 0.9581 | Current Mini-batch loss: 1.7946\n",
      "Training | EPOCH: 3 | Average Loss: 0.4799 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.3036 | Current Mini-batch loss: 0.1273\n",
      "Training | EPOCH: 3 | Average Loss: 0.1521 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0770 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0429 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 3 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 3 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 3 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 3 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.1545 | Current Mini-batch loss: 0.3039\n",
      "Training | EPOCH: 3 | Average Loss: 0.0797 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 3 | Average Loss: 0.0420 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.1096 | Current Mini-batch loss: 0.1773\n",
      "Training | EPOCH: 3 | Average Loss: 0.1101 | Current Mini-batch loss: 0.1106\n",
      "Training | EPOCH: 3 | Average Loss: 0.0576 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 3 | Average Loss: 0.0692 | Current Mini-batch loss: 0.0809\n",
      "Training | EPOCH: 3 | Average Loss: 0.0347 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.1036 | Current Mini-batch loss: 0.1724\n",
      "Training | EPOCH: 3 | Average Loss: 0.0781 | Current Mini-batch loss: 0.0526\n",
      "Training | EPOCH: 3 | Average Loss: 0.0767 | Current Mini-batch loss: 0.0754\n",
      "Training | EPOCH: 3 | Average Loss: 0.1149 | Current Mini-batch loss: 0.1531\n",
      "Training | EPOCH: 3 | Average Loss: 0.0575 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0289 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 3 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 3 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 3 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0405 | Current Mini-batch loss: 0.0803\n",
      "Training | EPOCH: 3 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1430 | Current Mini-batch loss: 0.2657\n",
      "Training | EPOCH: 3 | Average Loss: 0.1026 | Current Mini-batch loss: 0.0621\n",
      "Training | EPOCH: 3 | Average Loss: 0.0520 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.3921 | Current Mini-batch loss: 0.7323\n",
      "Training | EPOCH: 3 | Average Loss: 0.4510 | Current Mini-batch loss: 0.5099\n",
      "Training | EPOCH: 3 | Average Loss: 0.2283 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 3 | Average Loss: 0.1142 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0572 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0183 | Current Mini-batch loss: 0.0222\n",
      "Training | EPOCH: 3 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0231\n",
      "Training | EPOCH: 3 | Average Loss: 0.1270 | Current Mini-batch loss: 0.2333\n",
      "Training | EPOCH: 3 | Average Loss: 0.2029 | Current Mini-batch loss: 0.2787\n",
      "Training | EPOCH: 3 | Average Loss: 0.1074 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 3 | Average Loss: 0.0537 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0478 | Current Mini-batch loss: 0.0419\n",
      "Training | EPOCH: 3 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 3 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.4245 | Current Mini-batch loss: 0.8319\n",
      "Training | EPOCH: 3 | Average Loss: 0.2875 | Current Mini-batch loss: 0.1506\n",
      "Training | EPOCH: 3 | Average Loss: 0.2971 | Current Mini-batch loss: 0.3067\n",
      "Training | EPOCH: 3 | Average Loss: 0.1489 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0782 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 3 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 3 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0238\n",
      "Training | EPOCH: 3 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0218\n",
      "Training | EPOCH: 3 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 3 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0301\n",
      "Training | EPOCH: 3 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0217 | Current Mini-batch loss: 0.0336\n",
      "Training | EPOCH: 3 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0568 | Current Mini-batch loss: 0.1025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0661 | Current Mini-batch loss: 0.0755\n",
      "Training | EPOCH: 3 | Average Loss: 0.0332 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0170 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0101\n",
      "Training | EPOCH: 3 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0424\n",
      "Training | EPOCH: 3 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0423\n",
      "Training | EPOCH: 3 | Average Loss: 0.0195 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 3 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0257 | Current Mini-batch loss: 0.0414\n",
      "Training | EPOCH: 3 | Average Loss: 0.0237 | Current Mini-batch loss: 0.0218\n",
      "Training | EPOCH: 3 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 3 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 3 | Average Loss: 0.0535 | Current Mini-batch loss: 0.1015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0343 | Current Mini-batch loss: 0.0152\n",
      "Training | EPOCH: 3 | Average Loss: 0.0226 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 3 | Average Loss: 0.0922 | Current Mini-batch loss: 0.1617\n",
      "Training | EPOCH: 3 | Average Loss: 0.0463 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0887 | Current Mini-batch loss: 0.1310\n",
      "Training | EPOCH: 3 | Average Loss: 0.0453 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0251 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 3 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 3 | Average Loss: 0.0375 | Current Mini-batch loss: 0.0612\n",
      "Training | EPOCH: 3 | Average Loss: 0.1198 | Current Mini-batch loss: 0.2021\n",
      "Training | EPOCH: 3 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0099\n",
      "Training | EPOCH: 3 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 3 | Average Loss: 0.0572 | Current Mini-batch loss: 0.0803\n",
      "Training | EPOCH: 3 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 3 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 3 | Average Loss: 0.0682 | Current Mini-batch loss: 0.1253\n",
      "Training | EPOCH: 3 | Average Loss: 0.7455 | Current Mini-batch loss: 1.4229\n",
      "Training | EPOCH: 3 | Average Loss: 0.3778 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 3 | Average Loss: 0.3863 | Current Mini-batch loss: 0.3948\n",
      "Training | EPOCH: 3 | Average Loss: 0.1964 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 3 | Average Loss: 0.0986 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0495 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0874 | Current Mini-batch loss: 0.1495\n",
      "Training | EPOCH: 3 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 3 | Average Loss: 0.0456 | Current Mini-batch loss: 0.0390\n",
      "Training | EPOCH: 3 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 3 | Average Loss: 0.0369 | Current Mini-batch loss: 0.0476\n",
      "Training | EPOCH: 3 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0185\n",
      "Training | EPOCH: 3 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 3 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 3 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 3 | Average Loss: 0.2694 | Current Mini-batch loss: 0.5290\n",
      "Training | EPOCH: 3 | Average Loss: 0.1368 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.3166 | Current Mini-batch loss: 0.4965\n",
      "Training | EPOCH: 3 | Average Loss: 0.1594 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0911 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 3 | Average Loss: 0.0644 | Current Mini-batch loss: 0.0377\n",
      "Training | EPOCH: 3 | Average Loss: 0.0736 | Current Mini-batch loss: 0.0829\n",
      "Training | EPOCH: 3 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 3 | Average Loss: 0.2394 | Current Mini-batch loss: 0.4409\n",
      "Training | EPOCH: 3 | Average Loss: 0.1198 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0606 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 3 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0210\n",
      "Training | EPOCH: 3 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 3 | Average Loss: 0.0478 | Current Mini-batch loss: 0.0825\n",
      "Training | EPOCH: 3 | Average Loss: 0.9345 | Current Mini-batch loss: 1.8212\n",
      "Training | EPOCH: 3 | Average Loss: 1.0633 | Current Mini-batch loss: 1.1922\n",
      "Training | EPOCH: 3 | Average Loss: 0.5321 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.2708 | Current Mini-batch loss: 0.0094\n",
      "Training | EPOCH: 3 | Average Loss: 0.1357 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0699 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.2272 | Current Mini-batch loss: 0.4194\n",
      "Training | EPOCH: 3 | Average Loss: 0.1146 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.1049 | Current Mini-batch loss: 0.0952\n",
      "Training | EPOCH: 3 | Average Loss: 0.0568 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 3 | Average Loss: 0.0327 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 3 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0197\n",
      "Training | EPOCH: 3 | Average Loss: 0.2381 | Current Mini-batch loss: 0.4499\n",
      "Training | EPOCH: 3 | Average Loss: 0.1192 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0600 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.3748 | Current Mini-batch loss: 0.6896\n",
      "Training | EPOCH: 3 | Average Loss: 0.1880 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.1429 | Current Mini-batch loss: 0.0977\n",
      "Training | EPOCH: 3 | Average Loss: 0.1087 | Current Mini-batch loss: 0.0746\n",
      "Training | EPOCH: 3 | Average Loss: 0.0546 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0449 | Current Mini-batch loss: 0.0351\n",
      "Training | EPOCH: 3 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 3 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0196\n",
      "Training | EPOCH: 3 | Average Loss: 0.0466 | Current Mini-batch loss: 0.0818\n",
      "Training | EPOCH: 3 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 3 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 3 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 3 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 3 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0321\n",
      "Training | EPOCH: 3 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 3 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0413\n",
      "Training | EPOCH: 3 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 3 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 3 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0152\n",
      "Training | EPOCH: 3 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 3 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0101\n",
      "Training | EPOCH: 3 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.2580 | Current Mini-batch loss: 0.5136\n",
      "Training | EPOCH: 3 | Average Loss: 0.2879 | Current Mini-batch loss: 0.3177\n",
      "Training | EPOCH: 3 | Average Loss: 0.3397 | Current Mini-batch loss: 0.3915\n",
      "Training | EPOCH: 3 | Average Loss: 0.1699 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1298 | Current Mini-batch loss: 0.0898\n",
      "Training | EPOCH: 3 | Average Loss: 0.0659 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.2460 | Current Mini-batch loss: 0.4262\n",
      "Training | EPOCH: 3 | Average Loss: 0.1570 | Current Mini-batch loss: 0.0680\n",
      "Training | EPOCH: 3 | Average Loss: 0.1747 | Current Mini-batch loss: 0.1925\n",
      "Training | EPOCH: 3 | Average Loss: 0.0879 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.1569 | Current Mini-batch loss: 0.2258\n",
      "Training | EPOCH: 3 | Average Loss: 0.0809 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 3 | Average Loss: 0.2622 | Current Mini-batch loss: 0.4434\n",
      "Training | EPOCH: 3 | Average Loss: 0.1314 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0788 | Current Mini-batch loss: 0.0263\n",
      "Training | EPOCH: 3 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0308 | Current Mini-batch loss: 0.0221\n",
      "Training | EPOCH: 3 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0427\n",
      "Training | EPOCH: 3 | Average Loss: 0.0167 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 3 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0305\n",
      "Training | EPOCH: 3 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0248\n",
      "Training | EPOCH: 3 | Average Loss: 0.2691 | Current Mini-batch loss: 0.5174\n",
      "Training | EPOCH: 3 | Average Loss: 0.2564 | Current Mini-batch loss: 0.2438\n",
      "Training | EPOCH: 3 | Average Loss: 0.4080 | Current Mini-batch loss: 0.5596\n",
      "Training | EPOCH: 3 | Average Loss: 0.2078 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 3 | Average Loss: 0.1068 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0650 | Current Mini-batch loss: 0.0231\n",
      "Training | EPOCH: 3 | Average Loss: 0.0515 | Current Mini-batch loss: 0.0379\n",
      "Training | EPOCH: 3 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 3 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.1021 | Current Mini-batch loss: 0.1901\n",
      "Training | EPOCH: 3 | Average Loss: 0.0519 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0268 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 3 | Average Loss: 0.0279 | Current Mini-batch loss: 0.0451\n",
      "Training | EPOCH: 3 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.3474 | Current Mini-batch loss: 0.6874\n",
      "Training | EPOCH: 3 | Average Loss: 0.4784 | Current Mini-batch loss: 0.6094\n",
      "Training | EPOCH: 3 | Average Loss: 0.3338 | Current Mini-batch loss: 0.1893\n",
      "Training | EPOCH: 3 | Average Loss: 0.1993 | Current Mini-batch loss: 0.0648\n",
      "Training | EPOCH: 3 | Average Loss: 0.1089 | Current Mini-batch loss: 0.0184\n",
      "Training | EPOCH: 3 | Average Loss: 0.1504 | Current Mini-batch loss: 0.1920\n",
      "Training | EPOCH: 3 | Average Loss: 0.3150 | Current Mini-batch loss: 0.4797\n",
      "Training | EPOCH: 3 | Average Loss: 0.1575 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.5087 | Current Mini-batch loss: 0.8598\n",
      "Training | EPOCH: 3 | Average Loss: 0.2694 | Current Mini-batch loss: 0.0301\n",
      "Training | EPOCH: 3 | Average Loss: 0.1350 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.1231 | Current Mini-batch loss: 0.1112\n",
      "Training | EPOCH: 3 | Average Loss: 0.1401 | Current Mini-batch loss: 0.1570\n",
      "Training | EPOCH: 3 | Average Loss: 0.0702 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0883 | Current Mini-batch loss: 0.1065\n",
      "Training | EPOCH: 3 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0344\n",
      "Training | EPOCH: 3 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.3053 | Current Mini-batch loss: 0.6048\n",
      "Training | EPOCH: 3 | Average Loss: 0.1534 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0810 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 3 | Average Loss: 0.1250 | Current Mini-batch loss: 0.1691\n",
      "Training | EPOCH: 3 | Average Loss: 0.1058 | Current Mini-batch loss: 0.0865\n",
      "Training | EPOCH: 3 | Average Loss: 0.0953 | Current Mini-batch loss: 0.0849\n",
      "Training | EPOCH: 3 | Average Loss: 0.0480 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0247 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0793 | Current Mini-batch loss: 0.1443\n",
      "Training | EPOCH: 3 | Average Loss: 0.1179 | Current Mini-batch loss: 0.1566\n",
      "Training | EPOCH: 3 | Average Loss: 0.0657 | Current Mini-batch loss: 0.0134\n",
      "Training | EPOCH: 3 | Average Loss: 0.0328 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.1178 | Current Mini-batch loss: 0.2027\n",
      "Training | EPOCH: 3 | Average Loss: 0.1078 | Current Mini-batch loss: 0.0978\n",
      "Training | EPOCH: 3 | Average Loss: 0.0539 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0470 | Current Mini-batch loss: 0.0670\n",
      "Training | EPOCH: 3 | Average Loss: 0.0240 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0183\n",
      "Training | EPOCH: 3 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 3 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.1317 | Current Mini-batch loss: 0.2614\n",
      "Training | EPOCH: 3 | Average Loss: 0.1547 | Current Mini-batch loss: 0.1776\n",
      "Training | EPOCH: 3 | Average Loss: 0.0779 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0779 | Current Mini-batch loss: 0.0779\n",
      "Training | EPOCH: 3 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.6231 | Current Mini-batch loss: 1.2070\n",
      "Training | EPOCH: 3 | Average Loss: 0.3126 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 3 | Average Loss: 0.2506 | Current Mini-batch loss: 0.1885\n",
      "Training | EPOCH: 3 | Average Loss: 0.1262 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0710 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 3 | Average Loss: 0.0379 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 3 | Average Loss: 0.0190 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 3 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0264\n",
      "Training | EPOCH: 3 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0382\n",
      "Training | EPOCH: 3 | Average Loss: 0.0541 | Current Mini-batch loss: 0.0819\n",
      "Training | EPOCH: 3 | Average Loss: 0.0271 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 3 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0249\n",
      "Training | EPOCH: 3 | Average Loss: 0.1870 | Current Mini-batch loss: 0.3608\n",
      "Training | EPOCH: 3 | Average Loss: 0.0981 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 3 | Average Loss: 0.0660 | Current Mini-batch loss: 0.0339\n",
      "Training | EPOCH: 3 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0436 | Current Mini-batch loss: 0.0535\n",
      "Training | EPOCH: 3 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 3 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 3 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0072\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 3 | Average Loss: 0.0390 | Current Mini-batch loss: 0.0700\n",
      "Training | EPOCH: 3 | Average Loss: 0.0424 | Current Mini-batch loss: 0.0457\n",
      "Training | EPOCH: 3 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0234\n",
      "Training | EPOCH: 3 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0072\n",
      "Training | EPOCH: 3 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0421\n",
      "Training | EPOCH: 3 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0647 | Current Mini-batch loss: 0.1232\n",
      "Training | EPOCH: 3 | Average Loss: 0.0443 | Current Mini-batch loss: 0.0239\n",
      "Training | EPOCH: 3 | Average Loss: 0.0464 | Current Mini-batch loss: 0.0484\n",
      "Training | EPOCH: 3 | Average Loss: 0.0257 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 3 | Average Loss: 0.0464 | Current Mini-batch loss: 0.0672\n",
      "Training | EPOCH: 3 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 3 | Average Loss: 0.0741 | Current Mini-batch loss: 0.1399\n",
      "Training | EPOCH: 3 | Average Loss: 0.0371 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0178\n",
      "Training | EPOCH: 3 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 3 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.2865 | Current Mini-batch loss: 0.5710\n",
      "Training | EPOCH: 3 | Average Loss: 0.1484 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 3 | Average Loss: 0.1754 | Current Mini-batch loss: 0.2023\n",
      "Training | EPOCH: 3 | Average Loss: 0.0884 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.2693 | Current Mini-batch loss: 0.4502\n",
      "Training | EPOCH: 3 | Average Loss: 0.1352 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0708 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 3 | Average Loss: 0.1023 | Current Mini-batch loss: 0.1337\n",
      "Training | EPOCH: 3 | Average Loss: 0.0512 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.1003 | Current Mini-batch loss: 0.1747\n",
      "Training | EPOCH: 3 | Average Loss: 0.0504 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 3 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0094\n",
      "Training | EPOCH: 3 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.3812 | Current Mini-batch loss: 0.7611\n",
      "Training | EPOCH: 3 | Average Loss: 0.1923 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 3 | Average Loss: 0.1314 | Current Mini-batch loss: 0.0704\n",
      "Training | EPOCH: 3 | Average Loss: 0.2708 | Current Mini-batch loss: 0.4103\n",
      "Training | EPOCH: 3 | Average Loss: 0.1355 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0743 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 3 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 3 | Average Loss: 0.0286 | Current Mini-batch loss: 0.0127\n",
      "Training | EPOCH: 3 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0309 | Current Mini-batch loss: 0.0475\n",
      "Training | EPOCH: 3 | Average Loss: 0.1913 | Current Mini-batch loss: 0.3517\n",
      "Training | EPOCH: 3 | Average Loss: 0.1042 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 3 | Average Loss: 0.0524 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0238\n",
      "Training | EPOCH: 3 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.6468 | Current Mini-batch loss: 1.2894\n",
      "Training | EPOCH: 3 | Average Loss: 1.1417 | Current Mini-batch loss: 1.6366\n",
      "Training | EPOCH: 3 | Average Loss: 0.5994 | Current Mini-batch loss: 0.0570\n",
      "Training | EPOCH: 3 | Average Loss: 0.3009 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.1984 | Current Mini-batch loss: 0.0960\n",
      "Training | EPOCH: 3 | Average Loss: 0.1047 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 3 | Average Loss: 0.0532 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0811 | Current Mini-batch loss: 0.1091\n",
      "Training | EPOCH: 3 | Average Loss: 0.0599 | Current Mini-batch loss: 0.0387\n",
      "Training | EPOCH: 3 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 3 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0083\n",
      "Training | EPOCH: 3 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 3 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0297\n",
      "Training | EPOCH: 3 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.1256 | Current Mini-batch loss: 0.2471\n",
      "Training | EPOCH: 3 | Average Loss: 0.0630 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0996 | Current Mini-batch loss: 0.1363\n",
      "Training | EPOCH: 3 | Average Loss: 0.0572 | Current Mini-batch loss: 0.0148\n",
      "Training | EPOCH: 3 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 3 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 3 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.1956 | Current Mini-batch loss: 0.3832\n",
      "Training | EPOCH: 3 | Average Loss: 0.0987 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0797 | Current Mini-batch loss: 0.0607\n",
      "Training | EPOCH: 3 | Average Loss: 0.0747 | Current Mini-batch loss: 0.0697\n",
      "Training | EPOCH: 3 | Average Loss: 0.4696 | Current Mini-batch loss: 0.8644\n",
      "Training | EPOCH: 3 | Average Loss: 0.2348 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.1318 | Current Mini-batch loss: 0.0288\n",
      "Training | EPOCH: 3 | Average Loss: 0.0663 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0383 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 3 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0342\n",
      "Training | EPOCH: 3 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 3 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 3 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 3 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 3 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 3 | Average Loss: 0.0445 | Current Mini-batch loss: 0.0860\n",
      "Training | EPOCH: 3 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0072\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0454 | Current Mini-batch loss: 0.0881\n",
      "Training | EPOCH: 3 | Average Loss: 0.1019 | Current Mini-batch loss: 0.1583\n",
      "Training | EPOCH: 3 | Average Loss: 0.1386 | Current Mini-batch loss: 0.1754\n",
      "Training | EPOCH: 3 | Average Loss: 0.0817 | Current Mini-batch loss: 0.0247\n",
      "Training | EPOCH: 3 | Average Loss: 0.0456 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 3 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0830\n",
      "Training | EPOCH: 3 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0457\n",
      "Training | EPOCH: 3 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0356\n",
      "Training | EPOCH: 3 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 3 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 3 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 3 | Average Loss: 0.2999 | Current Mini-batch loss: 0.5907\n",
      "Training | EPOCH: 3 | Average Loss: 0.2004 | Current Mini-batch loss: 0.1009\n",
      "Training | EPOCH: 3 | Average Loss: 0.1002 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0960 | Current Mini-batch loss: 0.0919\n",
      "Training | EPOCH: 3 | Average Loss: 0.0483 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0242 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0853 | Current Mini-batch loss: 0.1464\n",
      "Training | EPOCH: 3 | Average Loss: 0.0428 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.1278 | Current Mini-batch loss: 0.2341\n",
      "Training | EPOCH: 3 | Average Loss: 0.0640 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0320 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0291\n",
      "Training | EPOCH: 3 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 3 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0162\n",
      "Training | EPOCH: 3 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 3 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.2053 | Current Mini-batch loss: 0.4070\n",
      "Training | EPOCH: 3 | Average Loss: 0.1366 | Current Mini-batch loss: 0.0678\n",
      "Training | EPOCH: 3 | Average Loss: 0.0684 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0646 | Current Mini-batch loss: 0.0608\n",
      "Training | EPOCH: 3 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 3 | Average Loss: 0.0288 | Current Mini-batch loss: 0.0238\n",
      "Training | EPOCH: 3 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 3 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0585\n",
      "Training | EPOCH: 3 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0269\n",
      "Training | EPOCH: 3 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.2230 | Current Mini-batch loss: 0.4441\n",
      "Training | EPOCH: 3 | Average Loss: 0.1276 | Current Mini-batch loss: 0.0322\n",
      "Training | EPOCH: 3 | Average Loss: 0.2101 | Current Mini-batch loss: 0.2926\n",
      "Training | EPOCH: 3 | Average Loss: 0.3068 | Current Mini-batch loss: 0.4035\n",
      "Training | EPOCH: 3 | Average Loss: 0.1672 | Current Mini-batch loss: 0.0277\n",
      "Training | EPOCH: 3 | Average Loss: 0.5786 | Current Mini-batch loss: 0.9900\n",
      "Training | EPOCH: 3 | Average Loss: 0.2922 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.1461 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0859 | Current Mini-batch loss: 0.0257\n",
      "Training | EPOCH: 3 | Average Loss: 0.0993 | Current Mini-batch loss: 0.1126\n",
      "Training | EPOCH: 3 | Average Loss: 0.0497 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0355\n",
      "Training | EPOCH: 3 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0269\n",
      "Training | EPOCH: 3 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0701\n",
      "Training | EPOCH: 3 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 3 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.1608 | Current Mini-batch loss: 0.3147\n",
      "Training | EPOCH: 3 | Average Loss: 0.0903 | Current Mini-batch loss: 0.0197\n",
      "Training | EPOCH: 3 | Average Loss: 0.0453 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1467 | Current Mini-batch loss: 0.2481\n",
      "Training | EPOCH: 3 | Average Loss: 0.0767 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0384 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 3 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 3 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.1085 | Current Mini-batch loss: 0.2108\n",
      "Training | EPOCH: 3 | Average Loss: 0.0547 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.6198 | Current Mini-batch loss: 1.2120\n",
      "Training | EPOCH: 3 | Average Loss: 0.3104 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.1863 | Current Mini-batch loss: 0.0623\n",
      "Training | EPOCH: 3 | Average Loss: 0.0932 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0469 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0127\n",
      "Training | EPOCH: 3 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 3 | Average Loss: 0.1057 | Current Mini-batch loss: 0.1930\n",
      "Training | EPOCH: 3 | Average Loss: 0.0543 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.0562 | Current Mini-batch loss: 0.0581\n",
      "Training | EPOCH: 3 | Average Loss: 0.0290 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 3 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 3 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.6084 | Current Mini-batch loss: 1.2129\n",
      "Training | EPOCH: 3 | Average Loss: 0.3046 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.1547 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 3 | Average Loss: 1.0392 | Current Mini-batch loss: 1.9237\n",
      "Training | EPOCH: 3 | Average Loss: 1.0606 | Current Mini-batch loss: 1.0820\n",
      "Training | EPOCH: 3 | Average Loss: 0.5305 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.2677 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 3 | Average Loss: 0.3161 | Current Mini-batch loss: 0.3646\n",
      "Training | EPOCH: 3 | Average Loss: 0.1581 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0796 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.1444 | Current Mini-batch loss: 0.2092\n",
      "Training | EPOCH: 3 | Average Loss: 0.0886 | Current Mini-batch loss: 0.0328\n",
      "Training | EPOCH: 3 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.1685 | Current Mini-batch loss: 0.2919\n",
      "Training | EPOCH: 3 | Average Loss: 0.0997 | Current Mini-batch loss: 0.0309\n",
      "Training | EPOCH: 3 | Average Loss: 0.0636 | Current Mini-batch loss: 0.0276\n",
      "Training | EPOCH: 3 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 3 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 3 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0261\n",
      "Training | EPOCH: 3 | Average Loss: 0.1522 | Current Mini-batch loss: 0.2891\n",
      "Training | EPOCH: 3 | Average Loss: 0.0768 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0389 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0195 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0297\n",
      "Training | EPOCH: 3 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.6622 | Current Mini-batch loss: 1.3232\n",
      "Training | EPOCH: 3 | Average Loss: 0.3314 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.3245 | Current Mini-batch loss: 0.3175\n",
      "Training | EPOCH: 3 | Average Loss: 0.1623 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.5810 | Current Mini-batch loss: 0.9997\n",
      "Training | EPOCH: 3 | Average Loss: 0.2912 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.1461 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0751 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 3 | Average Loss: 0.0378 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.3348 | Current Mini-batch loss: 0.6600\n",
      "Training | EPOCH: 3 | Average Loss: 0.1687 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.1207 | Current Mini-batch loss: 0.0727\n",
      "Training | EPOCH: 3 | Average Loss: 0.0605 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 3 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0133\n",
      "Training | EPOCH: 3 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0680 | Current Mini-batch loss: 0.1344\n",
      "Training | EPOCH: 3 | Average Loss: 0.0348 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 3 | Average Loss: 0.0629 | Current Mini-batch loss: 0.1110\n",
      "Training | EPOCH: 3 | Average Loss: 0.0317 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0381 | Current Mini-batch loss: 0.0603\n",
      "Training | EPOCH: 3 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 3 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0127\n",
      "Training | EPOCH: 3 | Average Loss: 0.2200 | Current Mini-batch loss: 0.4280\n",
      "Training | EPOCH: 3 | Average Loss: 0.1104 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0589 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 3 | Average Loss: 0.0649 | Current Mini-batch loss: 0.0709\n",
      "Training | EPOCH: 3 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.4263 | Current Mini-batch loss: 0.8197\n",
      "Training | EPOCH: 3 | Average Loss: 0.2249 | Current Mini-batch loss: 0.0236\n",
      "Training | EPOCH: 3 | Average Loss: 0.1142 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 3 | Average Loss: 0.0653 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 3 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0138\n",
      "Training | EPOCH: 3 | Average Loss: 0.0482 | Current Mini-batch loss: 0.0569\n",
      "Training | EPOCH: 3 | Average Loss: 0.1037 | Current Mini-batch loss: 0.1591\n",
      "Training | EPOCH: 3 | Average Loss: 0.0675 | Current Mini-batch loss: 0.0314\n",
      "Training | EPOCH: 3 | Average Loss: 0.0353 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 3 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 3 | Average Loss: 0.0788 | Current Mini-batch loss: 0.1376\n",
      "Training | EPOCH: 3 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 3 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0202\n",
      "Training | EPOCH: 3 | Average Loss: 0.2750 | Current Mini-batch loss: 0.5370\n",
      "Training | EPOCH: 3 | Average Loss: 0.1390 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 3 | Average Loss: 0.0748 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 3 | Average Loss: 0.2429 | Current Mini-batch loss: 0.4109\n",
      "Training | EPOCH: 3 | Average Loss: 0.1218 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 3 | Average Loss: 0.0637 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 3 | Average Loss: 0.0461 | Current Mini-batch loss: 0.0284\n",
      "Training | EPOCH: 3 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.1071 | Current Mini-batch loss: 0.2096\n",
      "Training | EPOCH: 3 | Average Loss: 0.0554 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 3 | Average Loss: 0.0321 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 3 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0345 | Current Mini-batch loss: 0.0609\n",
      "Training | EPOCH: 3 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 3 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0925 | Current Mini-batch loss: 0.1833\n",
      "Training | EPOCH: 3 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0234 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.3513 | Current Mini-batch loss: 0.6791\n",
      "Training | EPOCH: 3 | Average Loss: 0.5951 | Current Mini-batch loss: 0.8390\n",
      "Training | EPOCH: 3 | Average Loss: 0.2985 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.1500 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0751 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.4451 | Current Mini-batch loss: 0.8152\n",
      "Training | EPOCH: 3 | Average Loss: 0.2240 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 3 | Average Loss: 0.1211 | Current Mini-batch loss: 0.0182\n",
      "Training | EPOCH: 3 | Average Loss: 0.0609 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0321 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0609\n",
      "Training | EPOCH: 3 | Average Loss: 0.0539 | Current Mini-batch loss: 0.0614\n",
      "Training | EPOCH: 3 | Average Loss: 0.0282 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 3 | Average Loss: 0.0234 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 3 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 3 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 3 | Average Loss: 0.3471 | Current Mini-batch loss: 0.6886\n",
      "Training | EPOCH: 3 | Average Loss: 0.1741 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0998 | Current Mini-batch loss: 0.0255\n",
      "Training | EPOCH: 3 | Average Loss: 0.3177 | Current Mini-batch loss: 0.5355\n",
      "Training | EPOCH: 3 | Average Loss: 0.1892 | Current Mini-batch loss: 0.0607\n",
      "Training | EPOCH: 3 | Average Loss: 0.0948 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0701 | Current Mini-batch loss: 0.0454\n",
      "Training | EPOCH: 3 | Average Loss: 0.0360 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.5715 | Current Mini-batch loss: 1.1070\n",
      "Training | EPOCH: 3 | Average Loss: 0.2859 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.1438 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0960 | Current Mini-batch loss: 0.0482\n",
      "Training | EPOCH: 3 | Average Loss: 0.0486 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0687 | Current Mini-batch loss: 0.0888\n",
      "Training | EPOCH: 3 | Average Loss: 0.0356 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 3 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 3 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0471 | Current Mini-batch loss: 0.0911\n",
      "Training | EPOCH: 3 | Average Loss: 0.1052 | Current Mini-batch loss: 0.1634\n",
      "Training | EPOCH: 3 | Average Loss: 0.3896 | Current Mini-batch loss: 0.6740\n",
      "Training | EPOCH: 3 | Average Loss: 0.1962 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 3 | Average Loss: 0.1066 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 3 | Average Loss: 0.3019 | Current Mini-batch loss: 0.4971\n",
      "Training | EPOCH: 3 | Average Loss: 0.1540 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 3 | Average Loss: 0.1939 | Current Mini-batch loss: 0.2338\n",
      "Training | EPOCH: 3 | Average Loss: 0.0970 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0864 | Current Mini-batch loss: 0.0759\n",
      "Training | EPOCH: 3 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 3 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.2037 | Current Mini-batch loss: 0.4035\n",
      "Training | EPOCH: 3 | Average Loss: 0.1118 | Current Mini-batch loss: 0.0200\n",
      "Training | EPOCH: 3 | Average Loss: 0.0576 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 3 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0290 | Current Mini-batch loss: 0.0289\n",
      "Training | EPOCH: 3 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0191\n",
      "Training | EPOCH: 3 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0216\n",
      "Training | EPOCH: 3 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 3 | Average Loss: 0.0258 | Current Mini-batch loss: 0.0382\n",
      "Training | EPOCH: 3 | Average Loss: 0.0135 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 3 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0266\n",
      "Training | EPOCH: 3 | Average Loss: 0.0289 | Current Mini-batch loss: 0.0378\n",
      "Training | EPOCH: 3 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0366\n",
      "Training | EPOCH: 3 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0312\n",
      "Training | EPOCH: 3 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0147\n",
      "Training | EPOCH: 3 | Average Loss: 0.0429 | Current Mini-batch loss: 0.0680\n",
      "Training | EPOCH: 3 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0272\n",
      "Training | EPOCH: 3 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 3 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.4302 | Current Mini-batch loss: 0.8490\n",
      "Training | EPOCH: 3 | Average Loss: 0.3615 | Current Mini-batch loss: 0.2928\n",
      "Training | EPOCH: 3 | Average Loss: 0.1833 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 3 | Average Loss: 0.0973 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 3 | Average Loss: 0.0546 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 3 | Average Loss: 0.0479 | Current Mini-batch loss: 0.0413\n",
      "Training | EPOCH: 3 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 3 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 3 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 3 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0627\n",
      "Training | EPOCH: 3 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0529 | Current Mini-batch loss: 0.0972\n",
      "Training | EPOCH: 3 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1160 | Current Mini-batch loss: 0.2054\n",
      "Training | EPOCH: 3 | Average Loss: 0.1080 | Current Mini-batch loss: 0.1000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0936 | Current Mini-batch loss: 0.0792\n",
      "Training | EPOCH: 3 | Average Loss: 0.0469 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0304\n",
      "Training | EPOCH: 3 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 3 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.2437 | Current Mini-batch loss: 0.4843\n",
      "Training | EPOCH: 3 | Average Loss: 0.2168 | Current Mini-batch loss: 0.1900\n",
      "Training | EPOCH: 3 | Average Loss: 0.1114 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 3 | Average Loss: 0.7462 | Current Mini-batch loss: 1.3809\n",
      "Training | EPOCH: 3 | Average Loss: 0.5613 | Current Mini-batch loss: 0.3764\n",
      "Training | EPOCH: 3 | Average Loss: 0.2808 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.6636 | Current Mini-batch loss: 1.0463\n",
      "Training | EPOCH: 3 | Average Loss: 0.5074 | Current Mini-batch loss: 0.3513\n",
      "Training | EPOCH: 3 | Average Loss: 0.2826 | Current Mini-batch loss: 0.0578\n",
      "Training | EPOCH: 3 | Average Loss: 0.1428 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 3 | Average Loss: 0.0753 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 3 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 3 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0142\n",
      "Training | EPOCH: 3 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 3 | Average Loss: 0.4562 | Current Mini-batch loss: 0.9092\n",
      "Training | EPOCH: 3 | Average Loss: 0.2291 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 3 | Average Loss: 0.1150 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 3 | Average Loss: 0.0824 | Current Mini-batch loss: 0.0498\n",
      "Training | EPOCH: 3 | Average Loss: 0.0415 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 3 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 3 | Average Loss: 0.0565 | Current Mini-batch loss: 0.0973\n",
      "Training | EPOCH: 3 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.0188 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 3 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0284\n",
      "Training | EPOCH: 3 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0538\n",
      "Training | EPOCH: 3 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 3 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 3 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0289\n",
      "Training | EPOCH: 3 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.3617 | Current Mini-batch loss: 0.7118\n",
      "Training | EPOCH: 3 | Average Loss: 0.1809 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.3632 | Current Mini-batch loss: 0.5454\n",
      "Training | EPOCH: 3 | Average Loss: 0.1837 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 3 | Average Loss: 0.0923 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.3577 | Current Mini-batch loss: 0.6230\n",
      "Training | EPOCH: 3 | Average Loss: 0.1795 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.2881 | Current Mini-batch loss: 0.3967\n",
      "Training | EPOCH: 3 | Average Loss: 0.1718 | Current Mini-batch loss: 0.0555\n",
      "Training | EPOCH: 3 | Average Loss: 0.0860 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0446 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 3 | Average Loss: 0.0274 | Current Mini-batch loss: 0.0102\n",
      "Training | EPOCH: 3 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0208\n",
      "Training | EPOCH: 3 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 3 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 3 | Average Loss: 0.1200 | Current Mini-batch loss: 0.2233\n",
      "Training | EPOCH: 3 | Average Loss: 0.0746 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 3 | Average Loss: 0.0376 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 3 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0094\n",
      "Training | EPOCH: 3 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.3193 | Current Mini-batch loss: 0.6269\n",
      "Training | EPOCH: 3 | Average Loss: 0.4043 | Current Mini-batch loss: 0.4892\n",
      "Training | EPOCH: 3 | Average Loss: 0.2022 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.1159 | Current Mini-batch loss: 0.0296\n",
      "Training | EPOCH: 3 | Average Loss: 0.0580 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0274 | Current Mini-batch loss: 0.0248\n",
      "Training | EPOCH: 3 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 3 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 3 | Average Loss: 0.1225 | Current Mini-batch loss: 0.2413\n",
      "Training | EPOCH: 3 | Average Loss: 0.2025 | Current Mini-batch loss: 0.2825\n",
      "Training | EPOCH: 3 | Average Loss: 0.1014 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.2160 | Current Mini-batch loss: 0.3306\n",
      "Training | EPOCH: 3 | Average Loss: 0.1089 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 3 | Average Loss: 0.2410 | Current Mini-batch loss: 0.3731\n",
      "Training | EPOCH: 3 | Average Loss: 0.3042 | Current Mini-batch loss: 0.3674\n",
      "Training | EPOCH: 3 | Average Loss: 0.1521 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0905 | Current Mini-batch loss: 0.0289\n",
      "Training | EPOCH: 3 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 3 | Average Loss: 0.0251 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 3 | Average Loss: 0.0214 | Current Mini-batch loss: 0.0177\n",
      "Training | EPOCH: 3 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 3 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 3 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0193\n",
      "Training | EPOCH: 3 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 3 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 3 | Average Loss: 0.0913 | Current Mini-batch loss: 0.1796\n",
      "Training | EPOCH: 3 | Average Loss: 0.0623 | Current Mini-batch loss: 0.0333\n",
      "Training | EPOCH: 3 | Average Loss: 0.1134 | Current Mini-batch loss: 0.1646\n",
      "Training | EPOCH: 3 | Average Loss: 0.0571 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 3 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0424\n",
      "Training | EPOCH: 3 | Average Loss: 0.0261 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 3 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 3 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0206\n",
      "Training | EPOCH: 3 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0076\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0995 | Current Mini-batch loss: 0.0995\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0068\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0079\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1492 | Current Mini-batch loss: 0.2975\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0753 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0379 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0029\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0251 | Current Mini-batch loss: 0.0297\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0026\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0247 | Current Mini-batch loss: 0.0454\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0492 | Current Mini-batch loss: 0.0736\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0028\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0179\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0494 | Current Mini-batch loss: 0.0929\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0258 | Current Mini-batch loss: 0.0022\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0093\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0330\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0249\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0412\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 3 | Average Loss: 0.2362 | Current Mini-batch loss: 0.4588\n",
      "Validation | EPOCH: 3 | Average Loss: 0.2125 | Current Mini-batch loss: 0.1889\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1066 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0445 | Current Mini-batch loss: 0.0355\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0044\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0040\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0297\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.2712 | Current Mini-batch loss: 0.5401\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1362 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0685 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0182\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0063\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0026\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0123\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1036 | Current Mini-batch loss: 0.2003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0267 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0167 | Current Mini-batch loss: 0.0066\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0057\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0057\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0028\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0055\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1857 | Current Mini-batch loss: 0.3668\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1139 | Current Mini-batch loss: 0.0421\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0571 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0454 | Current Mini-batch loss: 0.0337\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0227 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0035\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1947 | Current Mini-batch loss: 0.3763\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0977 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0490 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0254 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0786 | Current Mini-batch loss: 0.1444\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0396 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0021\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0664\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0037\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0313\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1334 | Current Mini-batch loss: 0.2661\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0676 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0342 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0093\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.1066 | Current Mini-batch loss: 0.2116\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0269 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0426 | Current Mini-batch loss: 0.0584\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0272\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0107\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0633\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0787 | Current Mini-batch loss: 0.1210\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0313 | Current Mini-batch loss: 0.0421\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0193 | Current Mini-batch loss: 0.0227\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0068\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0036\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0160\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0093\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0865\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0119\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0037\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0054\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0642 | Current Mini-batch loss: 0.1267\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0391 | Current Mini-batch loss: 0.0140\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0784 | Current Mini-batch loss: 0.1371\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0226 | Current Mini-batch loss: 0.0058\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0620 | Current Mini-batch loss: 0.1015\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 3 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0441\n",
      "Training | EPOCH: 4 | Average Loss: 0.0951 | Current Mini-batch loss: 0.1667\n",
      "Training | EPOCH: 4 | Average Loss: 0.0478 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0255 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 4 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 4 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0142\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0946 | Current Mini-batch loss: 0.1841\n",
      "Training | EPOCH: 4 | Average Loss: 0.0533 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 4 | Average Loss: 0.3257 | Current Mini-batch loss: 0.5982\n",
      "Training | EPOCH: 4 | Average Loss: 0.2456 | Current Mini-batch loss: 0.1655\n",
      "Training | EPOCH: 4 | Average Loss: 0.1452 | Current Mini-batch loss: 0.0447\n",
      "Training | EPOCH: 4 | Average Loss: 0.0882 | Current Mini-batch loss: 0.0313\n",
      "Training | EPOCH: 4 | Average Loss: 0.0451 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 4 | Average Loss: 0.0226 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0280\n",
      "Training | EPOCH: 4 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 4 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0967 | Current Mini-batch loss: 0.1821\n",
      "Training | EPOCH: 4 | Average Loss: 0.6554 | Current Mini-batch loss: 1.2141\n",
      "Training | EPOCH: 4 | Average Loss: 0.4101 | Current Mini-batch loss: 0.1648\n",
      "Training | EPOCH: 4 | Average Loss: 0.2053 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.1594 | Current Mini-batch loss: 0.1135\n",
      "Training | EPOCH: 4 | Average Loss: 0.0810 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.1974 | Current Mini-batch loss: 0.3138\n",
      "Training | EPOCH: 4 | Average Loss: 0.3023 | Current Mini-batch loss: 0.4071\n",
      "Training | EPOCH: 4 | Average Loss: 0.1692 | Current Mini-batch loss: 0.0362\n",
      "Training | EPOCH: 4 | Average Loss: 0.0848 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 4 | Average Loss: 0.0291 | Current Mini-batch loss: 0.0125\n",
      "Training | EPOCH: 4 | Average Loss: 0.0340 | Current Mini-batch loss: 0.0388\n",
      "Training | EPOCH: 4 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 4 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.1654 | Current Mini-batch loss: 0.3257\n",
      "Training | EPOCH: 4 | Average Loss: 0.1989 | Current Mini-batch loss: 0.2324\n",
      "Training | EPOCH: 4 | Average Loss: 0.1094 | Current Mini-batch loss: 0.0200\n",
      "Training | EPOCH: 4 | Average Loss: 0.0567 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 4 | Average Loss: 0.0314 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 4 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 4 | Average Loss: 0.0822 | Current Mini-batch loss: 0.1539\n",
      "Training | EPOCH: 4 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0200\n",
      "Training | EPOCH: 4 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 4 | Average Loss: 0.1042 | Current Mini-batch loss: 0.1908\n",
      "Training | EPOCH: 4 | Average Loss: 0.0540 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 4 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0398\n",
      "Training | EPOCH: 4 | Average Loss: 0.0190 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 4 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 4 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 4 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0345\n",
      "Training | EPOCH: 4 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 4 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0364\n",
      "Training | EPOCH: 4 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 4 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0428\n",
      "Training | EPOCH: 4 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 4 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0379\n",
      "Training | EPOCH: 4 | Average Loss: 0.0954 | Current Mini-batch loss: 0.1616\n",
      "Training | EPOCH: 4 | Average Loss: 0.0486 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0247 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.1622 | Current Mini-batch loss: 0.3118\n",
      "Training | EPOCH: 4 | Average Loss: 0.0847 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 4 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0193 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 4 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0122\n",
      "Training | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0257 | Current Mini-batch loss: 0.0459\n",
      "Training | EPOCH: 4 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 4 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 4 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0131\n",
      "Training | EPOCH: 4 | Average Loss: 0.0884 | Current Mini-batch loss: 0.1662\n",
      "Training | EPOCH: 4 | Average Loss: 0.1367 | Current Mini-batch loss: 0.1850\n",
      "Training | EPOCH: 4 | Average Loss: 0.0689 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0345 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0684 | Current Mini-batch loss: 0.1186\n",
      "Training | EPOCH: 4 | Average Loss: 0.0361 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 4 | Average Loss: 0.1739 | Current Mini-batch loss: 0.3431\n",
      "Training | EPOCH: 4 | Average Loss: 0.0870 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0374\n",
      "Training | EPOCH: 4 | Average Loss: 0.0635 | Current Mini-batch loss: 0.0971\n",
      "Training | EPOCH: 4 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 4 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0416\n",
      "Training | EPOCH: 4 | Average Loss: 0.0604 | Current Mini-batch loss: 0.0828\n",
      "Training | EPOCH: 4 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0433\n",
      "Training | EPOCH: 4 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0475 | Current Mini-batch loss: 0.0882\n",
      "Training | EPOCH: 4 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 4 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0365\n",
      "Training | EPOCH: 4 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0285\n",
      "Training | EPOCH: 4 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 4 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0325\n",
      "Training | EPOCH: 4 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 4 | Average Loss: 0.1896 | Current Mini-batch loss: 0.3675\n",
      "Training | EPOCH: 4 | Average Loss: 0.2068 | Current Mini-batch loss: 0.2239\n",
      "Training | EPOCH: 4 | Average Loss: 0.1040 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.1703 | Current Mini-batch loss: 0.2885\n",
      "Training | EPOCH: 4 | Average Loss: 0.0865 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0564 | Current Mini-batch loss: 0.0264\n",
      "Training | EPOCH: 4 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 4 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0445 | Current Mini-batch loss: 0.0731\n",
      "Training | EPOCH: 4 | Average Loss: 0.0245 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 4 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 4 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0179\n",
      "Training | EPOCH: 4 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 4 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0367 | Current Mini-batch loss: 0.0712\n",
      "Training | EPOCH: 4 | Average Loss: 0.0196 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0406 | Current Mini-batch loss: 0.0713\n",
      "Training | EPOCH: 4 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 4 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0402\n",
      "Training | EPOCH: 4 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0383\n",
      "Training | EPOCH: 4 | Average Loss: 0.0478 | Current Mini-batch loss: 0.0745\n",
      "Training | EPOCH: 4 | Average Loss: 0.0243 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0269 | Current Mini-batch loss: 0.0497\n",
      "Training | EPOCH: 4 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0516 | Current Mini-batch loss: 0.0948\n",
      "Training | EPOCH: 4 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0331\n",
      "Training | EPOCH: 4 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0078\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0306\n",
      "Training | EPOCH: 4 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 4 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 4 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0132\n",
      "Training | EPOCH: 4 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0077\n",
      "Training | EPOCH: 4 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0117\n",
      "Training | EPOCH: 4 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0201\n",
      "Training | EPOCH: 4 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.1502 | Current Mini-batch loss: 0.2946\n",
      "Training | EPOCH: 4 | Average Loss: 0.0810 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 4 | Average Loss: 0.0433 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 4 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0377 | Current Mini-batch loss: 0.0681\n",
      "Training | EPOCH: 4 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 4 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0199\n",
      "Training | EPOCH: 4 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0374\n",
      "Training | EPOCH: 4 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 4 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0135\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.3338 | Current Mini-batch loss: 0.6629\n",
      "Training | EPOCH: 4 | Average Loss: 0.1672 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0839 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.3732 | Current Mini-batch loss: 0.6625\n",
      "Training | EPOCH: 4 | Average Loss: 0.1999 | Current Mini-batch loss: 0.0266\n",
      "Training | EPOCH: 4 | Average Loss: 0.1106 | Current Mini-batch loss: 0.0214\n",
      "Training | EPOCH: 4 | Average Loss: 0.0588 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 4 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 4 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0439\n",
      "Training | EPOCH: 4 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0845\n",
      "Training | EPOCH: 4 | Average Loss: 0.0274 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 4 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0252\n",
      "Training | EPOCH: 4 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 4 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0336\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0231\n",
      "Training | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0438 | Current Mini-batch loss: 0.0838\n",
      "Training | EPOCH: 4 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.1227 | Current Mini-batch loss: 0.2343\n",
      "Training | EPOCH: 4 | Average Loss: 0.0628 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0370\n",
      "Training | EPOCH: 4 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0385 | Current Mini-batch loss: 0.0645\n",
      "Training | EPOCH: 4 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0101\n",
      "Training | EPOCH: 4 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0320\n",
      "Training | EPOCH: 4 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 4 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0519\n",
      "Training | EPOCH: 4 | Average Loss: 0.1711 | Current Mini-batch loss: 0.3118\n",
      "Training | EPOCH: 4 | Average Loss: 0.1130 | Current Mini-batch loss: 0.0550\n",
      "Training | EPOCH: 4 | Average Loss: 0.0576 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 4 | Average Loss: 0.0347 | Current Mini-batch loss: 0.0360\n",
      "Training | EPOCH: 4 | Average Loss: 0.1935 | Current Mini-batch loss: 0.3523\n",
      "Training | EPOCH: 4 | Average Loss: 0.0968 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0546 | Current Mini-batch loss: 0.0124\n",
      "Training | EPOCH: 4 | Average Loss: 0.2370 | Current Mini-batch loss: 0.4194\n",
      "Training | EPOCH: 4 | Average Loss: 0.1185 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0615 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 4 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 4 | Average Loss: 0.0398 | Current Mini-batch loss: 0.0675\n",
      "Training | EPOCH: 4 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 4 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 4 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 4 | Average Loss: 0.1182 | Current Mini-batch loss: 0.2306\n",
      "Training | EPOCH: 4 | Average Loss: 0.1663 | Current Mini-batch loss: 0.2144\n",
      "Training | EPOCH: 4 | Average Loss: 0.0837 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0426 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 4 | Average Loss: 0.0551 | Current Mini-batch loss: 0.0818\n",
      "Training | EPOCH: 4 | Average Loss: 0.0290 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0183 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 4 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.4313 | Current Mini-batch loss: 0.8533\n",
      "Training | EPOCH: 4 | Average Loss: 0.5581 | Current Mini-batch loss: 0.6849\n",
      "Training | EPOCH: 4 | Average Loss: 0.2806 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 4 | Average Loss: 0.1412 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0727 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 4 | Average Loss: 0.0393 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 4 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0718\n",
      "Training | EPOCH: 4 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0488 | Current Mini-batch loss: 0.0932\n",
      "Training | EPOCH: 4 | Average Loss: 0.0342 | Current Mini-batch loss: 0.0197\n",
      "Training | EPOCH: 4 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 4 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0253\n",
      "Training | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 4 | Average Loss: 0.0563 | Current Mini-batch loss: 0.1086\n",
      "Training | EPOCH: 4 | Average Loss: 0.0486 | Current Mini-batch loss: 0.0409\n",
      "Training | EPOCH: 4 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 4 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0173\n",
      "Training | EPOCH: 4 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0251\n",
      "Training | EPOCH: 4 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0252\n",
      "Training | EPOCH: 4 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0180\n",
      "Training | EPOCH: 4 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 4 | Average Loss: 0.0753 | Current Mini-batch loss: 0.1363\n",
      "Training | EPOCH: 4 | Average Loss: 0.0653 | Current Mini-batch loss: 0.0552\n",
      "Training | EPOCH: 4 | Average Loss: 0.0328 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0665 | Current Mini-batch loss: 0.1002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0334 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0341\n",
      "Training | EPOCH: 4 | Average Loss: 0.0893 | Current Mini-batch loss: 0.1448\n",
      "Training | EPOCH: 4 | Average Loss: 0.0447 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0581 | Current Mini-batch loss: 0.0715\n",
      "Training | EPOCH: 4 | Average Loss: 0.0421 | Current Mini-batch loss: 0.0261\n",
      "Training | EPOCH: 4 | Average Loss: 0.1743 | Current Mini-batch loss: 0.3065\n",
      "Training | EPOCH: 4 | Average Loss: 0.0884 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0444 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 4 | Average Loss: 0.0361 | Current Mini-batch loss: 0.0458\n",
      "Training | EPOCH: 4 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0103\n",
      "Training | EPOCH: 4 | Average Loss: 0.0442 | Current Mini-batch loss: 0.0651\n",
      "Training | EPOCH: 4 | Average Loss: 0.1813 | Current Mini-batch loss: 0.3184\n",
      "Training | EPOCH: 4 | Average Loss: 0.0918 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.2002 | Current Mini-batch loss: 0.3085\n",
      "Training | EPOCH: 4 | Average Loss: 0.1004 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0521 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0397 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 4 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0441\n",
      "Training | EPOCH: 4 | Average Loss: 0.1026 | Current Mini-batch loss: 0.1726\n",
      "Training | EPOCH: 4 | Average Loss: 0.0515 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0724 | Current Mini-batch loss: 0.0933\n",
      "Training | EPOCH: 4 | Average Loss: 0.0373 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.2125 | Current Mini-batch loss: 0.4237\n",
      "Training | EPOCH: 4 | Average Loss: 0.1063 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.2785 | Current Mini-batch loss: 0.4508\n",
      "Training | EPOCH: 4 | Average Loss: 0.1395 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0700 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0371 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 4 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0651 | Current Mini-batch loss: 0.1113\n",
      "Training | EPOCH: 4 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0167 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0934 | Current Mini-batch loss: 0.1700\n",
      "Training | EPOCH: 4 | Average Loss: 0.1010 | Current Mini-batch loss: 0.1086\n",
      "Training | EPOCH: 4 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0268 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0333 | Current Mini-batch loss: 0.0399\n",
      "Training | EPOCH: 4 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0267\n",
      "Training | EPOCH: 4 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 4 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0083\n",
      "Training | EPOCH: 4 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0168\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0459\n",
      "Training | EPOCH: 4 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 4 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 4 | Average Loss: 0.1026 | Current Mini-batch loss: 0.1966\n",
      "Training | EPOCH: 4 | Average Loss: 0.0527 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.1021 | Current Mini-batch loss: 0.1515\n",
      "Training | EPOCH: 4 | Average Loss: 0.0511 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0262 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0497 | Current Mini-batch loss: 0.0948\n",
      "Training | EPOCH: 4 | Average Loss: 0.0272 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 4 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0407\n",
      "Training | EPOCH: 4 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 4 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 4 | Average Loss: 0.1101 | Current Mini-batch loss: 0.2077\n",
      "Training | EPOCH: 4 | Average Loss: 0.0556 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 4 | Average Loss: 0.0640 | Current Mini-batch loss: 0.0961\n",
      "Training | EPOCH: 4 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 4 | Average Loss: 0.0353 | Current Mini-batch loss: 0.0365\n",
      "Training | EPOCH: 4 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0229 | Current Mini-batch loss: 0.0273\n",
      "Training | EPOCH: 4 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0345\n",
      "Training | EPOCH: 4 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0174\n",
      "Training | EPOCH: 4 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0242\n",
      "Training | EPOCH: 4 | Average Loss: 0.1476 | Current Mini-batch loss: 0.2801\n",
      "Training | EPOCH: 4 | Average Loss: 0.0738 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0391 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 4 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0338\n",
      "Training | EPOCH: 4 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 4 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0417 | Current Mini-batch loss: 0.0789\n",
      "Training | EPOCH: 4 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0216\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 4 | Average Loss: 0.0326 | Current Mini-batch loss: 0.0535\n",
      "Training | EPOCH: 4 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0135\n",
      "Training | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.1533 | Current Mini-batch loss: 0.3060\n",
      "Training | EPOCH: 4 | Average Loss: 0.0850 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 4 | Average Loss: 0.0639 | Current Mini-batch loss: 0.0429\n",
      "Training | EPOCH: 4 | Average Loss: 0.0328 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0167 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0146 | Current Mini-batch loss: 0.0244\n",
      "Training | EPOCH: 4 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0585 | Current Mini-batch loss: 0.1161\n",
      "Training | EPOCH: 4 | Average Loss: 0.0293 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 4 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 4 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0451\n",
      "Training | EPOCH: 4 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 4 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 4 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0392\n",
      "Training | EPOCH: 4 | Average Loss: 0.1734 | Current Mini-batch loss: 0.3243\n",
      "Training | EPOCH: 4 | Average Loss: 0.0867 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.2005 | Current Mini-batch loss: 0.3143\n",
      "Training | EPOCH: 4 | Average Loss: 0.1018 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0518 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 4 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 4 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0916\n",
      "Training | EPOCH: 4 | Average Loss: 0.0269 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 4 | Average Loss: 0.0693 | Current Mini-batch loss: 0.1116\n",
      "Training | EPOCH: 4 | Average Loss: 0.0672 | Current Mini-batch loss: 0.0651\n",
      "Training | EPOCH: 4 | Average Loss: 0.0340 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0236 | Current Mini-batch loss: 0.0131\n",
      "Training | EPOCH: 4 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 1.0594 | Current Mini-batch loss: 2.1161\n",
      "Training | EPOCH: 4 | Average Loss: 0.5391 | Current Mini-batch loss: 0.0188\n",
      "Training | EPOCH: 4 | Average Loss: 0.2696 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.3199 | Current Mini-batch loss: 0.3702\n",
      "Training | EPOCH: 4 | Average Loss: 0.1610 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0806 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0404 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.2038 | Current Mini-batch loss: 0.4023\n",
      "Training | EPOCH: 4 | Average Loss: 0.1028 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0515 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.1391 | Current Mini-batch loss: 0.2267\n",
      "Training | EPOCH: 4 | Average Loss: 0.0844 | Current Mini-batch loss: 0.0296\n",
      "Training | EPOCH: 4 | Average Loss: 0.0423 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.1010 | Current Mini-batch loss: 0.1597\n",
      "Training | EPOCH: 4 | Average Loss: 0.0506 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0164 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 4 | Average Loss: 0.0330 | Current Mini-batch loss: 0.0496\n",
      "Training | EPOCH: 4 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 4 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0496\n",
      "Training | EPOCH: 4 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 4 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.5811 | Current Mini-batch loss: 1.1596\n",
      "Training | EPOCH: 4 | Average Loss: 0.4754 | Current Mini-batch loss: 0.3698\n",
      "Training | EPOCH: 4 | Average Loss: 0.2664 | Current Mini-batch loss: 0.0574\n",
      "Training | EPOCH: 4 | Average Loss: 0.1335 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0671 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.2041 | Current Mini-batch loss: 0.3410\n",
      "Training | EPOCH: 4 | Average Loss: 0.1275 | Current Mini-batch loss: 0.0509\n",
      "Training | EPOCH: 4 | Average Loss: 0.0943 | Current Mini-batch loss: 0.0612\n",
      "Training | EPOCH: 4 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 4 | Average Loss: 0.0251 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0260\n",
      "Training | EPOCH: 4 | Average Loss: 0.0527 | Current Mini-batch loss: 0.0902\n",
      "Training | EPOCH: 4 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 4 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 4 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 4 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0510\n",
      "Training | EPOCH: 4 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0516 | Current Mini-batch loss: 0.0893\n",
      "Training | EPOCH: 4 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 4 | Average Loss: 0.0407 | Current Mini-batch loss: 0.0530\n",
      "Training | EPOCH: 4 | Average Loss: 0.0205 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 4 | Average Loss: 0.0685 | Current Mini-batch loss: 0.1320\n",
      "Training | EPOCH: 4 | Average Loss: 0.1293 | Current Mini-batch loss: 0.1901\n",
      "Training | EPOCH: 4 | Average Loss: 0.0690 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 4 | Average Loss: 0.0903 | Current Mini-batch loss: 0.1116\n",
      "Training | EPOCH: 4 | Average Loss: 0.0460 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0270 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 4 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0371\n",
      "Training | EPOCH: 4 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 4 | Average Loss: 0.0764 | Current Mini-batch loss: 0.1467\n",
      "Training | EPOCH: 4 | Average Loss: 0.0410 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 4 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0353 | Current Mini-batch loss: 0.0494\n",
      "Training | EPOCH: 4 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0632 | Current Mini-batch loss: 0.1174\n",
      "Training | EPOCH: 4 | Average Loss: 0.0981 | Current Mini-batch loss: 0.1329\n",
      "Training | EPOCH: 4 | Average Loss: 0.0930 | Current Mini-batch loss: 0.0879\n",
      "Training | EPOCH: 4 | Average Loss: 0.2720 | Current Mini-batch loss: 0.4509\n",
      "Training | EPOCH: 4 | Average Loss: 0.1734 | Current Mini-batch loss: 0.0747\n",
      "Training | EPOCH: 4 | Average Loss: 0.0867 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 4 | Average Loss: 0.1740 | Current Mini-batch loss: 0.3339\n",
      "Training | EPOCH: 4 | Average Loss: 0.0876 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0461 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 4 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0279 | Current Mini-batch loss: 0.0441\n",
      "Training | EPOCH: 4 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 4 | Average Loss: 0.3527 | Current Mini-batch loss: 0.6893\n",
      "Training | EPOCH: 4 | Average Loss: 0.1773 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0887 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0489 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 4 | Average Loss: 0.1612 | Current Mini-batch loss: 0.2736\n",
      "Training | EPOCH: 4 | Average Loss: 0.5858 | Current Mini-batch loss: 1.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.2930 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.1468 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0993 | Current Mini-batch loss: 0.0519\n",
      "Training | EPOCH: 4 | Average Loss: 0.0504 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.1224 | Current Mini-batch loss: 0.1944\n",
      "Training | EPOCH: 4 | Average Loss: 0.0612 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0368 | Current Mini-batch loss: 0.0124\n",
      "Training | EPOCH: 4 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0136\n",
      "Training | EPOCH: 4 | Average Loss: 0.0344 | Current Mini-batch loss: 0.0595\n",
      "Training | EPOCH: 4 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0496 | Current Mini-batch loss: 0.0811\n",
      "Training | EPOCH: 4 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0131\n",
      "Training | EPOCH: 4 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0221\n",
      "Training | EPOCH: 4 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 4 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0446\n",
      "Training | EPOCH: 4 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0289 | Current Mini-batch loss: 0.0431\n",
      "Training | EPOCH: 4 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0090\n",
      "Training | EPOCH: 4 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.1814 | Current Mini-batch loss: 0.3614\n",
      "Training | EPOCH: 4 | Average Loss: 0.1282 | Current Mini-batch loss: 0.0750\n",
      "Training | EPOCH: 4 | Average Loss: 0.0785 | Current Mini-batch loss: 0.0288\n",
      "Training | EPOCH: 4 | Average Loss: 0.0400 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0245\n",
      "Training | EPOCH: 4 | Average Loss: 0.0392 | Current Mini-batch loss: 0.0462\n",
      "Training | EPOCH: 4 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.1115 | Current Mini-batch loss: 0.2032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0598 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.0312 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 4 | Average Loss: 0.0603 | Current Mini-batch loss: 0.1028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0314 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.1233 | Current Mini-batch loss: 0.2153\n",
      "Training | EPOCH: 4 | Average Loss: 0.0624 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.1124 | Current Mini-batch loss: 0.1625\n",
      "Training | EPOCH: 4 | Average Loss: 0.0564 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.2431 | Current Mini-batch loss: 0.4580\n",
      "Training | EPOCH: 4 | Average Loss: 0.1243 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 4 | Average Loss: 0.0634 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0318 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0207\n",
      "Training | EPOCH: 4 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 4 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0224 | Current Mini-batch loss: 0.0362\n",
      "Training | EPOCH: 4 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0281 | Current Mini-batch loss: 0.0449\n",
      "Training | EPOCH: 4 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 4 | Average Loss: 0.2425 | Current Mini-batch loss: 0.4673\n",
      "Training | EPOCH: 4 | Average Loss: 0.2962 | Current Mini-batch loss: 0.3499\n",
      "Training | EPOCH: 4 | Average Loss: 0.1482 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0743 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0416 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 4 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 4 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 4 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0330\n",
      "Training | EPOCH: 4 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 4 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0278\n",
      "Training | EPOCH: 4 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 4 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 4 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0230\n",
      "Training | EPOCH: 4 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 4 | Average Loss: 0.6369 | Current Mini-batch loss: 1.2688\n",
      "Training | EPOCH: 4 | Average Loss: 0.7021 | Current Mini-batch loss: 0.7674\n",
      "Training | EPOCH: 4 | Average Loss: 0.3567 | Current Mini-batch loss: 0.0112\n",
      "Training | EPOCH: 4 | Average Loss: 0.4127 | Current Mini-batch loss: 0.4688\n",
      "Training | EPOCH: 4 | Average Loss: 0.2136 | Current Mini-batch loss: 0.0145\n",
      "Training | EPOCH: 4 | Average Loss: 0.1468 | Current Mini-batch loss: 0.0801\n",
      "Training | EPOCH: 4 | Average Loss: 0.0751 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 4 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 4 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0617 | Current Mini-batch loss: 0.1123\n",
      "Training | EPOCH: 4 | Average Loss: 0.0312 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.2305 | Current Mini-batch loss: 0.4453\n",
      "Training | EPOCH: 4 | Average Loss: 0.1209 | Current Mini-batch loss: 0.0113\n",
      "Training | EPOCH: 4 | Average Loss: 0.0960 | Current Mini-batch loss: 0.0711\n",
      "Training | EPOCH: 4 | Average Loss: 0.0495 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 4 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.3131 | Current Mini-batch loss: 0.6009\n",
      "Training | EPOCH: 4 | Average Loss: 0.1627 | Current Mini-batch loss: 0.0123\n",
      "Training | EPOCH: 4 | Average Loss: 0.0826 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0626 | Current Mini-batch loss: 0.0427\n",
      "Training | EPOCH: 4 | Average Loss: 0.0698 | Current Mini-batch loss: 0.0770\n",
      "Training | EPOCH: 4 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 4 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 4 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0135\n",
      "Training | EPOCH: 4 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0195\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0420 | Current Mini-batch loss: 0.0820\n",
      "Training | EPOCH: 4 | Average Loss: 0.1119 | Current Mini-batch loss: 0.1817\n",
      "Training | EPOCH: 4 | Average Loss: 0.0577 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 4 | Average Loss: 0.0374 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 4 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 4 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 4 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0159\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0142\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 4 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0159\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 4 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 4 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0488\n",
      "Training | EPOCH: 4 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 4 | Average Loss: 0.4747 | Current Mini-batch loss: 0.9459\n",
      "Training | EPOCH: 4 | Average Loss: 0.2743 | Current Mini-batch loss: 0.0739\n",
      "Training | EPOCH: 4 | Average Loss: 0.1834 | Current Mini-batch loss: 0.0926\n",
      "Training | EPOCH: 4 | Average Loss: 0.0918 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0493 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 4 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.2442 | Current Mini-batch loss: 0.4628\n",
      "Training | EPOCH: 4 | Average Loss: 0.1241 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 4 | Average Loss: 0.0622 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0786 | Current Mini-batch loss: 0.0950\n",
      "Training | EPOCH: 4 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0281\n",
      "Training | EPOCH: 4 | Average Loss: 0.0506 | Current Mini-batch loss: 0.0478\n",
      "Training | EPOCH: 4 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0088\n",
      "Training | EPOCH: 4 | Average Loss: 0.0885 | Current Mini-batch loss: 0.1474\n",
      "Training | EPOCH: 4 | Average Loss: 0.0449 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0882 | Current Mini-batch loss: 0.1315\n",
      "Training | EPOCH: 4 | Average Loss: 0.4297 | Current Mini-batch loss: 0.7711\n",
      "Training | EPOCH: 4 | Average Loss: 0.2149 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.1075 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0538 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.2287 | Current Mini-batch loss: 0.4301\n",
      "Training | EPOCH: 4 | Average Loss: 0.1164 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 4 | Average Loss: 0.0585 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.3189 | Current Mini-batch loss: 0.6080\n",
      "Training | EPOCH: 4 | Average Loss: 0.1595 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0851 | Current Mini-batch loss: 0.0107\n",
      "Training | EPOCH: 4 | Average Loss: 0.0733 | Current Mini-batch loss: 0.0615\n",
      "Training | EPOCH: 4 | Average Loss: 0.0376 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 4 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 4 | Average Loss: 0.0629 | Current Mini-batch loss: 0.1196\n",
      "Training | EPOCH: 4 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 4 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0309\n",
      "Training | EPOCH: 4 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0641 | Current Mini-batch loss: 0.1216\n",
      "Training | EPOCH: 4 | Average Loss: 0.0348 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 4 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 4 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 4 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 4 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0295\n",
      "Training | EPOCH: 4 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.2033 | Current Mini-batch loss: 0.4023\n",
      "Training | EPOCH: 4 | Average Loss: 0.1225 | Current Mini-batch loss: 0.0417\n",
      "Training | EPOCH: 4 | Average Loss: 0.0645 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 4 | Average Loss: 0.0464 | Current Mini-batch loss: 0.0284\n",
      "Training | EPOCH: 4 | Average Loss: 0.0234 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0729 | Current Mini-batch loss: 0.1225\n",
      "Training | EPOCH: 4 | Average Loss: 0.0365 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0279 | Current Mini-batch loss: 0.0373\n",
      "Training | EPOCH: 4 | Average Loss: 0.0460 | Current Mini-batch loss: 0.0642\n",
      "Training | EPOCH: 4 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 4 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.1532 | Current Mini-batch loss: 0.3051\n",
      "Training | EPOCH: 4 | Average Loss: 0.0802 | Current Mini-batch loss: 0.0072\n",
      "Training | EPOCH: 4 | Average Loss: 0.0437 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 4 | Average Loss: 0.0232 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0914\n",
      "Training | EPOCH: 4 | Average Loss: 0.0295 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 4 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0185\n",
      "Training | EPOCH: 4 | Average Loss: 0.0083 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 4 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 4 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 4 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0521 | Current Mini-batch loss: 0.0989\n",
      "Training | EPOCH: 4 | Average Loss: 0.0261 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.1833 | Current Mini-batch loss: 0.3525\n",
      "Training | EPOCH: 4 | Average Loss: 0.1983 | Current Mini-batch loss: 0.2132\n",
      "Training | EPOCH: 4 | Average Loss: 0.0993 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0528 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 4 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 4 | Average Loss: 0.0416 | Current Mini-batch loss: 0.0558\n",
      "Training | EPOCH: 4 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0139\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 4 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0608\n",
      "Training | EPOCH: 4 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.2038 | Current Mini-batch loss: 0.4055\n",
      "Training | EPOCH: 4 | Average Loss: 0.1374 | Current Mini-batch loss: 0.0710\n",
      "Training | EPOCH: 4 | Average Loss: 0.0772 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 4 | Average Loss: 0.0386 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 4 | Average Loss: 0.3553 | Current Mini-batch loss: 0.6894\n",
      "Training | EPOCH: 4 | Average Loss: 0.1777 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.1579 | Current Mini-batch loss: 0.1381\n",
      "Training | EPOCH: 4 | Average Loss: 0.0911 | Current Mini-batch loss: 0.0243\n",
      "Training | EPOCH: 4 | Average Loss: 0.0671 | Current Mini-batch loss: 0.0431\n",
      "Training | EPOCH: 4 | Average Loss: 0.0449 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 4 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0145 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 4 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 4 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 4 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0129\n",
      "Training | EPOCH: 4 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0988\n",
      "Training | EPOCH: 4 | Average Loss: 0.0612 | Current Mini-batch loss: 0.0725\n",
      "Training | EPOCH: 4 | Average Loss: 0.2079 | Current Mini-batch loss: 0.3545\n",
      "Training | EPOCH: 4 | Average Loss: 0.1052 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0531 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0581 | Current Mini-batch loss: 0.0632\n",
      "Training | EPOCH: 4 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0337\n",
      "Training | EPOCH: 4 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0298 | Current Mini-batch loss: 0.0479\n",
      "Training | EPOCH: 4 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0272\n",
      "Training | EPOCH: 4 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0171\n",
      "Training | EPOCH: 4 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 4 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0394 | Current Mini-batch loss: 0.0737\n",
      "Training | EPOCH: 4 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 4 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0189\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.1475 | Current Mini-batch loss: 0.2883\n",
      "Training | EPOCH: 4 | Average Loss: 0.0743 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0388 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0821 | Current Mini-batch loss: 0.1254\n",
      "Training | EPOCH: 4 | Average Loss: 0.0432 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 4 | Average Loss: 0.0678 | Current Mini-batch loss: 0.0925\n",
      "Training | EPOCH: 4 | Average Loss: 0.0422 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 4 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0510\n",
      "Training | EPOCH: 4 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0161\n",
      "Training | EPOCH: 4 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 4 | Average Loss: 0.5087 | Current Mini-batch loss: 1.0119\n",
      "Training | EPOCH: 4 | Average Loss: 0.2640 | Current Mini-batch loss: 0.0193\n",
      "Training | EPOCH: 4 | Average Loss: 0.1343 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 4 | Average Loss: 0.1398 | Current Mini-batch loss: 0.1452\n",
      "Training | EPOCH: 4 | Average Loss: 0.0708 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0356 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.5682 | Current Mini-batch loss: 1.1327\n",
      "Training | EPOCH: 4 | Average Loss: 0.2843 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.1423 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0715 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0363 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0136\n",
      "Training | EPOCH: 4 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 4 | Average Loss: 0.0929 | Current Mini-batch loss: 0.1774\n",
      "Training | EPOCH: 4 | Average Loss: 0.0491 | Current Mini-batch loss: 0.0052\n",
      "Training | EPOCH: 4 | Average Loss: 0.0691 | Current Mini-batch loss: 0.0891\n",
      "Training | EPOCH: 4 | Average Loss: 0.0351 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.1109 | Current Mini-batch loss: 0.1867\n",
      "Training | EPOCH: 4 | Average Loss: 0.0582 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 4 | Average Loss: 0.1680 | Current Mini-batch loss: 0.2777\n",
      "Training | EPOCH: 4 | Average Loss: 0.0843 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0433 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 4 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.4695 | Current Mini-batch loss: 0.9267\n",
      "Training | EPOCH: 4 | Average Loss: 0.2422 | Current Mini-batch loss: 0.0150\n",
      "Training | EPOCH: 4 | Average Loss: 0.1211 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0608 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0308 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0830 | Current Mini-batch loss: 0.1352\n",
      "Training | EPOCH: 4 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0085\n",
      "Training | EPOCH: 4 | Average Loss: 0.1338 | Current Mini-batch loss: 0.2219\n",
      "Training | EPOCH: 4 | Average Loss: 0.0710 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 4 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 4 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 4 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0282\n",
      "Training | EPOCH: 4 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0884 | Current Mini-batch loss: 0.1709\n",
      "Training | EPOCH: 4 | Average Loss: 0.6546 | Current Mini-batch loss: 1.2209\n",
      "Training | EPOCH: 4 | Average Loss: 0.3305 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 4 | Average Loss: 0.1653 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.5731 | Current Mini-batch loss: 0.9808\n",
      "Training | EPOCH: 4 | Average Loss: 0.2897 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 4 | Average Loss: 0.1496 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 4 | Average Loss: 0.1830 | Current Mini-batch loss: 0.2163\n",
      "Training | EPOCH: 4 | Average Loss: 0.6042 | Current Mini-batch loss: 1.0254\n",
      "Training | EPOCH: 4 | Average Loss: 0.8648 | Current Mini-batch loss: 1.1255\n",
      "Training | EPOCH: 4 | Average Loss: 1.7691 | Current Mini-batch loss: 2.6734\n",
      "Training | EPOCH: 4 | Average Loss: 0.8890 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 4 | Average Loss: 0.4485 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 4 | Average Loss: 0.2250 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 4 | Average Loss: 0.3185 | Current Mini-batch loss: 0.4120\n",
      "Training | EPOCH: 4 | Average Loss: 0.3213 | Current Mini-batch loss: 0.3242\n",
      "Training | EPOCH: 4 | Average Loss: 0.1619 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.1941 | Current Mini-batch loss: 0.2263\n",
      "Training | EPOCH: 4 | Average Loss: 0.1101 | Current Mini-batch loss: 0.0262\n",
      "Training | EPOCH: 4 | Average Loss: 0.0584 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 4 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 4 | Average Loss: 0.0241 | Current Mini-batch loss: 0.0324\n",
      "Training | EPOCH: 4 | Average Loss: 0.0122 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 4 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 4 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 4 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 4 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 4 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0837\n",
      "Training | EPOCH: 4 | Average Loss: 0.0566 | Current Mini-batch loss: 0.0697\n",
      "Training | EPOCH: 4 | Average Loss: 0.0348 | Current Mini-batch loss: 0.0129\n",
      "Training | EPOCH: 4 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0649\n",
      "Training | EPOCH: 4 | Average Loss: 0.1071 | Current Mini-batch loss: 0.1644\n",
      "Training | EPOCH: 4 | Average Loss: 0.0598 | Current Mini-batch loss: 0.0125\n",
      "Training | EPOCH: 4 | Average Loss: 0.0299 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0611 | Current Mini-batch loss: 0.0922\n",
      "Training | EPOCH: 4 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0427 | Current Mini-batch loss: 0.0699\n",
      "Training | EPOCH: 4 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0495 | Current Mini-batch loss: 0.0874\n",
      "Training | EPOCH: 4 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0320\n",
      "Training | EPOCH: 4 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 4 | Average Loss: 0.0271 | Current Mini-batch loss: 0.0279\n",
      "Training | EPOCH: 4 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0162\n",
      "Training | EPOCH: 4 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 4 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0130\n",
      "Training | EPOCH: 4 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 4 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 4 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 4 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0544 | Current Mini-batch loss: 0.1043\n",
      "Training | EPOCH: 4 | Average Loss: 0.1297 | Current Mini-batch loss: 0.2049\n",
      "Training | EPOCH: 4 | Average Loss: 0.0655 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0340 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 4 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 4 | Average Loss: 0.0547 | Current Mini-batch loss: 0.0920\n",
      "Training | EPOCH: 4 | Average Loss: 0.0274 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 4 | Average Loss: 0.0595 | Current Mini-batch loss: 0.1036\n",
      "Training | EPOCH: 4 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 4 | Average Loss: 0.1996 | Current Mini-batch loss: 0.3653\n",
      "Training | EPOCH: 4 | Average Loss: 0.1000 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0503 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 4 | Average Loss: 0.0721 | Current Mini-batch loss: 0.0939\n",
      "Training | EPOCH: 4 | Average Loss: 0.0646 | Current Mini-batch loss: 0.0570\n",
      "Training | EPOCH: 4 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 4 | Average Loss: 0.0831 | Current Mini-batch loss: 0.1323\n",
      "Training | EPOCH: 4 | Average Loss: 0.1127 | Current Mini-batch loss: 0.1423\n",
      "Training | EPOCH: 4 | Average Loss: 0.0595 | Current Mini-batch loss: 0.0062\n",
      "Training | EPOCH: 4 | Average Loss: 0.0770 | Current Mini-batch loss: 0.0944\n",
      "Training | EPOCH: 4 | Average Loss: 0.0399 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 4 | Average Loss: 0.0609 | Current Mini-batch loss: 0.0819\n",
      "Training | EPOCH: 4 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0150\n",
      "Training | EPOCH: 4 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 4 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 4 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0651\n",
      "Training | EPOCH: 4 | Average Loss: 0.0422 | Current Mini-batch loss: 0.0457\n",
      "Training | EPOCH: 4 | Average Loss: 0.0328 | Current Mini-batch loss: 0.0234\n",
      "Training | EPOCH: 4 | Average Loss: 0.0170 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0928 | Current Mini-batch loss: 0.1771\n",
      "Training | EPOCH: 4 | Average Loss: 0.2598 | Current Mini-batch loss: 0.4268\n",
      "Training | EPOCH: 4 | Average Loss: 0.1301 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0650 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.1125 | Current Mini-batch loss: 0.1599\n",
      "Training | EPOCH: 4 | Average Loss: 0.0571 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 4 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0161\n",
      "Training | EPOCH: 4 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 4 | Average Loss: 0.0386 | Current Mini-batch loss: 0.0569\n",
      "Training | EPOCH: 4 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0248\n",
      "Training | EPOCH: 4 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 4 | Average Loss: 0.0725 | Current Mini-batch loss: 0.1338\n",
      "Training | EPOCH: 4 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0188 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 4 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 4 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 4 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 4 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0124\n",
      "Training | EPOCH: 4 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 4 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0275\n",
      "Training | EPOCH: 4 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0019\n",
      "Training | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 4 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 4 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 4 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 4 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 4 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 4 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 4 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0057\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0101\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0045\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0233\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0022\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0216 | Current Mini-batch loss: 0.0427\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0029\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0034\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0492 | Current Mini-batch loss: 0.0981\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0846 | Current Mini-batch loss: 0.1201\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0424 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0214 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0259\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0044\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0365\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0309 | Current Mini-batch loss: 0.0417\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0586 | Current Mini-batch loss: 0.1148\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0293 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0885 | Current Mini-batch loss: 0.1476\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0469 | Current Mini-batch loss: 0.0053\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1714 | Current Mini-batch loss: 0.3193\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0858 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0436 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0295 | Current Mini-batch loss: 0.0561\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0898 | Current Mini-batch loss: 0.1501\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0063\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.2141 | Current Mini-batch loss: 0.4152\n",
      "Validation | EPOCH: 4 | Average Loss: 0.3012 | Current Mini-batch loss: 0.3884\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1506 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0755 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0740 | Current Mini-batch loss: 0.0724\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0622 | Current Mini-batch loss: 0.0504\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0052\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0158\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.2520 | Current Mini-batch loss: 0.5029\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1262 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0631 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1155 | Current Mini-batch loss: 0.1679\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0608 | Current Mini-batch loss: 0.0062\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0305 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0925\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0237 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0051\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0019\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0108\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1773 | Current Mini-batch loss: 0.3483\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1175 | Current Mini-batch loss: 0.0577\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0588 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0573 | Current Mini-batch loss: 0.0558\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0028\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1018 | Current Mini-batch loss: 0.1878\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0510 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0255 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0943 | Current Mini-batch loss: 0.1821\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0487 | Current Mini-batch loss: 0.0030\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0244 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0310\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0049\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0431 | Current Mini-batch loss: 0.0854\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0140\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.1621 | Current Mini-batch loss: 0.3235\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0812 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0407 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0138\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0011\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0411 | Current Mini-batch loss: 0.0796\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0453\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0436\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0546\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0049\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0142\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0226\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0361 | Current Mini-batch loss: 0.0670\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0409 | Current Mini-batch loss: 0.0635\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0212 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0056\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0298\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0851 | Current Mini-batch loss: 0.1624\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0425 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0079\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0162 | Current Mini-batch loss: 0.0195\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0328\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0016\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0831 | Current Mini-batch loss: 0.1654\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0439 | Current Mini-batch loss: 0.0047\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0306 | Current Mini-batch loss: 0.0392\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0115\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0893 | Current Mini-batch loss: 0.1652\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0448 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 4 | Average Loss: 0.0839 | Current Mini-batch loss: 0.1230\n",
      "Training | EPOCH: 5 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0128\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 5 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 5 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0463\n",
      "Training | EPOCH: 5 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0816 | Current Mini-batch loss: 0.1498\n",
      "Training | EPOCH: 5 | Average Loss: 0.5219 | Current Mini-batch loss: 0.9622\n",
      "Training | EPOCH: 5 | Average Loss: 0.2612 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.3352 | Current Mini-batch loss: 0.4091\n",
      "Training | EPOCH: 5 | Average Loss: 0.1676 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0838 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0419 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.0129 | Current Mini-batch loss: 0.0133\n",
      "Training | EPOCH: 5 | Average Loss: 0.2780 | Current Mini-batch loss: 0.5431\n",
      "Training | EPOCH: 5 | Average Loss: 0.1583 | Current Mini-batch loss: 0.0387\n",
      "Training | EPOCH: 5 | Average Loss: 0.0793 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0412 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 5 | Average Loss: 0.0220 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.1669 | Current Mini-batch loss: 0.3225\n",
      "Training | EPOCH: 5 | Average Loss: 0.0865 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 5 | Average Loss: 0.0434 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0273 | Current Mini-batch loss: 0.0111\n",
      "Training | EPOCH: 5 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 5 | Average Loss: 0.0406 | Current Mini-batch loss: 0.0678\n",
      "Training | EPOCH: 5 | Average Loss: 0.1565 | Current Mini-batch loss: 0.2724\n",
      "Training | EPOCH: 5 | Average Loss: 0.1028 | Current Mini-batch loss: 0.0490\n",
      "Training | EPOCH: 5 | Average Loss: 0.5026 | Current Mini-batch loss: 0.9024\n",
      "Training | EPOCH: 5 | Average Loss: 0.2785 | Current Mini-batch loss: 0.0545\n",
      "Training | EPOCH: 5 | Average Loss: 0.1396 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0698 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0845 | Current Mini-batch loss: 0.0993\n",
      "Training | EPOCH: 5 | Average Loss: 0.0438 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0807 | Current Mini-batch loss: 0.1383\n",
      "Training | EPOCH: 5 | Average Loss: 0.0407 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.1491 | Current Mini-batch loss: 0.2574\n",
      "Training | EPOCH: 5 | Average Loss: 0.1122 | Current Mini-batch loss: 0.0753\n",
      "Training | EPOCH: 5 | Average Loss: 0.0561 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0831 | Current Mini-batch loss: 0.1582\n",
      "Training | EPOCH: 5 | Average Loss: 0.0444 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 5 | Average Loss: 0.0318 | Current Mini-batch loss: 0.0192\n",
      "Training | EPOCH: 5 | Average Loss: 0.4329 | Current Mini-batch loss: 0.8340\n",
      "Training | EPOCH: 5 | Average Loss: 0.2228 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 5 | Average Loss: 0.1115 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.2905 | Current Mini-batch loss: 0.4694\n",
      "Training | EPOCH: 5 | Average Loss: 0.1493 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 5 | Average Loss: 0.0761 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 5 | Average Loss: 0.0383 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0231 | Current Mini-batch loss: 0.0078\n",
      "Training | EPOCH: 5 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0078\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0874\n",
      "Training | EPOCH: 5 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0120\n",
      "Training | EPOCH: 5 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.4873 | Current Mini-batch loss: 0.9721\n",
      "Training | EPOCH: 5 | Average Loss: 0.2441 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.1222 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0624 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0160 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.1001 | Current Mini-batch loss: 0.1989\n",
      "Training | EPOCH: 5 | Average Loss: 0.0505 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 5 | Average Loss: 0.0981 | Current Mini-batch loss: 0.1626\n",
      "Training | EPOCH: 5 | Average Loss: 0.0510 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.2668 | Current Mini-batch loss: 0.4826\n",
      "Training | EPOCH: 5 | Average Loss: 0.1501 | Current Mini-batch loss: 0.0334\n",
      "Training | EPOCH: 5 | Average Loss: 0.0796 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 5 | Average Loss: 0.0399 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0312\n",
      "Training | EPOCH: 5 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.1219 | Current Mini-batch loss: 0.2421\n",
      "Training | EPOCH: 5 | Average Loss: 0.0611 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0389 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 5 | Average Loss: 0.0197 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.1603 | Current Mini-batch loss: 0.3008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0807 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0412 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 5 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 5 | Average Loss: 0.1056 | Current Mini-batch loss: 0.2069\n",
      "Training | EPOCH: 5 | Average Loss: 0.0657 | Current Mini-batch loss: 0.0257\n",
      "Training | EPOCH: 5 | Average Loss: 0.0423 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 5 | Average Loss: 0.2242 | Current Mini-batch loss: 0.4061\n",
      "Training | EPOCH: 5 | Average Loss: 0.1123 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.1155 | Current Mini-batch loss: 0.1187\n",
      "Training | EPOCH: 5 | Average Loss: 0.0582 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0214 | Current Mini-batch loss: 0.0343\n",
      "Training | EPOCH: 5 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 5 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.2967 | Current Mini-batch loss: 0.5906\n",
      "Training | EPOCH: 5 | Average Loss: 0.2995 | Current Mini-batch loss: 0.3023\n",
      "Training | EPOCH: 5 | Average Loss: 0.1499 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0750 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.2996 | Current Mini-batch loss: 0.5241\n",
      "Training | EPOCH: 5 | Average Loss: 0.1499 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0771 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 5 | Average Loss: 0.0612 | Current Mini-batch loss: 0.0453\n",
      "Training | EPOCH: 5 | Average Loss: 0.0311 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0719 | Current Mini-batch loss: 0.1127\n",
      "Training | EPOCH: 5 | Average Loss: 0.0367 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 5 | Average Loss: 0.1882 | Current Mini-batch loss: 0.3559\n",
      "Training | EPOCH: 5 | Average Loss: 0.0958 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.2083 | Current Mini-batch loss: 0.3208\n",
      "Training | EPOCH: 5 | Average Loss: 0.1896 | Current Mini-batch loss: 0.1708\n",
      "Training | EPOCH: 5 | Average Loss: 0.0952 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0480 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0250 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 5 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0218 | Current Mini-batch loss: 0.0402\n",
      "Training | EPOCH: 5 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0240\n",
      "Training | EPOCH: 5 | Average Loss: 0.0172 | Current Mini-batch loss: 0.0170\n",
      "Training | EPOCH: 5 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 5 | Average Loss: 0.0109 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 5 | Average Loss: 0.2305 | Current Mini-batch loss: 0.4502\n",
      "Training | EPOCH: 5 | Average Loss: 0.1159 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0662 | Current Mini-batch loss: 0.0164\n",
      "Training | EPOCH: 5 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0353\n",
      "Training | EPOCH: 5 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0153\n",
      "Training | EPOCH: 5 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 5 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0409 | Current Mini-batch loss: 0.0806\n",
      "Training | EPOCH: 5 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 5 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0129\n",
      "Training | EPOCH: 5 | Average Loss: 0.0281 | Current Mini-batch loss: 0.0386\n",
      "Training | EPOCH: 5 | Average Loss: 0.0142 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0263\n",
      "Training | EPOCH: 5 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.2536 | Current Mini-batch loss: 0.5028\n",
      "Training | EPOCH: 5 | Average Loss: 0.1268 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0636 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0330 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0341\n",
      "Training | EPOCH: 5 | Average Loss: 0.0169 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 5 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.0055 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 5 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0360\n",
      "Training | EPOCH: 5 | Average Loss: 0.3423 | Current Mini-batch loss: 0.6639\n",
      "Training | EPOCH: 5 | Average Loss: 0.1713 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.1338 | Current Mini-batch loss: 0.0963\n",
      "Training | EPOCH: 5 | Average Loss: 0.0873 | Current Mini-batch loss: 0.0408\n",
      "Training | EPOCH: 5 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 5 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0274\n",
      "Training | EPOCH: 5 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 5 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0465 | Current Mini-batch loss: 0.0862\n",
      "Training | EPOCH: 5 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0369 | Current Mini-batch loss: 0.0726\n",
      "Training | EPOCH: 5 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 5 | Average Loss: 0.0118 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0267 | Current Mini-batch loss: 0.0416\n",
      "Training | EPOCH: 5 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 5 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0291\n",
      "Training | EPOCH: 5 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.1101 | Current Mini-batch loss: 0.2112\n",
      "Training | EPOCH: 5 | Average Loss: 0.0622 | Current Mini-batch loss: 0.0144\n",
      "Training | EPOCH: 5 | Average Loss: 0.0311 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0139\n",
      "Training | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0164 | Current Mini-batch loss: 0.0210\n",
      "Training | EPOCH: 5 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.3043 | Current Mini-batch loss: 0.6003\n",
      "Training | EPOCH: 5 | Average Loss: 0.1534 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0815 | Current Mini-batch loss: 0.0096\n",
      "Training | EPOCH: 5 | Average Loss: 0.0890 | Current Mini-batch loss: 0.0965\n",
      "Training | EPOCH: 5 | Average Loss: 0.0528 | Current Mini-batch loss: 0.0165\n",
      "Training | EPOCH: 5 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0132 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.6626 | Current Mini-batch loss: 1.3178\n",
      "Training | EPOCH: 5 | Average Loss: 1.1164 | Current Mini-batch loss: 1.5702\n",
      "Training | EPOCH: 5 | Average Loss: 0.5587 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.2800 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.1402 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0843 | Current Mini-batch loss: 0.0284\n",
      "Training | EPOCH: 5 | Average Loss: 0.0422 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0084 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 5 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0881 | Current Mini-batch loss: 0.1725\n",
      "Training | EPOCH: 5 | Average Loss: 0.0444 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0228 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0117 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.1364 | Current Mini-batch loss: 0.2612\n",
      "Training | EPOCH: 5 | Average Loss: 0.0684 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0511 | Current Mini-batch loss: 0.0337\n",
      "Training | EPOCH: 5 | Average Loss: 0.0256 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0042 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 5 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0699 | Current Mini-batch loss: 0.1384\n",
      "Training | EPOCH: 5 | Average Loss: 0.0454 | Current Mini-batch loss: 0.0210\n",
      "Training | EPOCH: 5 | Average Loss: 0.0247 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 5 | Average Loss: 0.5140 | Current Mini-batch loss: 1.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.3642 | Current Mini-batch loss: 0.2145\n",
      "Training | EPOCH: 5 | Average Loss: 0.1823 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.1709 | Current Mini-batch loss: 0.1595\n",
      "Training | EPOCH: 5 | Average Loss: 0.1065 | Current Mini-batch loss: 0.0421\n",
      "Training | EPOCH: 5 | Average Loss: 0.0752 | Current Mini-batch loss: 0.0439\n",
      "Training | EPOCH: 5 | Average Loss: 0.0377 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0098 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0318\n",
      "Training | EPOCH: 5 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0079\n",
      "Training | EPOCH: 5 | Average Loss: 0.1209 | Current Mini-batch loss: 0.2276\n",
      "Training | EPOCH: 5 | Average Loss: 0.0606 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0305 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0412 | Current Mini-batch loss: 0.0746\n",
      "Training | EPOCH: 5 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0020\n",
      "Training | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0320\n",
      "Training | EPOCH: 5 | Average Loss: 0.0191 | Current Mini-batch loss: 0.0191\n",
      "Training | EPOCH: 5 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0455\n",
      "Training | EPOCH: 5 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 5 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 5 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0608 | Current Mini-batch loss: 0.1180\n",
      "Training | EPOCH: 5 | Average Loss: 0.1038 | Current Mini-batch loss: 0.1468\n",
      "Training | EPOCH: 5 | Average Loss: 0.0530 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0320 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 5 | Average Loss: 0.0323 | Current Mini-batch loss: 0.0326\n",
      "Training | EPOCH: 5 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0421\n",
      "Training | EPOCH: 5 | Average Loss: 0.0138 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0641 | Current Mini-batch loss: 0.1211\n",
      "Training | EPOCH: 5 | Average Loss: 0.0321 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0264 | Current Mini-batch loss: 0.0207\n",
      "Training | EPOCH: 5 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0143\n",
      "Training | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 5 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0320\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.1350 | Current Mini-batch loss: 0.2661\n",
      "Training | EPOCH: 5 | Average Loss: 0.0704 | Current Mini-batch loss: 0.0058\n",
      "Training | EPOCH: 5 | Average Loss: 0.0538 | Current Mini-batch loss: 0.0372\n",
      "Training | EPOCH: 5 | Average Loss: 0.0345 | Current Mini-batch loss: 0.0151\n",
      "Training | EPOCH: 5 | Average Loss: 0.0277 | Current Mini-batch loss: 0.0208\n",
      "Training | EPOCH: 5 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 5 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0349\n",
      "Training | EPOCH: 5 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0077\n",
      "Training | EPOCH: 5 | Average Loss: 0.0137 | Current Mini-batch loss: 0.0109\n",
      "Training | EPOCH: 5 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0406\n",
      "Training | EPOCH: 5 | Average Loss: 0.0123 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0692\n",
      "Training | EPOCH: 5 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 5 | Average Loss: 0.2040 | Current Mini-batch loss: 0.3988\n",
      "Training | EPOCH: 5 | Average Loss: 0.1024 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0683 | Current Mini-batch loss: 0.0343\n",
      "Training | EPOCH: 5 | Average Loss: 0.0358 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0165 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 5 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0624\n",
      "Training | EPOCH: 5 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 5 | Average Loss: 0.0435 | Current Mini-batch loss: 0.0617\n",
      "Training | EPOCH: 5 | Average Loss: 0.0221 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0175\n",
      "Training | EPOCH: 5 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 5 | Average Loss: 0.1958 | Current Mini-batch loss: 0.3824\n",
      "Training | EPOCH: 5 | Average Loss: 0.0986 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.1245 | Current Mini-batch loss: 0.1503\n",
      "Training | EPOCH: 5 | Average Loss: 0.0834 | Current Mini-batch loss: 0.0423\n",
      "Training | EPOCH: 5 | Average Loss: 0.0716 | Current Mini-batch loss: 0.0598\n",
      "Training | EPOCH: 5 | Average Loss: 0.0358 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 5 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0137\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.1106 | Current Mini-batch loss: 0.2200\n",
      "Training | EPOCH: 5 | Average Loss: 0.0561 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0283 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0341 | Current Mini-batch loss: 0.0399\n",
      "Training | EPOCH: 5 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0336\n",
      "Training | EPOCH: 5 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0278 | Current Mini-batch loss: 0.0521\n",
      "Training | EPOCH: 5 | Average Loss: 0.0452 | Current Mini-batch loss: 0.0626\n",
      "Training | EPOCH: 5 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.1733 | Current Mini-batch loss: 0.3237\n",
      "Training | EPOCH: 5 | Average Loss: 0.2214 | Current Mini-batch loss: 0.2694\n",
      "Training | EPOCH: 5 | Average Loss: 0.1157 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 5 | Average Loss: 0.0580 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0318 | Current Mini-batch loss: 0.0334\n",
      "Training | EPOCH: 5 | Average Loss: 0.0304 | Current Mini-batch loss: 0.0289\n",
      "Training | EPOCH: 5 | Average Loss: 0.0419 | Current Mini-batch loss: 0.0534\n",
      "Training | EPOCH: 5 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.2688 | Current Mini-batch loss: 0.5162\n",
      "Training | EPOCH: 5 | Average Loss: 0.4690 | Current Mini-batch loss: 0.6693\n",
      "Training | EPOCH: 5 | Average Loss: 0.2382 | Current Mini-batch loss: 0.0073\n",
      "Training | EPOCH: 5 | Average Loss: 0.1282 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 5 | Average Loss: 0.0950 | Current Mini-batch loss: 0.0618\n",
      "Training | EPOCH: 5 | Average Loss: 0.0621 | Current Mini-batch loss: 0.0293\n",
      "Training | EPOCH: 5 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0159 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0095\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0438\n",
      "Training | EPOCH: 5 | Average Loss: 0.0275 | Current Mini-batch loss: 0.0298\n",
      "Training | EPOCH: 5 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0562 | Current Mini-batch loss: 0.1039\n",
      "Training | EPOCH: 5 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 5 | Average Loss: 0.0179 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0359 | Current Mini-batch loss: 0.0539\n",
      "Training | EPOCH: 5 | Average Loss: 0.0321 | Current Mini-batch loss: 0.0282\n",
      "Training | EPOCH: 5 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 5 | Average Loss: 0.0819 | Current Mini-batch loss: 0.1425\n",
      "Training | EPOCH: 5 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0081\n",
      "Training | EPOCH: 5 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0113 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 5 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.0877 | Current Mini-batch loss: 0.1685\n",
      "Training | EPOCH: 5 | Average Loss: 0.2127 | Current Mini-batch loss: 0.3376\n",
      "Training | EPOCH: 5 | Average Loss: 0.1068 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0534 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0312 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 5 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0194\n",
      "Training | EPOCH: 5 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.3292 | Current Mini-batch loss: 0.6496\n",
      "Training | EPOCH: 5 | Average Loss: 0.1692 | Current Mini-batch loss: 0.0092\n",
      "Training | EPOCH: 5 | Average Loss: 0.1138 | Current Mini-batch loss: 0.0585\n",
      "Training | EPOCH: 5 | Average Loss: 0.0573 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.2509 | Current Mini-batch loss: 0.4446\n",
      "Training | EPOCH: 5 | Average Loss: 2.2637 | Current Mini-batch loss: 4.2764\n",
      "Training | EPOCH: 5 | Average Loss: 1.7101 | Current Mini-batch loss: 1.1565\n",
      "Training | EPOCH: 5 | Average Loss: 0.8568 | Current Mini-batch loss: 0.0036\n",
      "Training | EPOCH: 5 | Average Loss: 0.6577 | Current Mini-batch loss: 0.4585\n",
      "Training | EPOCH: 5 | Average Loss: 0.3295 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.1767 | Current Mini-batch loss: 0.0239\n",
      "Training | EPOCH: 5 | Average Loss: 0.0884 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0592 | Current Mini-batch loss: 0.0299\n",
      "Training | EPOCH: 5 | Average Loss: 0.1833 | Current Mini-batch loss: 0.3075\n",
      "Training | EPOCH: 5 | Average Loss: 0.0917 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0319 | Current Mini-batch loss: 0.0164\n",
      "Training | EPOCH: 5 | Average Loss: 0.0459 | Current Mini-batch loss: 0.0600\n",
      "Training | EPOCH: 5 | Average Loss: 0.0310 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 5 | Average Loss: 0.3121 | Current Mini-batch loss: 0.5933\n",
      "Training | EPOCH: 5 | Average Loss: 0.1570 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0789 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.2432 | Current Mini-batch loss: 0.4075\n",
      "Training | EPOCH: 5 | Average Loss: 0.1217 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0610 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 5 | Average Loss: 0.0121 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 5 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0086\n",
      "Training | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.1873 | Current Mini-batch loss: 0.3732\n",
      "Training | EPOCH: 5 | Average Loss: 0.0941 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0472 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0267 | Current Mini-batch loss: 0.0281\n",
      "Training | EPOCH: 5 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0167 | Current Mini-batch loss: 0.0198\n",
      "Training | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0662 | Current Mini-batch loss: 0.1300\n",
      "Training | EPOCH: 5 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0170 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0551 | Current Mini-batch loss: 0.1091\n",
      "Training | EPOCH: 5 | Average Loss: 0.0301 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 5 | Average Loss: 0.1345 | Current Mini-batch loss: 0.2389\n",
      "Training | EPOCH: 5 | Average Loss: 0.0673 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0351 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0548\n",
      "Training | EPOCH: 5 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 5 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.1436 | Current Mini-batch loss: 0.2727\n",
      "Training | EPOCH: 5 | Average Loss: 0.0719 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0397 | Current Mini-batch loss: 0.0074\n",
      "Training | EPOCH: 5 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 5 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0347\n",
      "Training | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0316 | Current Mini-batch loss: 0.0607\n",
      "Training | EPOCH: 5 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 5 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0383 | Current Mini-batch loss: 0.0763\n",
      "Training | EPOCH: 5 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.1653 | Current Mini-batch loss: 0.3199\n",
      "Training | EPOCH: 5 | Average Loss: 0.0827 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0765 | Current Mini-batch loss: 0.0703\n",
      "Training | EPOCH: 5 | Average Loss: 0.0982 | Current Mini-batch loss: 0.1199\n",
      "Training | EPOCH: 5 | Average Loss: 0.0491 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0347 | Current Mini-batch loss: 0.0203\n",
      "Training | EPOCH: 5 | Average Loss: 0.0211 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 5 | Average Loss: 0.3375 | Current Mini-batch loss: 0.6538\n",
      "Training | EPOCH: 5 | Average Loss: 0.1689 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.1836 | Current Mini-batch loss: 0.1983\n",
      "Training | EPOCH: 5 | Average Loss: 0.2576 | Current Mini-batch loss: 0.3316\n",
      "Training | EPOCH: 5 | Average Loss: 0.1306 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0659 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0329 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.1577 | Current Mini-batch loss: 0.2987\n",
      "Training | EPOCH: 5 | Average Loss: 0.0792 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0396 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0986 | Current Mini-batch loss: 0.1916\n",
      "Training | EPOCH: 5 | Average Loss: 0.0591 | Current Mini-batch loss: 0.0196\n",
      "Training | EPOCH: 5 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0078 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0283\n",
      "Training | EPOCH: 5 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 5 | Average Loss: 0.0762 | Current Mini-batch loss: 0.1409\n",
      "Training | EPOCH: 5 | Average Loss: 0.0381 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.7278 | Current Mini-batch loss: 1.4447\n",
      "Training | EPOCH: 5 | Average Loss: 0.4452 | Current Mini-batch loss: 0.1625\n",
      "Training | EPOCH: 5 | Average Loss: 0.2226 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.1113 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0578 | Current Mini-batch loss: 0.0042\n",
      "Training | EPOCH: 5 | Average Loss: 0.0297 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0149 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0270\n",
      "Training | EPOCH: 5 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.3294 | Current Mini-batch loss: 0.6528\n",
      "Training | EPOCH: 5 | Average Loss: 0.1671 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 5 | Average Loss: 0.0850 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0569 | Current Mini-batch loss: 0.0288\n",
      "Training | EPOCH: 5 | Average Loss: 0.0810 | Current Mini-batch loss: 0.1051\n",
      "Training | EPOCH: 5 | Average Loss: 0.0411 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 5 | Average Loss: 0.0180 | Current Mini-batch loss: 0.0114\n",
      "Training | EPOCH: 5 | Average Loss: 0.0097 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0396\n",
      "Training | EPOCH: 5 | Average Loss: 0.0230 | Current Mini-batch loss: 0.0213\n",
      "Training | EPOCH: 5 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0196\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0733 | Current Mini-batch loss: 0.1407\n",
      "Training | EPOCH: 5 | Average Loss: 0.0692 | Current Mini-batch loss: 0.0650\n",
      "Training | EPOCH: 5 | Average Loss: 0.0366 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 5 | Average Loss: 0.0198 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0100 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0899 | Current Mini-batch loss: 0.1698\n",
      "Training | EPOCH: 5 | Average Loss: 0.4748 | Current Mini-batch loss: 0.8596\n",
      "Training | EPOCH: 5 | Average Loss: 0.2381 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.1191 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0599 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0150 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0386\n",
      "Training | EPOCH: 5 | Average Loss: 0.0115 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0067\n",
      "Training | EPOCH: 5 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0461\n",
      "Training | EPOCH: 5 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 5 | Average Loss: 0.0087 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0089 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 5 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0001 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0695 | Current Mini-batch loss: 0.1384\n",
      "Training | EPOCH: 5 | Average Loss: 0.0790 | Current Mini-batch loss: 0.0886\n",
      "Training | EPOCH: 5 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0708\n",
      "Training | EPOCH: 5 | Average Loss: 0.0189 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0507 | Current Mini-batch loss: 0.0914\n",
      "Training | EPOCH: 5 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0068\n",
      "Training | EPOCH: 5 | Average Loss: 0.0166 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0071\n",
      "Training | EPOCH: 5 | Average Loss: 0.0064 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0028\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0040\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0079 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 5 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0100\n",
      "Training | EPOCH: 5 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.3218 | Current Mini-batch loss: 0.6385\n",
      "Training | EPOCH: 5 | Average Loss: 1.0792 | Current Mini-batch loss: 1.8366\n",
      "Training | EPOCH: 5 | Average Loss: 0.5399 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.2711 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.1356 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0679 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0374 | Current Mini-batch loss: 0.0070\n",
      "Training | EPOCH: 5 | Average Loss: 0.0410 | Current Mini-batch loss: 0.0446\n",
      "Training | EPOCH: 5 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0268\n",
      "Training | EPOCH: 5 | Average Loss: 0.0361 | Current Mini-batch loss: 0.0384\n",
      "Training | EPOCH: 5 | Average Loss: 0.0184 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0392 | Current Mini-batch loss: 0.0599\n",
      "Training | EPOCH: 5 | Average Loss: 0.0414 | Current Mini-batch loss: 0.0437\n",
      "Training | EPOCH: 5 | Average Loss: 0.0237 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 5 | Average Loss: 0.0805 | Current Mini-batch loss: 0.1373\n",
      "Training | EPOCH: 5 | Average Loss: 0.2081 | Current Mini-batch loss: 0.3358\n",
      "Training | EPOCH: 5 | Average Loss: 0.1972 | Current Mini-batch loss: 0.1862\n",
      "Training | EPOCH: 5 | Average Loss: 0.0997 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 5 | Average Loss: 0.0154 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0515 | Current Mini-batch loss: 0.0951\n",
      "Training | EPOCH: 5 | Average Loss: 0.0266 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0076\n",
      "Training | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.2455 | Current Mini-batch loss: 0.4901\n",
      "Training | EPOCH: 5 | Average Loss: 0.2339 | Current Mini-batch loss: 0.2223\n",
      "Training | EPOCH: 5 | Average Loss: 0.1213 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 5 | Average Loss: 0.0610 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0307 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.1467 | Current Mini-batch loss: 0.2780\n",
      "Training | EPOCH: 5 | Average Loss: 0.0737 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0557 | Current Mini-batch loss: 0.0377\n",
      "Training | EPOCH: 5 | Average Loss: 0.3563 | Current Mini-batch loss: 0.6569\n",
      "Training | EPOCH: 5 | Average Loss: 0.1853 | Current Mini-batch loss: 0.0144\n",
      "Training | EPOCH: 5 | Average Loss: 0.0927 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0069\n",
      "Training | EPOCH: 5 | Average Loss: 0.0777 | Current Mini-batch loss: 0.1057\n",
      "Training | EPOCH: 5 | Average Loss: 0.0687 | Current Mini-batch loss: 0.0597\n",
      "Training | EPOCH: 5 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0303 | Current Mini-batch loss: 0.0256\n",
      "Training | EPOCH: 5 | Average Loss: 0.0348 | Current Mini-batch loss: 0.0393\n",
      "Training | EPOCH: 5 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0276 | Current Mini-batch loss: 0.0367\n",
      "Training | EPOCH: 5 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 5 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0199\n",
      "Training | EPOCH: 5 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0201\n",
      "Training | EPOCH: 5 | Average Loss: 0.0817 | Current Mini-batch loss: 0.1477\n",
      "Training | EPOCH: 5 | Average Loss: 0.0473 | Current Mini-batch loss: 0.0129\n",
      "Training | EPOCH: 5 | Average Loss: 0.0387 | Current Mini-batch loss: 0.0300\n",
      "Training | EPOCH: 5 | Average Loss: 0.0194 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0350 | Current Mini-batch loss: 0.0507\n",
      "Training | EPOCH: 5 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0095 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.1373 | Current Mini-batch loss: 0.2651\n",
      "Training | EPOCH: 5 | Average Loss: 0.0765 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 5 | Average Loss: 0.0383 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0193 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0111 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0054\n",
      "Training | EPOCH: 5 | Average Loss: 0.0585 | Current Mini-batch loss: 0.1088\n",
      "Training | EPOCH: 5 | Average Loss: 0.0293 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0096 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.0065 | Current Mini-batch loss: 0.0034\n",
      "Training | EPOCH: 5 | Average Loss: 0.0049 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0082\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0346 | Current Mini-batch loss: 0.0666\n",
      "Training | EPOCH: 5 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0124 | Current Mini-batch loss: 0.0075\n",
      "Training | EPOCH: 5 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0158\n",
      "Training | EPOCH: 5 | Average Loss: 0.5979 | Current Mini-batch loss: 1.1817\n",
      "Training | EPOCH: 5 | Average Loss: 0.3993 | Current Mini-batch loss: 0.2007\n",
      "Training | EPOCH: 5 | Average Loss: 0.2545 | Current Mini-batch loss: 0.1098\n",
      "Training | EPOCH: 5 | Average Loss: 0.1274 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.1092 | Current Mini-batch loss: 0.0910\n",
      "Training | EPOCH: 5 | Average Loss: 0.0547 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0364 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 5 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0178\n",
      "Training | EPOCH: 5 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.4984 | Current Mini-batch loss: 0.9936\n",
      "Training | EPOCH: 5 | Average Loss: 0.2565 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 5 | Average Loss: 0.1289 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0670 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 5 | Average Loss: 0.0335 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0482 | Current Mini-batch loss: 0.0629\n",
      "Training | EPOCH: 5 | Average Loss: 0.0783 | Current Mini-batch loss: 0.1083\n",
      "Training | EPOCH: 5 | Average Loss: 0.0574 | Current Mini-batch loss: 0.0365\n",
      "Training | EPOCH: 5 | Average Loss: 0.1028 | Current Mini-batch loss: 0.1482\n",
      "Training | EPOCH: 5 | Average Loss: 0.0574 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 5 | Average Loss: 0.0292 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0337\n",
      "Training | EPOCH: 5 | Average Loss: 0.1732 | Current Mini-batch loss: 0.3149\n",
      "Training | EPOCH: 5 | Average Loss: 0.1969 | Current Mini-batch loss: 0.2205\n",
      "Training | EPOCH: 5 | Average Loss: 0.1041 | Current Mini-batch loss: 0.0112\n",
      "Training | EPOCH: 5 | Average Loss: 0.0521 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0177 | Current Mini-batch loss: 0.0089\n",
      "Training | EPOCH: 5 | Average Loss: 0.1945 | Current Mini-batch loss: 0.3714\n",
      "Training | EPOCH: 5 | Average Loss: 0.1194 | Current Mini-batch loss: 0.0443\n",
      "Training | EPOCH: 5 | Average Loss: 0.0675 | Current Mini-batch loss: 0.0156\n",
      "Training | EPOCH: 5 | Average Loss: 0.0338 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0794 | Current Mini-batch loss: 0.1250\n",
      "Training | EPOCH: 5 | Average Loss: 0.0401 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0202 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0032 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.1036 | Current Mini-batch loss: 0.2040\n",
      "Training | EPOCH: 5 | Average Loss: 0.0526 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0154\n",
      "Training | EPOCH: 5 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0038\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0039\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0104\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0260 | Current Mini-batch loss: 0.0486\n",
      "Training | EPOCH: 5 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0055\n",
      "Training | EPOCH: 5 | Average Loss: 0.3952 | Current Mini-batch loss: 0.7747\n",
      "Training | EPOCH: 5 | Average Loss: 0.1988 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0995 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0528 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 5 | Average Loss: 0.0278 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0141 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.4795 | Current Mini-batch loss: 0.9520\n",
      "Training | EPOCH: 5 | Average Loss: 0.2478 | Current Mini-batch loss: 0.0160\n",
      "Training | EPOCH: 5 | Average Loss: 0.1279 | Current Mini-batch loss: 0.0080\n",
      "Training | EPOCH: 5 | Average Loss: 0.0646 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.2689 | Current Mini-batch loss: 0.4733\n",
      "Training | EPOCH: 5 | Average Loss: 0.1373 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 5 | Average Loss: 0.1007 | Current Mini-batch loss: 0.0642\n",
      "Training | EPOCH: 5 | Average Loss: 0.0511 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0259 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0077\n",
      "Training | EPOCH: 5 | Average Loss: 0.0161 | Current Mini-batch loss: 0.0154\n",
      "Training | EPOCH: 5 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0118\n",
      "Training | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0110\n",
      "Training | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0053 | Current Mini-batch loss: 0.0060\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0175 | Current Mini-batch loss: 0.0329\n",
      "Training | EPOCH: 5 | Average Loss: 0.4744 | Current Mini-batch loss: 0.9313\n",
      "Training | EPOCH: 5 | Average Loss: 0.2372 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.2159 | Current Mini-batch loss: 0.1947\n",
      "Training | EPOCH: 5 | Average Loss: 0.1083 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0754 | Current Mini-batch loss: 0.0425\n",
      "Training | EPOCH: 5 | Average Loss: 0.0388 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0196 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 5 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0041\n",
      "Training | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0068 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0020 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0051\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0057\n",
      "Training | EPOCH: 5 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0686\n",
      "Training | EPOCH: 5 | Average Loss: 0.0182 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0151 | Current Mini-batch loss: 0.0121\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0032\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0003 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0026 | Current Mini-batch loss: 0.0049\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0429 | Current Mini-batch loss: 0.0830\n",
      "Training | EPOCH: 5 | Average Loss: 0.1863 | Current Mini-batch loss: 0.3296\n",
      "Training | EPOCH: 5 | Average Loss: 0.0932 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0467 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0248 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0272 | Current Mini-batch loss: 0.0417\n",
      "Training | EPOCH: 5 | Average Loss: 0.0136 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0072 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.2452 | Current Mini-batch loss: 0.4868\n",
      "Training | EPOCH: 5 | Average Loss: 0.1506 | Current Mini-batch loss: 0.0559\n",
      "Training | EPOCH: 5 | Average Loss: 0.3120 | Current Mini-batch loss: 0.4734\n",
      "Training | EPOCH: 5 | Average Loss: 0.2075 | Current Mini-batch loss: 0.1030\n",
      "Training | EPOCH: 5 | Average Loss: 0.1052 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0527 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0498 | Current Mini-batch loss: 0.0469\n",
      "Training | EPOCH: 5 | Average Loss: 0.0409 | Current Mini-batch loss: 0.0320\n",
      "Training | EPOCH: 5 | Average Loss: 0.0206 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0133 | Current Mini-batch loss: 0.0059\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0822 | Current Mini-batch loss: 0.1632\n",
      "Training | EPOCH: 5 | Average Loss: 0.0413 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0623 | Current Mini-batch loss: 0.1039\n",
      "Training | EPOCH: 5 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 5 | Average Loss: 0.0253 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 5 | Average Loss: 0.0140 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0674 | Current Mini-batch loss: 0.1208\n",
      "Training | EPOCH: 5 | Average Loss: 0.0351 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0050\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0128 | Current Mini-batch loss: 0.0149\n",
      "Training | EPOCH: 5 | Average Loss: 0.0066 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0163\n",
      "Training | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0108 | Current Mini-batch loss: 0.0172\n",
      "Training | EPOCH: 5 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0110 | Current Mini-batch loss: 0.0162\n",
      "Training | EPOCH: 5 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0065\n",
      "Training | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0025\n",
      "Training | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0126 | Current Mini-batch loss: 0.0222\n",
      "Training | EPOCH: 5 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0357\n",
      "Training | EPOCH: 5 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0406\n",
      "Training | EPOCH: 5 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0228\n",
      "Training | EPOCH: 5 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0164\n",
      "Training | EPOCH: 5 | Average Loss: 0.0107 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0167\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0062 | Current Mini-batch loss: 0.0061\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0169\n",
      "Training | EPOCH: 5 | Average Loss: 0.0664 | Current Mini-batch loss: 0.1226\n",
      "Training | EPOCH: 5 | Average Loss: 0.0346 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0450 | Current Mini-batch loss: 0.0723\n",
      "Training | EPOCH: 5 | Average Loss: 0.0284 | Current Mini-batch loss: 0.0119\n",
      "Training | EPOCH: 5 | Average Loss: 0.0143 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0390 | Current Mini-batch loss: 0.0638\n",
      "Training | EPOCH: 5 | Average Loss: 0.0403 | Current Mini-batch loss: 0.0416\n",
      "Training | EPOCH: 5 | Average Loss: 0.0207 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0564 | Current Mini-batch loss: 0.1099\n",
      "Training | EPOCH: 5 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0148 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0664 | Current Mini-batch loss: 0.1180\n",
      "Training | EPOCH: 5 | Average Loss: 0.0339 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.1557 | Current Mini-batch loss: 0.2774\n",
      "Training | EPOCH: 5 | Average Loss: 0.0783 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0991 | Current Mini-batch loss: 0.1199\n",
      "Training | EPOCH: 5 | Average Loss: 0.0499 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0395 | Current Mini-batch loss: 0.0290\n",
      "Training | EPOCH: 5 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0331 | Current Mini-batch loss: 0.0461\n",
      "Training | EPOCH: 5 | Average Loss: 0.0740 | Current Mini-batch loss: 0.1150\n",
      "Training | EPOCH: 5 | Average Loss: 0.0370 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0026\n",
      "Training | EPOCH: 5 | Average Loss: 0.0321 | Current Mini-batch loss: 0.0536\n",
      "Training | EPOCH: 5 | Average Loss: 0.0233 | Current Mini-batch loss: 0.0146\n",
      "Training | EPOCH: 5 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.0067 | Current Mini-batch loss: 0.0091\n",
      "Training | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 5 | Average Loss: 0.0296 | Current Mini-batch loss: 0.0559\n",
      "Training | EPOCH: 5 | Average Loss: 0.0152 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0039 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0046\n",
      "Training | EPOCH: 5 | Average Loss: 0.7774 | Current Mini-batch loss: 1.5506\n",
      "Training | EPOCH: 5 | Average Loss: 0.3911 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 5 | Average Loss: 0.1964 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0983 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0742 | Current Mini-batch loss: 0.0501\n",
      "Training | EPOCH: 5 | Average Loss: 0.0920 | Current Mini-batch loss: 0.1098\n",
      "Training | EPOCH: 5 | Average Loss: 0.0487 | Current Mini-batch loss: 0.0053\n",
      "Training | EPOCH: 5 | Average Loss: 0.0246 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0213 | Current Mini-batch loss: 0.0181\n",
      "Training | EPOCH: 5 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0093\n",
      "Training | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0013\n",
      "Training | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0083\n",
      "Training | EPOCH: 5 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0056\n",
      "Training | EPOCH: 5 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0360\n",
      "Training | EPOCH: 5 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0064\n",
      "Training | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0286 | Current Mini-batch loss: 0.0550\n",
      "Training | EPOCH: 5 | Average Loss: 0.0157 | Current Mini-batch loss: 0.0029\n",
      "Training | EPOCH: 5 | Average Loss: 0.0082 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.1971 | Current Mini-batch loss: 0.3934\n",
      "Training | EPOCH: 5 | Average Loss: 0.1399 | Current Mini-batch loss: 0.0827\n",
      "Training | EPOCH: 5 | Average Loss: 0.0700 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0356 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.1035 | Current Mini-batch loss: 0.1715\n",
      "Training | EPOCH: 5 | Average Loss: 0.0533 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0678 | Current Mini-batch loss: 0.0824\n",
      "Training | EPOCH: 5 | Average Loss: 0.0460 | Current Mini-batch loss: 0.0242\n",
      "Training | EPOCH: 5 | Average Loss: 0.0618 | Current Mini-batch loss: 0.0776\n",
      "Training | EPOCH: 5 | Average Loss: 0.0795 | Current Mini-batch loss: 0.0972\n",
      "Training | EPOCH: 5 | Average Loss: 0.0408 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0418 | Current Mini-batch loss: 0.0427\n",
      "Training | EPOCH: 5 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0442 | Current Mini-batch loss: 0.0673\n",
      "Training | EPOCH: 5 | Average Loss: 0.0225 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 0.0131 | Current Mini-batch loss: 0.0037\n",
      "Training | EPOCH: 5 | Average Loss: 0.0525 | Current Mini-batch loss: 0.0919\n",
      "Training | EPOCH: 5 | Average Loss: 0.0573 | Current Mini-batch loss: 0.0621\n",
      "Training | EPOCH: 5 | Average Loss: 0.0294 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0210 | Current Mini-batch loss: 0.0126\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0106 | Current Mini-batch loss: 0.0097\n",
      "Training | EPOCH: 5 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 5 | Average Loss: 0.0427 | Current Mini-batch loss: 0.0803\n",
      "Training | EPOCH: 5 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0043\n",
      "Training | EPOCH: 5 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0252 | Current Mini-batch loss: 0.0383\n",
      "Training | EPOCH: 5 | Average Loss: 0.0134 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0070 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0168\n",
      "Training | EPOCH: 5 | Average Loss: 0.0130 | Current Mini-batch loss: 0.0158\n",
      "Training | EPOCH: 5 | Average Loss: 0.0080 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 5 | Average Loss: 0.0417 | Current Mini-batch loss: 0.0754\n",
      "Training | EPOCH: 5 | Average Loss: 0.0381 | Current Mini-batch loss: 0.0346\n",
      "Training | EPOCH: 5 | Average Loss: 0.0733 | Current Mini-batch loss: 0.1086\n",
      "Training | EPOCH: 5 | Average Loss: 0.0372 | Current Mini-batch loss: 0.0010\n",
      "Training | EPOCH: 5 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0031\n",
      "Training | EPOCH: 5 | Average Loss: 0.0116 | Current Mini-batch loss: 0.0030\n",
      "Training | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0166\n",
      "Training | EPOCH: 5 | Average Loss: 0.0059 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0008\n",
      "Training | EPOCH: 5 | Average Loss: 1.1818 | Current Mini-batch loss: 2.3602\n",
      "Training | EPOCH: 5 | Average Loss: 0.6330 | Current Mini-batch loss: 0.0841\n",
      "Training | EPOCH: 5 | Average Loss: 0.5996 | Current Mini-batch loss: 0.5662\n",
      "Training | EPOCH: 5 | Average Loss: 0.3171 | Current Mini-batch loss: 0.0347\n",
      "Training | EPOCH: 5 | Average Loss: 0.1586 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0888 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 5 | Average Loss: 0.0445 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0223 | Current Mini-batch loss: 0.0000\n",
      "Training | EPOCH: 5 | Average Loss: 0.0516 | Current Mini-batch loss: 0.0810\n",
      "Training | EPOCH: 5 | Average Loss: 0.0280 | Current Mini-batch loss: 0.0044\n",
      "Training | EPOCH: 5 | Average Loss: 0.0173 | Current Mini-batch loss: 0.0066\n",
      "Training | EPOCH: 5 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0249 | Current Mini-batch loss: 0.0406\n",
      "Training | EPOCH: 5 | Average Loss: 0.0125 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0075 | Current Mini-batch loss: 0.0087\n",
      "Training | EPOCH: 5 | Average Loss: 0.3127 | Current Mini-batch loss: 0.6180\n",
      "Training | EPOCH: 5 | Average Loss: 0.2925 | Current Mini-batch loss: 0.2724\n",
      "Training | EPOCH: 5 | Average Loss: 0.1514 | Current Mini-batch loss: 0.0102\n",
      "Training | EPOCH: 5 | Average Loss: 0.1258 | Current Mini-batch loss: 0.1002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0640 | Current Mini-batch loss: 0.0023\n",
      "Training | EPOCH: 5 | Average Loss: 0.0337 | Current Mini-batch loss: 0.0033\n",
      "Training | EPOCH: 5 | Average Loss: 0.0380 | Current Mini-batch loss: 0.0424\n",
      "Training | EPOCH: 5 | Average Loss: 0.0195 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0104 | Current Mini-batch loss: 0.0014\n",
      "Training | EPOCH: 5 | Average Loss: 0.0063 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.0054 | Current Mini-batch loss: 0.0045\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0324\n",
      "Training | EPOCH: 5 | Average Loss: 0.0088 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0036 | Current Mini-batch loss: 0.0048\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0003\n",
      "Training | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0217\n",
      "Training | EPOCH: 5 | Average Loss: 0.0301 | Current Mini-batch loss: 0.0491\n",
      "Training | EPOCH: 5 | Average Loss: 0.0502 | Current Mini-batch loss: 0.0702\n",
      "Training | EPOCH: 5 | Average Loss: 0.0293 | Current Mini-batch loss: 0.0084\n",
      "Training | EPOCH: 5 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0022\n",
      "Training | EPOCH: 5 | Average Loss: 0.0190 | Current Mini-batch loss: 0.0296\n",
      "Training | EPOCH: 5 | Average Loss: 0.0190 | Current Mini-batch loss: 0.0190\n",
      "Training | EPOCH: 5 | Average Loss: 0.7892 | Current Mini-batch loss: 1.5594\n",
      "Training | EPOCH: 5 | Average Loss: 0.5392 | Current Mini-batch loss: 0.2892\n",
      "Training | EPOCH: 5 | Average Loss: 0.5752 | Current Mini-batch loss: 0.6111\n",
      "Training | EPOCH: 5 | Average Loss: 0.2893 | Current Mini-batch loss: 0.0035\n",
      "Training | EPOCH: 5 | Average Loss: 0.1500 | Current Mini-batch loss: 0.0106\n",
      "Training | EPOCH: 5 | Average Loss: 0.0757 | Current Mini-batch loss: 0.0015\n",
      "Training | EPOCH: 5 | Average Loss: 0.0384 | Current Mini-batch loss: 0.0011\n",
      "Training | EPOCH: 5 | Average Loss: 0.0200 | Current Mini-batch loss: 0.0016\n",
      "Training | EPOCH: 5 | Average Loss: 0.0391 | Current Mini-batch loss: 0.0583\n",
      "Training | EPOCH: 5 | Average Loss: 0.0199 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0474\n",
      "Training | EPOCH: 5 | Average Loss: 0.0192 | Current Mini-batch loss: 0.0047\n",
      "Training | EPOCH: 5 | Average Loss: 0.0127 | Current Mini-batch loss: 0.0063\n",
      "Training | EPOCH: 5 | Average Loss: 0.0439 | Current Mini-batch loss: 0.0751\n",
      "Training | EPOCH: 5 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0163 | Current Mini-batch loss: 0.0105\n",
      "Training | EPOCH: 5 | Average Loss: 0.0085 | Current Mini-batch loss: 0.0007\n",
      "Training | EPOCH: 5 | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0024\n",
      "Training | EPOCH: 5 | Average Loss: 0.0018 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.7039 | Current Mini-batch loss: 1.4058\n",
      "Training | EPOCH: 5 | Average Loss: 0.6678 | Current Mini-batch loss: 0.6318\n",
      "Training | EPOCH: 5 | Average Loss: 0.3583 | Current Mini-batch loss: 0.0488\n",
      "Training | EPOCH: 5 | Average Loss: 0.1792 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0906 | Current Mini-batch loss: 0.0021\n",
      "Training | EPOCH: 5 | Average Loss: 0.2181 | Current Mini-batch loss: 0.3456\n",
      "Training | EPOCH: 5 | Average Loss: 0.3457 | Current Mini-batch loss: 0.4733\n",
      "Training | EPOCH: 5 | Average Loss: 0.1729 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0869 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.6339 | Current Mini-batch loss: 1.1809\n",
      "Training | EPOCH: 5 | Average Loss: 0.3179 | Current Mini-batch loss: 0.0018\n",
      "Training | EPOCH: 5 | Average Loss: 0.1603 | Current Mini-batch loss: 0.0027\n",
      "Training | EPOCH: 5 | Average Loss: 0.0806 | Current Mini-batch loss: 0.0009\n",
      "Training | EPOCH: 5 | Average Loss: 0.0412 | Current Mini-batch loss: 0.0017\n",
      "Training | EPOCH: 5 | Average Loss: 0.0245 | Current Mini-batch loss: 0.0078\n",
      "Training | EPOCH: 5 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0157\n",
      "Training | EPOCH: 5 | Average Loss: 0.0103 | Current Mini-batch loss: 0.0006\n",
      "Training | EPOCH: 5 | Average Loss: 0.0052 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0028 | Current Mini-batch loss: 0.0005\n",
      "Training | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0215 | Current Mini-batch loss: 0.0415\n",
      "Training | EPOCH: 5 | Average Loss: 0.0114 | Current Mini-batch loss: 0.0012\n",
      "Training | EPOCH: 5 | Average Loss: 0.0058 | Current Mini-batch loss: 0.0002\n",
      "Training | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0001\n",
      "Training | EPOCH: 5 | Average Loss: 0.0596 | Current Mini-batch loss: 0.1163\n",
      "Training | EPOCH: 5 | Average Loss: 0.0300 | Current Mini-batch loss: 0.0004\n",
      "Training | EPOCH: 5 | Average Loss: 0.0164 | Current Mini-batch loss: 0.0027\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0171 | Current Mini-batch loss: 0.0171\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0086 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0071 | Current Mini-batch loss: 0.0119\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0045 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0153 | Current Mini-batch loss: 0.0300\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0077 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0044 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0039\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1363 | Current Mini-batch loss: 0.2704\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0682 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0343 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0176 | Current Mini-batch loss: 0.0009\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0231\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0056 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0139 | Current Mini-batch loss: 0.0250\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0370 | Current Mini-batch loss: 0.0600\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0187 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0099 | Current Mini-batch loss: 0.0012\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0030 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0144 | Current Mini-batch loss: 0.0258\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0852 | Current Mini-batch loss: 0.1632\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0437 | Current Mini-batch loss: 0.0022\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0219 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0847 | Current Mini-batch loss: 0.1475\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0424 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0222 | Current Mini-batch loss: 0.0020\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0112 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0057 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0029 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0458 | Current Mini-batch loss: 0.0901\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1758 | Current Mini-batch loss: 0.3059\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0880 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0578 | Current Mini-batch loss: 0.0277\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0290 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0960 | Current Mini-batch loss: 0.1630\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1043 | Current Mini-batch loss: 0.1126\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0522 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0263 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0213\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0168 | Current Mini-batch loss: 0.0098\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0090 | Current Mini-batch loss: 0.0013\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0023 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0012 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0038\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0404 | Current Mini-batch loss: 0.0801\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0204 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.2450 | Current Mini-batch loss: 0.4849\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1227 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0614 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0633 | Current Mini-batch loss: 0.0653\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0355 | Current Mini-batch loss: 0.0076\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0178 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0015 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0685 | Current Mini-batch loss: 0.1355\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0342 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0174 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0048 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0027 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0022 | Current Mini-batch loss: 0.0018\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0048\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0050 | Current Mini-batch loss: 0.0069\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1405 | Current Mini-batch loss: 0.2761\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1070 | Current Mini-batch loss: 0.0735\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0535 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0604 | Current Mini-batch loss: 0.0672\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0302 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0156 | Current Mini-batch loss: 0.0010\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1608 | Current Mini-batch loss: 0.3059\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0805 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0403 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0203 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0102 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0429\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0181 | Current Mini-batch loss: 0.0096\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0091 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0073 | Current Mini-batch loss: 0.0056\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0202\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0017 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0009 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0201 | Current Mini-batch loss: 0.0395\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0101 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0051 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0069 | Current Mini-batch loss: 0.0088\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0035 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0008\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0731 | Current Mini-batch loss: 0.1455\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0369 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0185 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0093 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0047 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0025 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0119 | Current Mini-batch loss: 0.0230\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0033 | Current Mini-batch loss: 0.0006\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0627 | Current Mini-batch loss: 0.1220\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0315 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0158 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0092 | Current Mini-batch loss: 0.0027\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0046 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0024 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0074 | Current Mini-batch loss: 0.0123\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0037 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0147 | Current Mini-batch loss: 0.0257\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0040 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0287 | Current Mini-batch loss: 0.0533\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0208 | Current Mini-batch loss: 0.0129\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0060 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0186 | Current Mini-batch loss: 0.0312\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0094 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0155 | Current Mini-batch loss: 0.0217\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0081 | Current Mini-batch loss: 0.0007\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0041 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0021 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0017\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0013 | Current Mini-batch loss: 0.0025\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0014 | Current Mini-batch loss: 0.0014\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0007 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0076 | Current Mini-batch loss: 0.0145\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0038 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0723 | Current Mini-batch loss: 0.1408\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0362 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0209 | Current Mini-batch loss: 0.0056\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0105 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0235 | Current Mini-batch loss: 0.0365\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0120 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0061 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0031 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0016 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0004 | Current Mini-batch loss: 0.0002\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0034 | Current Mini-batch loss: 0.0066\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0019 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0011 | Current Mini-batch loss: 0.0004\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0008 | Current Mini-batch loss: 0.0005\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0010 | Current Mini-batch loss: 0.0015\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0005 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.1251 | Current Mini-batch loss: 0.2497\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0671 | Current Mini-batch loss: 0.0091\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0336 | Current Mini-batch loss: 0.0000\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0530 | Current Mini-batch loss: 0.0724\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0265 | Current Mini-batch loss: 0.0001\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0238 | Current Mini-batch loss: 0.0210\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0586 | Current Mini-batch loss: 0.0935\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0295 | Current Mini-batch loss: 0.0003\n",
      "Validation | EPOCH: 5 | Average Loss: 0.0461 | Current Mini-batch loss: 0.0627\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "epoch_train_losses = []\n",
    "train_correct_epoch = []\n",
    "train_correct_epoch_perc = []\n",
    "\n",
    "\n",
    "validation_losses = []\n",
    "epoch_val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_epoch_loss = None\n",
    "    running_correct = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x,y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # 1. Get the prediction\n",
    "        pred = model(x)\n",
    "        \n",
    "        # 2. Compute the loss\n",
    "        loss = loss_fn(pred,y)\n",
    "        \n",
    "        # 3. Zerout the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Compute new gradients and acumulate them\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Perform a optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 3. Get num correct\n",
    "        _correct = (torch.argmax(pred, dim=1) == y).sum()\n",
    "        running_correct += _correct.detach()\n",
    "        \n",
    "        # Log metrics\n",
    "        loss_detached = loss.detach()\n",
    "        \n",
    "        train_losses.append(loss_detached.cpu())\n",
    "        \n",
    "        if running_epoch_loss is not None:\n",
    "            running_epoch_loss = (running_epoch_loss + loss_detached) / 2.0\n",
    "        else:\n",
    "            running_epoch_loss = loss_detached\n",
    "            \n",
    "        print(f\"Training | EPOCH: {epoch+1} | Average Loss: {running_epoch_loss:.4f} | Current Mini-batch loss: {loss_detached:.4f}\")\n",
    "    \n",
    "    epoch_train_losses.append(running_epoch_loss)\n",
    "    train_correct_epoch.append(running_correct.cpu())\n",
    "    train_correct_epoch_perc.append(running_correct.cpu()/len(train))\n",
    "    \n",
    "    running_epoch_loss = None\n",
    "    model.eval()\n",
    "    for batch in validation_loader:\n",
    "        x,y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # 1. Get the prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            \n",
    "            # 2. Compute the loss\n",
    "            loss = loss_fn(pred,y)\n",
    "        \n",
    "        # Log metrics\n",
    "        loss_detached = loss.detach()\n",
    "        \n",
    "        validation_losses.append(loss_detached.cpu())\n",
    "        \n",
    "        if running_epoch_loss is not None:\n",
    "            running_epoch_loss = (running_epoch_loss + loss_detached) / 2.0\n",
    "        else:\n",
    "            running_epoch_loss = loss_detached\n",
    "            \n",
    "        print(f\"Validation | EPOCH: {epoch+1} | Average Loss: {running_epoch_loss:.4f} | Current Mini-batch loss: {loss_detached:.4f}\")\n",
    "        \n",
    "    epoch_val_losses.append(running_epoch_loss)   \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 366.862918 248.518125\" width=\"366.862918pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-11-28T19:22:44.940816</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 366.862918 248.518125 \r\nL 366.862918 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\nL 355.3625 7.2 \r\nL 20.5625 7.2 \r\nz\r\n\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m11928be818\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"87.640221\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(74.915221 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"139.49976\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(126.77476 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"191.3593\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(178.6343 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"243.218839\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(230.493839 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"295.078378\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(282.353378 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"346.937918\" xlink:href=\"#m11928be818\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 6000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(334.212918 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m623f1d7120\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"214.756473\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 218.555691)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"186.427428\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 190.226647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"158.098384\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 2 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 161.897602)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"129.769339\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 3 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 133.568558)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"101.440295\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 4 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 105.239514)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"73.11125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 76.910469)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"44.782206\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 6 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 48.581425)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m623f1d7120\" y=\"16.453162\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 7 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 20.25238)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-37\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p00332668e4)\" d=\"M 35.780682 190.213997 \r\nL 35.832541 214.752537 \r\nL 35.884401 170.183753 \r\nL 35.93626 197.943036 \r\nL 35.98812 208.68688 \r\nL 36.03998 203.027741 \r\nL 36.091839 197.905589 \r\nL 36.143699 197.913699 \r\nL 36.195558 211.109213 \r\nL 36.247418 209.928687 \r\nL 36.299277 203.785593 \r\nL 36.351137 173.489017 \r\nL 36.402996 214.756017 \r\nL 36.454856 146.20483 \r\nL 36.506715 214.309397 \r\nL 36.714154 185.93355 \r\nL 36.921592 212.12211 \r\nL 36.973451 209.568643 \r\nL 37.025311 198.764885 \r\nL 37.07717 203.663715 \r\nL 37.12903 213.269175 \r\nL 37.180889 212.333193 \r\nL 37.336468 113.253015 \r\nL 37.440187 214.501133 \r\nL 37.492047 214.007734 \r\nL 37.543906 213.542762 \r\nL 37.595766 196.675936 \r\nL 37.647625 197.619043 \r\nL 37.699485 214.688541 \r\nL 37.751344 207.225035 \r\nL 37.803204 206.926995 \r\nL 38.010642 214.060861 \r\nL 38.21808 195.326391 \r\nL 38.26994 198.138138 \r\nL 38.373659 212.443978 \r\nL 38.425518 210.686051 \r\nL 38.477378 213.300223 \r\nL 38.632956 202.887604 \r\nL 38.684816 208.406145 \r\nL 38.736676 200.474575 \r\nL 38.788535 200.866906 \r\nL 38.840395 214.394992 \r\nL 38.892254 206.078594 \r\nL 38.944114 195.604006 \r\nL 39.099692 213.030019 \r\nL 39.151552 210.353534 \r\nL 39.203411 214.706811 \r\nL 39.30713 17.083636 \r\nL 39.35899 151.255801 \r\nL 39.462709 214.380572 \r\nL 39.514569 211.008161 \r\nL 39.670147 214.087467 \r\nL 39.773866 214.507802 \r\nL 39.825726 214.434941 \r\nL 39.929445 214.623873 \r\nL 39.981305 214.509193 \r\nL 40.033164 213.995405 \r\nL 40.085024 214.265907 \r\nL 40.136883 214.5338 \r\nL 40.292462 185.787656 \r\nL 40.44804 214.683366 \r\nL 40.4999 214.245389 \r\nL 40.603619 210.930545 \r\nL 40.759198 214.559498 \r\nL 40.811057 214.631027 \r\nL 40.862917 213.68064 \r\nL 40.914776 209.72176 \r\nL 41.018495 214.644082 \r\nL 41.070355 191.769241 \r\nL 41.122214 206.890179 \r\nL 41.174074 214.356161 \r\nL 41.225933 206.704667 \r\nL 41.381512 214.441514 \r\nL 41.485231 192.310331 \r\nL 41.537091 214.166903 \r\nL 41.58895 191.773623 \r\nL 41.692669 213.269833 \r\nL 41.796388 199.847474 \r\nL 41.900107 214.654866 \r\nL 41.951967 213.184085 \r\nL 42.003827 202.761231 \r\nL 42.055686 214.668173 \r\nL 42.159405 212.775249 \r\nL 42.211265 214.674663 \r\nL 42.263124 213.51236 \r\nL 42.314984 200.095335 \r\nL 42.366843 214.525867 \r\nL 42.418703 209.244908 \r\nL 42.470562 214.585067 \r\nL 42.522422 213.189033 \r\nL 42.574281 214.172077 \r\nL 42.626141 214.702245 \r\nL 42.833579 193.961248 \r\nL 42.937298 214.381282 \r\nL 42.989158 212.53344 \r\nL 43.092877 214.665258 \r\nL 43.144736 214.322835 \r\nL 43.196596 213.886436 \r\nL 43.248455 214.641379 \r\nL 43.300315 202.298065 \r\nL 43.352175 210.97073 \r\nL 43.455894 214.557845 \r\nL 43.507753 214.489702 \r\nL 43.559613 193.763097 \r\nL 43.611472 214.697533 \r\nL 43.663332 214.677386 \r\nL 43.715191 211.132745 \r\nL 43.767051 214.686876 \r\nL 43.922629 207.543989 \r\nL 43.974489 118.308187 \r\nL 44.026349 212.596307 \r\nL 44.078208 171.102465 \r\nL 44.130068 214.634905 \r\nL 44.181927 214.74458 \r\nL 44.233787 213.33471 \r\nL 44.285646 214.51903 \r\nL 44.337506 202.80768 \r\nL 44.389365 211.670703 \r\nL 44.441225 211.744516 \r\nL 44.544944 214.222557 \r\nL 44.648663 196.822473 \r\nL 44.700523 207.967305 \r\nL 44.752382 214.602617 \r\nL 44.856101 214.302208 \r\nL 44.907961 210.875479 \r\nL 44.95982 212.428375 \r\nL 45.01168 214.716506 \r\nL 45.167258 185.051097 \r\nL 45.270978 214.736655 \r\nL 45.322837 214.182288 \r\nL 45.426556 214.580079 \r\nL 45.478416 214.230154 \r\nL 45.530275 211.75783 \r\nL 45.582135 213.082049 \r\nL 45.633994 214.337627 \r\nL 45.685854 206.497496 \r\nL 45.737713 180.921884 \r\nL 45.789573 196.83449 \r\nL 45.841432 211.944107 \r\nL 45.945152 210.828171 \r\nL 45.997011 214.724166 \r\nL 46.10073 214.629426 \r\nL 46.204449 211.469438 \r\nL 46.256309 214.668329 \r\nL 46.360028 214.431482 \r\nL 46.463747 213.324958 \r\nL 46.515606 188.116232 \r\nL 46.567466 212.851929 \r\nL 46.619326 214.088093 \r\nL 46.671185 210.022826 \r\nL 46.723045 214.379981 \r\nL 46.826764 214.732557 \r\nL 46.878623 211.382531 \r\nL 46.930483 214.694809 \r\nL 46.982342 205.164585 \r\nL 47.034202 213.026215 \r\nL 47.086061 212.885388 \r\nL 47.137921 214.686656 \r\nL 47.24164 214.544718 \r\nL 47.2935 213.011028 \r\nL 47.345359 208.081482 \r\nL 47.397219 213.257485 \r\nL 47.449078 211.091861 \r\nL 47.500938 214.521656 \r\nL 47.604657 214.412023 \r\nL 47.656516 207.229599 \r\nL 47.708376 214.068322 \r\nL 47.760235 213.37197 \r\nL 47.812095 207.278187 \r\nL 47.863954 213.475235 \r\nL 47.967674 214.425954 \r\nL 48.019533 213.598793 \r\nL 48.071393 214.108314 \r\nL 48.123252 214.61345 \r\nL 48.278831 211.852101 \r\nL 48.434409 214.74619 \r\nL 48.538128 213.108326 \r\nL 48.693707 214.601788 \r\nL 48.745567 206.141768 \r\nL 48.797426 211.385377 \r\nL 48.953005 214.736355 \r\nL 49.108583 214.381222 \r\nL 49.160443 214.45832 \r\nL 49.212303 213.584182 \r\nL 49.264162 214.568666 \r\nL 49.316022 213.98279 \r\nL 49.367881 214.734017 \r\nL 49.419741 204.215095 \r\nL 49.52346 205.450584 \r\nL 49.575319 201.817922 \r\nL 49.730898 214.596753 \r\nL 49.886477 210.24742 \r\nL 50.042055 214.700831 \r\nL 50.197634 213.546715 \r\nL 50.249493 208.896335 \r\nL 50.301353 209.971458 \r\nL 50.405072 214.12571 \r\nL 50.456931 214.069311 \r\nL 50.508791 213.649878 \r\nL 50.560651 209.554368 \r\nL 50.61251 214.499962 \r\nL 50.66437 198.702934 \r\nL 50.716229 212.494399 \r\nL 50.819948 214.686302 \r\nL 50.871808 214.447616 \r\nL 50.923667 214.717298 \r\nL 50.975527 214.619435 \r\nL 51.027386 202.268125 \r\nL 51.079246 213.717544 \r\nL 51.131105 212.641801 \r\nL 51.182965 208.072807 \r\nL 51.234825 214.734553 \r\nL 51.286684 214.703914 \r\nL 51.338544 213.48676 \r\nL 51.390403 202.724932 \r\nL 51.442263 213.963739 \r\nL 51.494122 214.326406 \r\nL 51.545982 183.060619 \r\nL 51.597841 214.075873 \r\nL 51.649701 213.263391 \r\nL 51.70156 175.460074 \r\nL 51.75342 197.96113 \r\nL 51.857139 206.38243 \r\nL 51.908999 197.5439 \r\nL 51.960858 214.753881 \r\nL 52.064577 212.627443 \r\nL 52.116437 213.753586 \r\nL 52.168296 206.871259 \r\nL 52.220156 210.037608 \r\nL 52.272015 214.701195 \r\nL 52.323875 211.974221 \r\nL 52.375734 211.757134 \r\nL 52.479453 214.630116 \r\nL 52.531313 210.642574 \r\nL 52.583173 213.970705 \r\nL 52.738751 214.673991 \r\nL 52.89433 214.736692 \r\nL 52.946189 209.438507 \r\nL 52.998049 213.761195 \r\nL 53.101768 214.732857 \r\nL 53.153627 214.667669 \r\nL 53.257347 214.694946 \r\nL 53.309206 212.659466 \r\nL 53.412925 214.753239 \r\nL 53.464785 214.738409 \r\nL 53.568504 214.673238 \r\nL 53.620363 214.525573 \r\nL 53.672223 213.795854 \r\nL 53.724082 214.718536 \r\nL 53.775942 214.443951 \r\nL 53.827802 213.997943 \r\nL 53.879661 212.524656 \r\nL 53.931521 214.292853 \r\nL 53.98338 201.741004 \r\nL 54.03524 210.953604 \r\nL 54.087099 209.82577 \r\nL 54.138959 214.741614 \r\nL 54.190818 214.517344 \r\nL 54.242678 211.626399 \r\nL 54.294537 214.010509 \r\nL 54.346397 213.35751 \r\nL 54.398256 195.248507 \r\nL 54.450116 214.724859 \r\nL 54.501976 214.735616 \r\nL 54.553835 213.798026 \r\nL 54.605695 214.698776 \r\nL 54.657554 214.688206 \r\nL 54.709414 200.575165 \r\nL 54.761273 206.671927 \r\nL 54.813133 214.237842 \r\nL 54.864992 204.369951 \r\nL 54.916852 213.049177 \r\nL 54.968711 214.642977 \r\nL 55.020571 213.893229 \r\nL 55.07243 214.372775 \r\nL 55.12429 190.367501 \r\nL 55.17615 205.485005 \r\nL 55.228009 214.672991 \r\nL 55.331728 214.624402 \r\nL 55.487307 214.689265 \r\nL 55.539166 214.426954 \r\nL 55.591026 211.967455 \r\nL 55.642885 212.956945 \r\nL 55.694745 214.400796 \r\nL 55.746604 201.2884 \r\nL 55.798464 214.748339 \r\nL 55.850324 214.222703 \r\nL 55.902183 202.787247 \r\nL 55.954043 214.732688 \r\nL 56.005902 214.675477 \r\nL 56.057762 214.303025 \r\nL 56.109621 199.890839 \r\nL 56.161481 127.567986 \r\nL 56.21334 200.04888 \r\nL 56.2652 167.950949 \r\nL 56.317059 214.750965 \r\nL 56.420778 214.72396 \r\nL 56.472638 214.627393 \r\nL 56.524498 213.853682 \r\nL 56.576357 214.529964 \r\nL 56.731936 214.750887 \r\nL 56.991233 214.552528 \r\nL 57.094952 214.728361 \r\nL 57.146812 214.703023 \r\nL 57.198672 214.288505 \r\nL 57.250531 170.086672 \r\nL 57.302391 208.100665 \r\nL 57.35425 196.858144 \r\nL 57.40611 214.750677 \r\nL 57.509829 213.211732 \r\nL 57.665407 214.726337 \r\nL 57.717267 214.255022 \r\nL 57.769126 214.481444 \r\nL 57.820986 191.747177 \r\nL 57.872846 214.607689 \r\nL 57.924705 214.646073 \r\nL 58.080284 210.681307 \r\nL 58.132143 214.679508 \r\nL 58.287722 202.791283 \r\nL 58.391441 214.734927 \r\nL 58.49516 214.555627 \r\nL 58.54702 200.911876 \r\nL 58.598879 214.743062 \r\nL 58.650739 207.460021 \r\nL 58.702598 214.739494 \r\nL 58.754458 209.513682 \r\nL 58.806317 214.574917 \r\nL 58.858177 211.421298 \r\nL 58.910036 214.719648 \r\nL 59.013755 214.741555 \r\nL 59.169334 213.447714 \r\nL 59.273053 205.542049 \r\nL 59.324913 214.704488 \r\nL 59.376772 214.311896 \r\nL 59.428632 209.888413 \r\nL 59.480491 213.16652 \r\nL 59.532351 214.691329 \r\nL 59.58421 201.346959 \r\nL 59.63607 210.095332 \r\nL 59.687929 214.624433 \r\nL 59.791649 214.513325 \r\nL 59.895368 214.751462 \r\nL 59.947227 214.628234 \r\nL 60.102806 196.377301 \r\nL 60.154665 214.751374 \r\nL 60.258384 214.310182 \r\nL 60.310244 210.293263 \r\nL 60.362103 214.625726 \r\nL 60.517682 212.550391 \r\nL 60.569542 214.565293 \r\nL 60.72512 205.254862 \r\nL 60.77698 214.750058 \r\nL 60.828839 214.387517 \r\nL 60.932558 205.485756 \r\nL 60.984418 214.495118 \r\nL 61.036277 201.119268 \r\nL 61.088137 214.344113 \r\nL 61.139997 214.627017 \r\nL 61.243716 213.302643 \r\nL 61.295575 193.710315 \r\nL 61.347435 213.871746 \r\nL 61.399294 213.17289 \r\nL 61.451154 169.80607 \r\nL 61.503013 189.812946 \r\nL 61.554873 214.026259 \r\nL 61.658592 213.230801 \r\nL 61.710451 203.487131 \r\nL 61.762311 214.752113 \r\nL 61.814171 192.743297 \r\nL 61.86603 211.199509 \r\nL 62.021609 214.735428 \r\nL 62.073468 214.565423 \r\nL 62.125328 169.694751 \r\nL 62.177187 206.476273 \r\nL 62.280906 214.679075 \r\nL 62.332766 213.717822 \r\nL 62.384625 210.824465 \r\nL 62.436485 214.45605 \r\nL 62.488345 212.524849 \r\nL 62.540204 210.835268 \r\nL 62.695783 214.705354 \r\nL 62.747642 197.089406 \r\nL 62.799502 214.67927 \r\nL 62.851361 207.79916 \r\nL 62.903221 214.548824 \r\nL 63.00694 207.241106 \r\nL 63.0588 210.05465 \r\nL 63.214378 214.745629 \r\nL 63.318097 214.578064 \r\nL 63.369957 214.634588 \r\nL 63.421816 214.622387 \r\nL 63.525535 209.155087 \r\nL 63.629254 214.635902 \r\nL 63.681114 210.488239 \r\nL 63.732974 214.659978 \r\nL 63.784833 212.191659 \r\nL 63.836693 214.110436 \r\nL 63.888552 212.768868 \r\nL 63.940412 213.029241 \r\nL 64.044131 214.749193 \r\nL 64.09599 214.739847 \r\nL 64.407148 214.715633 \r\nL 64.459007 213.585339 \r\nL 64.510867 213.748352 \r\nL 64.666445 214.739848 \r\nL 64.718305 214.110982 \r\nL 64.770164 214.719322 \r\nL 64.822024 214.669757 \r\nL 64.873883 211.045177 \r\nL 64.925743 214.728141 \r\nL 64.977602 214.714074 \r\nL 65.081322 210.531871 \r\nL 65.133181 214.743308 \r\nL 65.2369 214.679272 \r\nL 65.28876 214.173979 \r\nL 65.340619 214.578734 \r\nL 65.392479 214.429476 \r\nL 65.444338 214.511699 \r\nL 65.496198 205.81106 \r\nL 65.548057 214.628776 \r\nL 65.599917 214.342014 \r\nL 65.651776 214.725499 \r\nL 65.703636 214.646159 \r\nL 65.755496 214.642581 \r\nL 65.807355 204.933762 \r\nL 65.859215 214.745551 \r\nL 66.014793 206.81054 \r\nL 66.066653 214.694663 \r\nL 66.118512 214.541772 \r\nL 66.222231 211.890692 \r\nL 66.274091 214.680097 \r\nL 66.32595 214.490245 \r\nL 66.37781 213.063703 \r\nL 66.42967 214.594266 \r\nL 66.533389 214.713278 \r\nL 66.585248 214.363086 \r\nL 66.637108 193.883019 \r\nL 66.688967 214.523758 \r\nL 66.740827 214.274049 \r\nL 66.792686 214.500025 \r\nL 66.844546 214.707334 \r\nL 66.896405 199.143621 \r\nL 66.948265 214.729708 \r\nL 67.000124 205.592746 \r\nL 67.051984 214.608424 \r\nL 67.103844 212.10429 \r\nL 67.155703 214.635181 \r\nL 67.207563 214.017159 \r\nL 67.259422 214.547129 \r\nL 67.415001 214.733243 \r\nL 67.46686 214.306568 \r\nL 67.51872 200.447698 \r\nL 67.570579 213.581678 \r\nL 67.622439 213.15565 \r\nL 67.674299 214.108807 \r\nL 67.726158 205.931234 \r\nL 67.778018 214.04272 \r\nL 67.829877 212.827186 \r\nL 67.881737 214.659871 \r\nL 67.933596 214.495039 \r\nL 67.985456 212.183615 \r\nL 68.037315 214.74981 \r\nL 68.089175 214.747406 \r\nL 68.141034 193.357433 \r\nL 68.192894 214.295218 \r\nL 68.244753 214.365982 \r\nL 68.296613 213.479487 \r\nL 68.348473 214.72366 \r\nL 68.400332 214.606715 \r\nL 68.504051 208.473796 \r\nL 68.555911 212.754774 \r\nL 68.60777 210.841249 \r\nL 68.65963 207.569355 \r\nL 68.711489 209.532243 \r\nL 68.763349 214.720006 \r\nL 68.867068 214.314635 \r\nL 68.970787 214.697488 \r\nL 69.022647 191.579574 \r\nL 69.074506 214.732115 \r\nL 69.126366 214.599127 \r\nL 69.333804 200.910142 \r\nL 69.385663 173.562348 \r\nL 69.437523 214.691395 \r\nL 69.541242 213.726662 \r\nL 69.593101 214.723591 \r\nL 69.644961 214.586116 \r\nL 69.696821 202.82972 \r\nL 69.74868 214.640306 \r\nL 69.956118 214.750497 \r\nL 70.007978 214.730837 \r\nL 70.059837 214.387141 \r\nL 70.111697 212.139953 \r\nL 70.163556 213.59849 \r\nL 70.319135 214.741749 \r\nL 70.422854 208.496402 \r\nL 70.474714 213.504697 \r\nL 70.526573 206.765892 \r\nL 70.630292 214.707823 \r\nL 70.682152 213.474326 \r\nL 70.734011 214.332826 \r\nL 70.83773 214.671807 \r\nL 70.941449 213.947858 \r\nL 70.993309 212.487349 \r\nL 71.045169 214.739119 \r\nL 71.097028 211.202002 \r\nL 71.148888 214.699113 \r\nL 71.252607 214.691738 \r\nL 71.408185 214.531015 \r\nL 71.460045 214.639197 \r\nL 71.511904 214.560866 \r\nL 71.563764 212.52227 \r\nL 71.615623 214.264932 \r\nL 71.719343 214.737973 \r\nL 71.823062 214.415171 \r\nL 71.926781 214.734411 \r\nL 71.97864 214.713179 \r\nL 72.0305 209.046038 \r\nL 72.082359 213.43103 \r\nL 72.237938 214.71605 \r\nL 72.289798 206.73149 \r\nL 72.341657 208.779173 \r\nL 72.393517 214.679935 \r\nL 72.497236 214.542449 \r\nL 72.600955 214.661099 \r\nL 72.652814 212.631839 \r\nL 72.704674 214.640925 \r\nL 72.756533 214.482911 \r\nL 72.808393 214.673328 \r\nL 72.860252 214.648869 \r\nL 72.912112 199.795068 \r\nL 72.963972 207.746056 \r\nL 73.067691 214.722826 \r\nL 73.11955 210.864746 \r\nL 73.17141 214.691217 \r\nL 73.223269 214.75142 \r\nL 73.275129 172.723189 \r\nL 73.326988 214.751482 \r\nL 73.378848 214.692668 \r\nL 73.430707 203.602535 \r\nL 73.482567 214.670499 \r\nL 73.534426 210.626692 \r\nL 73.586286 214.661152 \r\nL 73.638146 213.943239 \r\nL 73.690005 214.621524 \r\nL 73.741865 214.616744 \r\nL 73.845584 139.160574 \r\nL 74.001162 214.684113 \r\nL 74.053022 214.32919 \r\nL 74.104881 214.746219 \r\nL 74.156741 205.853926 \r\nL 74.2086 214.594862 \r\nL 74.26046 214.70009 \r\nL 74.31232 212.931481 \r\nL 74.364179 214.601192 \r\nL 74.416039 214.599329 \r\nL 74.467898 213.252485 \r\nL 74.623477 187.549975 \r\nL 74.727196 214.711752 \r\nL 74.779055 209.735968 \r\nL 74.830915 214.249312 \r\nL 74.882774 214.740675 \r\nL 74.934634 214.669724 \r\nL 75.038353 214.152676 \r\nL 75.090213 213.42777 \r\nL 75.142072 214.736128 \r\nL 75.193932 214.591171 \r\nL 75.245791 211.404263 \r\nL 75.297651 214.706791 \r\nL 75.34951 214.39877 \r\nL 75.40137 202.954146 \r\nL 75.453229 214.618524 \r\nL 75.556948 214.622309 \r\nL 75.608808 210.702588 \r\nL 75.660668 214.704138 \r\nL 75.764387 214.36371 \r\nL 75.816246 214.748569 \r\nL 75.868106 214.660988 \r\nL 76.075544 214.720611 \r\nL 76.127403 213.782852 \r\nL 76.179263 214.382203 \r\nL 76.231123 196.003959 \r\nL 76.282982 209.528043 \r\nL 76.334842 214.75406 \r\nL 76.438561 214.608949 \r\nL 76.49042 208.244271 \r\nL 76.54228 214.725231 \r\nL 76.594139 209.593247 \r\nL 76.645999 214.65352 \r\nL 76.697858 214.717514 \r\nL 76.749718 213.892902 \r\nL 76.801577 214.312331 \r\nL 76.853437 202.293255 \r\nL 76.905297 214.608707 \r\nL 76.957156 213.623312 \r\nL 77.009016 214.705747 \r\nL 77.060875 207.298655 \r\nL 77.112735 214.243243 \r\nL 77.164594 214.695643 \r\nL 77.216454 214.660786 \r\nL 77.268313 214.166817 \r\nL 77.320173 214.285351 \r\nL 77.372032 214.673696 \r\nL 77.527611 213.491354 \r\nL 77.579471 214.423866 \r\nL 77.63133 212.560285 \r\nL 77.68319 214.452976 \r\nL 77.735049 213.649478 \r\nL 77.838768 214.704179 \r\nL 77.890628 214.69757 \r\nL 78.046206 214.611317 \r\nL 78.098066 214.46085 \r\nL 78.149925 214.73428 \r\nL 78.201785 214.00894 \r\nL 78.253645 214.748717 \r\nL 78.305504 214.447838 \r\nL 78.357364 214.58465 \r\nL 78.461083 214.721166 \r\nL 78.512942 211.123889 \r\nL 78.564802 214.727249 \r\nL 78.616661 213.629641 \r\nL 78.668521 213.760878 \r\nL 78.824099 214.658934 \r\nL 78.875959 213.940914 \r\nL 78.927819 214.062227 \r\nL 79.083397 201.73775 \r\nL 79.135257 213.73076 \r\nL 79.187116 213.200479 \r\nL 79.238976 205.870227 \r\nL 79.290835 214.566717 \r\nL 79.342695 212.059508 \r\nL 79.394554 211.909967 \r\nL 79.498273 214.717172 \r\nL 79.550133 212.190796 \r\nL 79.601993 214.662028 \r\nL 79.653852 213.323928 \r\nL 79.705712 214.70239 \r\nL 79.757571 214.146568 \r\nL 79.809431 214.340934 \r\nL 79.86129 214.720942 \r\nL 79.965009 214.695113 \r\nL 80.016869 180.586016 \r\nL 80.068728 213.421401 \r\nL 80.120588 213.326711 \r\nL 80.172447 214.737771 \r\nL 80.224307 214.705747 \r\nL 80.379886 212.420758 \r\nL 80.535464 214.745813 \r\nL 80.691043 214.066328 \r\nL 80.742902 195.298477 \r\nL 80.794762 214.375818 \r\nL 80.846622 201.913969 \r\nL 80.898481 214.691123 \r\nL 81.0022 214.727617 \r\nL 81.05406 213.726806 \r\nL 81.105919 214.622548 \r\nL 81.157779 214.01374 \r\nL 81.209638 214.524787 \r\nL 81.261498 186.415845 \r\nL 81.313357 209.576699 \r\nL 81.365217 207.195949 \r\nL 81.468936 214.666982 \r\nL 81.520796 214.611786 \r\nL 81.572655 214.035341 \r\nL 81.676374 214.304074 \r\nL 81.728234 209.801042 \r\nL 81.883812 214.383603 \r\nL 81.935672 203.405446 \r\nL 81.987531 208.690766 \r\nL 82.039391 214.629802 \r\nL 82.14311 214.60848 \r\nL 82.19497 214.630067 \r\nL 82.246829 214.172819 \r\nL 82.298689 214.682435 \r\nL 82.350548 214.745247 \r\nL 82.402408 195.920406 \r\nL 82.454267 211.528107 \r\nL 82.506127 200.192933 \r\nL 82.557986 209.950764 \r\nL 82.765424 214.671709 \r\nL 82.817284 214.726117 \r\nL 82.869144 214.657976 \r\nL 82.972863 214.74064 \r\nL 83.024722 213.868619 \r\nL 83.076582 214.656928 \r\nL 83.128441 214.332953 \r\nL 83.180301 214.658898 \r\nL 83.23216 210.823026 \r\nL 83.28402 214.648869 \r\nL 83.335879 214.712857 \r\nL 83.387739 214.174314 \r\nL 83.439598 214.750907 \r\nL 83.491458 214.322158 \r\nL 83.543318 214.726493 \r\nL 83.698896 212.367473 \r\nL 83.854475 214.748617 \r\nL 83.906334 193.234102 \r\nL 83.958194 214.72803 \r\nL 84.010053 214.7202 \r\nL 84.113772 206.733285 \r\nL 84.269351 214.719654 \r\nL 84.42493 213.926233 \r\nL 84.476789 214.698615 \r\nL 84.528649 209.850565 \r\nL 84.580508 213.171073 \r\nL 84.632368 212.247701 \r\nL 84.684227 204.389939 \r\nL 84.736087 214.69147 \r\nL 84.787946 206.24612 \r\nL 84.839806 206.674727 \r\nL 84.891666 202.162922 \r\nL 84.995385 214.735161 \r\nL 85.047244 214.454367 \r\nL 85.099104 214.751753 \r\nL 85.150963 214.300003 \r\nL 85.202823 154.027855 \r\nL 85.254682 214.587185 \r\nL 85.306542 214.475524 \r\nL 85.462121 214.749397 \r\nL 85.51398 213.607672 \r\nL 85.56584 214.234045 \r\nL 85.617699 214.673915 \r\nL 85.669559 213.978798 \r\nL 85.721418 210.198227 \r\nL 85.773278 213.982869 \r\nL 85.876997 214.717527 \r\nL 85.928856 214.181541 \r\nL 85.980716 214.354679 \r\nL 86.032575 198.018529 \r\nL 86.084435 214.733138 \r\nL 86.136295 214.001288 \r\nL 86.188154 205.184444 \r\nL 86.240014 207.463054 \r\nL 86.343733 214.691352 \r\nL 86.395592 214.274715 \r\nL 86.447452 214.638672 \r\nL 86.499311 204.381316 \r\nL 86.551171 214.136026 \r\nL 86.60303 214.727205 \r\nL 86.706749 214.667661 \r\nL 86.758609 214.659308 \r\nL 86.810469 212.954312 \r\nL 86.862328 204.670756 \r\nL 86.914188 214.73965 \r\nL 86.966047 214.578849 \r\nL 87.017907 212.07701 \r\nL 87.069766 214.517452 \r\nL 87.121626 214.733765 \r\nL 87.173485 214.573221 \r\nL 87.277204 214.735354 \r\nL 87.329064 214.570691 \r\nL 87.380923 214.67034 \r\nL 87.432783 210.744849 \r\nL 87.484643 192.357642 \r\nL 87.640221 214.657762 \r\nL 87.692081 214.692462 \r\nL 87.847659 206.235473 \r\nL 87.951378 214.602508 \r\nL 88.003238 207.480798 \r\nL 88.055097 214.701951 \r\nL 88.106957 214.21845 \r\nL 88.158817 214.722246 \r\nL 88.210676 214.595802 \r\nL 88.262536 214.744842 \r\nL 88.418114 214.649164 \r\nL 88.469974 213.247555 \r\nL 88.521833 214.4686 \r\nL 88.573693 214.747767 \r\nL 88.625552 208.44655 \r\nL 88.677412 212.555125 \r\nL 88.729271 209.786067 \r\nL 88.832991 214.575688 \r\nL 88.88485 214.343326 \r\nL 89.040429 214.734261 \r\nL 89.092288 213.058703 \r\nL 89.144148 213.792412 \r\nL 89.247867 213.754561 \r\nL 89.351586 214.728786 \r\nL 89.403445 214.315419 \r\nL 89.455305 212.55694 \r\nL 89.507165 214.717663 \r\nL 89.559024 214.668292 \r\nL 89.610884 200.716773 \r\nL 89.662743 214.241369 \r\nL 89.714603 214.716883 \r\nL 89.766462 214.055264 \r\nL 89.818322 214.735594 \r\nL 89.870181 192.493491 \r\nL 89.922041 207.945229 \r\nL 89.9739 214.679764 \r\nL 90.07762 214.544075 \r\nL 90.129479 214.260957 \r\nL 90.181339 214.501833 \r\nL 90.285058 208.022398 \r\nL 90.440636 214.671332 \r\nL 90.492496 214.621669 \r\nL 90.544355 211.186575 \r\nL 90.596215 212.449074 \r\nL 90.699934 212.902165 \r\nL 90.751794 214.732926 \r\nL 90.803653 189.688122 \r\nL 90.855513 214.727495 \r\nL 90.907372 198.764015 \r\nL 90.959232 214.690665 \r\nL 91.11481 214.721119 \r\nL 91.16667 214.102218 \r\nL 91.218529 214.601601 \r\nL 91.270389 199.394939 \r\nL 91.322248 213.267402 \r\nL 91.374108 213.496926 \r\nL 91.425968 214.45846 \r\nL 91.477827 210.038999 \r\nL 91.529687 213.782576 \r\nL 91.581546 212.87865 \r\nL 91.685265 214.73879 \r\nL 91.737125 212.767446 \r\nL 91.788984 214.733339 \r\nL 91.840844 214.629078 \r\nL 91.892703 205.175151 \r\nL 91.944563 214.67142 \r\nL 91.996422 214.737974 \r\nL 92.048282 214.560683 \r\nL 92.100142 214.755739 \r\nL 92.152001 214.749846 \r\nL 92.25572 92.940604 \r\nL 92.30758 214.756286 \r\nL 92.411299 214.396835 \r\nL 92.463158 206.301314 \r\nL 92.515018 214.722557 \r\nL 92.566877 214.501696 \r\nL 92.618737 214.720781 \r\nL 92.722456 214.672536 \r\nL 92.774316 209.277176 \r\nL 92.826175 214.710034 \r\nL 92.878035 212.297142 \r\nL 92.929894 214.71438 \r\nL 92.981754 214.485075 \r\nL 93.085473 214.677092 \r\nL 93.137332 210.46251 \r\nL 93.189192 213.960267 \r\nL 93.241051 205.043208 \r\nL 93.292911 212.574735 \r\nL 93.34477 214.755063 \r\nL 93.39663 196.664 \r\nL 93.44849 211.211516 \r\nL 93.552209 214.740902 \r\nL 93.604068 214.706452 \r\nL 93.655928 214.705897 \r\nL 93.759647 210.113994 \r\nL 93.811506 214.746908 \r\nL 93.915225 214.477694 \r\nL 93.967085 214.746959 \r\nL 94.070804 210.376669 \r\nL 94.174523 214.748573 \r\nL 94.226383 214.63559 \r\nL 94.278242 214.73737 \r\nL 94.381961 212.571202 \r\nL 94.48568 214.746764 \r\nL 94.53754 213.107561 \r\nL 94.589399 213.725675 \r\nL 94.744978 214.706757 \r\nL 94.848697 214.751996 \r\nL 94.900557 198.851775 \r\nL 95.004276 214.746816 \r\nL 95.211714 214.303894 \r\nL 95.263573 214.637025 \r\nL 95.315433 213.792781 \r\nL 95.367293 214.747658 \r\nL 95.471012 214.655307 \r\nL 95.522871 213.206615 \r\nL 95.574731 214.749214 \r\nL 95.62659 214.695818 \r\nL 95.67845 214.128244 \r\nL 95.730309 214.749236 \r\nL 95.885888 214.695179 \r\nL 96.041467 212.428562 \r\nL 96.093326 213.02663 \r\nL 96.145186 214.748337 \r\nL 96.248905 214.741829 \r\nL 96.404483 208.806317 \r\nL 96.560062 214.745396 \r\nL 96.611921 212.646824 \r\nL 96.663781 213.713698 \r\nL 96.715641 214.600033 \r\nL 96.7675 214.349665 \r\nL 96.81936 211.267839 \r\nL 96.871219 214.364579 \r\nL 96.923079 214.752703 \r\nL 96.974938 212.471032 \r\nL 97.026798 213.959559 \r\nL 97.078657 214.113656 \r\nL 97.130517 214.745757 \r\nL 97.182376 214.517391 \r\nL 97.286095 214.638377 \r\nL 97.337955 208.744408 \r\nL 97.493534 214.749333 \r\nL 97.545393 214.433149 \r\nL 97.597253 214.742188 \r\nL 97.752831 214.723612 \r\nL 97.804691 212.328782 \r\nL 97.85655 214.727852 \r\nL 97.960269 214.034295 \r\nL 98.115848 214.749225 \r\nL 98.167708 214.369089 \r\nL 98.219567 214.750488 \r\nL 98.271427 214.717494 \r\nL 98.323286 212.162297 \r\nL 98.375146 213.355759 \r\nL 98.427005 214.711246 \r\nL 98.478865 209.77314 \r\nL 98.530724 194.131174 \r\nL 98.686303 214.748955 \r\nL 98.790022 214.711114 \r\nL 98.945601 214.67591 \r\nL 98.99746 149.163325 \r\nL 99.04932 178.427718 \r\nL 99.101179 214.724931 \r\nL 99.153039 214.584378 \r\nL 99.204898 204.17957 \r\nL 99.256758 214.714322 \r\nL 99.308618 211.123452 \r\nL 99.360477 214.701606 \r\nL 99.412337 214.718018 \r\nL 99.464196 213.14097 \r\nL 99.516056 181.20948 \r\nL 99.567915 214.620831 \r\nL 99.671634 214.733541 \r\nL 99.723494 203.747694 \r\nL 99.775353 214.479997 \r\nL 99.827213 213.656631 \r\nL 99.879072 214.74962 \r\nL 99.982792 178.00392 \r\nL 100.13837 214.746438 \r\nL 100.19023 204.039927 \r\nL 100.242089 208.117215 \r\nL 100.345808 214.650839 \r\nL 100.397668 214.636176 \r\nL 100.449527 214.501706 \r\nL 100.605106 214.737034 \r\nL 100.656966 214.730846 \r\nL 100.708825 214.535224 \r\nL 100.760685 214.743807 \r\nL 100.812544 213.270759 \r\nL 100.864404 214.595519 \r\nL 100.968123 214.591867 \r\nL 101.019982 214.740283 \r\nL 101.071842 214.702906 \r\nL 101.123701 214.568703 \r\nL 101.175561 212.27081 \r\nL 101.22742 214.75226 \r\nL 101.382999 213.988767 \r\nL 101.434859 214.69008 \r\nL 101.486718 212.472532 \r\nL 101.538578 214.637182 \r\nL 101.590437 214.743463 \r\nL 101.642297 211.731835 \r\nL 101.694156 211.785152 \r\nL 101.849735 214.720662 \r\nL 101.901594 214.703316 \r\nL 101.953454 213.795931 \r\nL 102.005314 203.737975 \r\nL 102.057173 208.60163 \r\nL 102.160892 214.659003 \r\nL 102.212752 214.608311 \r\nL 102.264611 214.506098 \r\nL 102.316471 213.426017 \r\nL 102.36833 214.372411 \r\nL 102.627628 214.720856 \r\nL 102.679488 214.165522 \r\nL 102.731347 210.60894 \r\nL 102.783207 214.471623 \r\nL 102.835066 214.409417 \r\nL 102.938785 212.342607 \r\nL 103.042504 214.74749 \r\nL 103.094364 214.144637 \r\nL 103.146223 214.754694 \r\nL 103.198083 214.741128 \r\nL 103.301802 214.191723 \r\nL 103.353662 214.720161 \r\nL 103.405521 214.620838 \r\nL 103.457381 213.979955 \r\nL 103.50924 214.678112 \r\nL 103.5611 214.041043 \r\nL 103.716678 214.741843 \r\nL 103.768538 213.235445 \r\nL 103.820397 213.7885 \r\nL 103.975976 214.715631 \r\nL 104.079695 214.597386 \r\nL 104.131555 214.178574 \r\nL 104.183414 214.326334 \r\nL 104.338993 214.73672 \r\nL 104.390852 176.432607 \r\nL 104.442712 214.732899 \r\nL 104.546431 194.711651 \r\nL 104.598291 212.895135 \r\nL 104.70201 211.981976 \r\nL 104.753869 214.72277 \r\nL 104.805729 165.635639 \r\nL 104.857588 197.967796 \r\nL 104.909448 187.157667 \r\nL 105.013167 214.72642 \r\nL 105.065026 203.331102 \r\nL 105.116886 213.709962 \r\nL 105.220605 214.747588 \r\nL 105.272465 200.91858 \r\nL 105.324324 214.609432 \r\nL 105.428043 214.724441 \r\nL 105.479903 203.081604 \r\nL 105.531762 210.917776 \r\nL 105.583622 214.734829 \r\nL 105.635481 186.077869 \r\nL 105.687341 208.132104 \r\nL 105.7392 214.671634 \r\nL 105.79106 211.400302 \r\nL 105.842919 211.456914 \r\nL 105.894779 214.701489 \r\nL 105.946639 214.63619 \r\nL 105.998498 212.857068 \r\nL 106.050358 214.74517 \r\nL 106.102217 214.747809 \r\nL 106.154077 211.0457 \r\nL 106.205936 212.501461 \r\nL 106.465234 214.752831 \r\nL 106.517093 214.727207 \r\nL 106.672672 213.125993 \r\nL 106.724532 198.310478 \r\nL 106.776391 213.675773 \r\nL 106.828251 214.75464 \r\nL 106.88011 214.677985 \r\nL 106.93197 205.934132 \r\nL 106.983829 214.721878 \r\nL 107.035689 214.658159 \r\nL 107.087548 213.589695 \r\nL 107.139408 214.736293 \r\nL 107.191267 202.461126 \r\nL 107.243127 214.724555 \r\nL 107.294987 214.221758 \r\nL 107.346846 214.662328 \r\nL 107.398706 199.916344 \r\nL 107.450565 211.571747 \r\nL 107.554284 214.735682 \r\nL 107.606144 214.277036 \r\nL 107.658003 201.130768 \r\nL 107.709863 213.297799 \r\nL 107.761722 214.747683 \r\nL 107.813582 214.632624 \r\nL 107.865441 210.664512 \r\nL 107.917301 214.258335 \r\nL 107.969161 214.663861 \r\nL 108.02102 213.99002 \r\nL 108.07288 214.715666 \r\nL 108.124739 214.556499 \r\nL 108.176599 214.714735 \r\nL 108.228458 213.504916 \r\nL 108.280318 207.221415 \r\nL 108.435896 214.755007 \r\nL 108.487756 210.615442 \r\nL 108.539616 214.697015 \r\nL 108.643335 214.738779 \r\nL 108.695194 213.710362 \r\nL 108.747054 214.020421 \r\nL 108.902632 214.752676 \r\nL 109.058211 214.65811 \r\nL 109.11007 214.729764 \r\nL 109.16193 213.859761 \r\nL 109.21379 199.37813 \r\nL 109.265649 213.926817 \r\nL 109.317509 213.387981 \r\nL 109.421228 214.722343 \r\nL 109.473087 214.393804 \r\nL 109.524947 214.703905 \r\nL 109.628666 214.739537 \r\nL 109.732385 214.748577 \r\nL 109.784244 208.90216 \r\nL 109.836104 212.396835 \r\nL 109.887964 212.490964 \r\nL 110.043542 214.749637 \r\nL 110.25098 214.724345 \r\nL 110.510278 214.721288 \r\nL 110.613997 214.549815 \r\nL 110.717716 214.616594 \r\nL 110.769576 204.393441 \r\nL 110.925154 214.626202 \r\nL 110.977014 214.654859 \r\nL 111.080733 214.277679 \r\nL 111.184452 214.75229 \r\nL 111.236312 214.720432 \r\nL 111.39189 214.491896 \r\nL 111.495609 214.741157 \r\nL 111.547469 207.965946 \r\nL 111.599328 212.002162 \r\nL 111.806766 214.747779 \r\nL 111.858626 214.726743 \r\nL 112.014205 214.606307 \r\nL 112.117924 214.755123 \r\nL 112.169783 214.467419 \r\nL 112.221643 214.748094 \r\nL 112.273502 214.74329 \r\nL 112.429081 213.898481 \r\nL 112.58466 214.634728 \r\nL 112.688379 214.574819 \r\nL 112.792098 214.241109 \r\nL 112.843957 214.75137 \r\nL 112.947676 214.712505 \r\nL 112.999536 214.543315 \r\nL 113.051395 214.728281 \r\nL 113.103255 212.14188 \r\nL 113.155115 214.748124 \r\nL 113.206974 214.220503 \r\nL 113.258834 214.699834 \r\nL 113.362553 214.750527 \r\nL 113.414412 195.911987 \r\nL 113.466272 214.73516 \r\nL 113.518131 211.716402 \r\nL 113.569991 214.58058 \r\nL 113.62185 214.492331 \r\nL 113.67371 213.871245 \r\nL 113.725569 214.732637 \r\nL 113.829289 210.44893 \r\nL 113.881148 212.478965 \r\nL 113.933008 214.705575 \r\nL 114.036727 214.67344 \r\nL 114.088586 212.935511 \r\nL 114.140446 214.677768 \r\nL 114.192305 214.342598 \r\nL 114.244165 214.667108 \r\nL 114.296024 214.7301 \r\nL 114.347884 214.098336 \r\nL 114.399743 214.677373 \r\nL 114.451603 214.245586 \r\nL 114.503463 214.504776 \r\nL 114.607182 214.746799 \r\nL 114.659041 212.533407 \r\nL 114.710901 214.28821 \r\nL 114.81462 214.740499 \r\nL 114.866479 213.684642 \r\nL 114.918339 214.519859 \r\nL 115.022058 214.713193 \r\nL 115.073917 214.541982 \r\nL 115.125777 214.650799 \r\nL 115.229496 214.730358 \r\nL 115.281356 214.459952 \r\nL 115.333215 205.277294 \r\nL 115.385075 212.053502 \r\nL 115.436934 214.747912 \r\nL 115.540653 214.705296 \r\nL 115.696232 213.942612 \r\nL 115.748091 214.730478 \r\nL 115.799951 214.601756 \r\nL 115.851811 214.162524 \r\nL 115.90367 214.234858 \r\nL 115.95553 214.715057 \r\nL 116.007389 214.621822 \r\nL 116.059249 209.307667 \r\nL 116.111108 214.208093 \r\nL 116.162968 213.047847 \r\nL 116.214827 214.18452 \r\nL 116.266687 214.137077 \r\nL 116.318546 214.750775 \r\nL 116.370406 214.510804 \r\nL 116.577844 214.72612 \r\nL 116.629704 214.67497 \r\nL 116.681563 214.2952 \r\nL 116.733423 214.754177 \r\nL 116.785282 214.422057 \r\nL 116.837142 213.505463 \r\nL 116.889001 214.547401 \r\nL 116.99272 214.752634 \r\nL 117.04458 214.619854 \r\nL 117.096439 214.739105 \r\nL 117.148299 214.678378 \r\nL 117.252018 214.744594 \r\nL 117.355737 211.228267 \r\nL 117.511316 214.749465 \r\nL 117.563175 214.735084 \r\nL 117.718754 213.924199 \r\nL 117.770614 214.73803 \r\nL 117.874333 214.745519 \r\nL 117.926192 211.696374 \r\nL 117.978052 214.750441 \r\nL 118.029911 214.74662 \r\nL 118.081771 211.123953 \r\nL 118.18549 211.438453 \r\nL 118.289209 214.227863 \r\nL 118.392928 214.311003 \r\nL 118.444788 186.250685 \r\nL 118.548507 214.753638 \r\nL 118.600366 214.726166 \r\nL 118.652226 208.535078 \r\nL 118.704085 214.737854 \r\nL 118.755945 214.646731 \r\nL 118.807804 185.543412 \r\nL 118.859664 213.806273 \r\nL 118.911523 214.295348 \r\nL 118.963383 213.281232 \r\nL 119.015242 214.654521 \r\nL 119.067102 212.783739 \r\nL 119.118962 214.728054 \r\nL 119.27454 214.753897 \r\nL 119.430119 210.688905 \r\nL 119.481978 214.72952 \r\nL 119.585697 214.61644 \r\nL 119.637557 207.983561 \r\nL 119.689416 209.600875 \r\nL 119.793136 214.715701 \r\nL 119.844995 213.828422 \r\nL 119.948714 214.714327 \r\nL 120.000574 214.230225 \r\nL 120.052433 214.751401 \r\nL 120.104293 208.01485 \r\nL 120.156152 214.746492 \r\nL 120.208012 214.727266 \r\nL 120.259871 214.152503 \r\nL 120.36359 201.649648 \r\nL 120.519169 214.671726 \r\nL 120.622888 213.680549 \r\nL 120.726607 214.178114 \r\nL 120.882186 214.731887 \r\nL 120.934045 214.726306 \r\nL 121.089624 206.099907 \r\nL 121.193343 214.755022 \r\nL 121.245203 200.610918 \r\nL 121.297062 214.557001 \r\nL 121.348922 214.734605 \r\nL 121.452641 214.747242 \r\nL 121.5045 213.394667 \r\nL 121.55636 214.741623 \r\nL 121.608219 214.459823 \r\nL 121.660079 190.840218 \r\nL 121.711938 213.890684 \r\nL 121.763798 214.723415 \r\nL 121.919377 203.35018 \r\nL 121.971236 214.752789 \r\nL 122.074955 214.645832 \r\nL 122.126815 214.597488 \r\nL 122.178674 214.748612 \r\nL 122.334253 182.102094 \r\nL 122.489832 214.385526 \r\nL 122.541691 212.504266 \r\nL 122.593551 202.106657 \r\nL 122.64541 210.949748 \r\nL 122.69727 214.734732 \r\nL 122.749129 214.731034 \r\nL 122.800989 211.449766 \r\nL 122.852848 214.020653 \r\nL 123.008427 214.739512 \r\nL 123.060287 209.945338 \r\nL 123.112146 214.72798 \r\nL 123.215865 214.429381 \r\nL 123.319584 214.746243 \r\nL 123.371444 213.411622 \r\nL 123.423303 214.363562 \r\nL 123.475163 214.71839 \r\nL 123.527022 197.796381 \r\nL 123.578882 213.049868 \r\nL 123.630741 202.024073 \r\nL 123.682601 214.741576 \r\nL 123.734461 211.596208 \r\nL 123.78632 208.104371 \r\nL 123.83818 214.744874 \r\nL 123.890039 214.606569 \r\nL 123.941899 200.05932 \r\nL 123.993758 214.350285 \r\nL 124.097477 214.734175 \r\nL 124.201196 191.39537 \r\nL 124.253056 214.582313 \r\nL 124.356775 211.350068 \r\nL 124.408635 214.753713 \r\nL 124.460494 214.737857 \r\nL 124.564213 214.74154 \r\nL 124.616073 189.535932 \r\nL 124.771651 214.745047 \r\nL 124.823511 214.613511 \r\nL 124.87537 202.006261 \r\nL 124.92723 203.736732 \r\nL 124.979089 214.568951 \r\nL 125.082809 214.289368 \r\nL 125.238387 214.74918 \r\nL 125.290247 187.157032 \r\nL 125.342106 214.284159 \r\nL 125.393966 212.967092 \r\nL 125.549544 214.732117 \r\nL 125.601404 214.290005 \r\nL 125.653263 214.723845 \r\nL 125.808842 214.710965 \r\nL 125.912561 214.740852 \r\nL 125.964421 213.353056 \r\nL 126.01628 214.740189 \r\nL 126.06814 214.675083 \r\nL 126.119999 196.070481 \r\nL 126.171859 214.722636 \r\nL 126.223718 214.411635 \r\nL 126.275578 205.030055 \r\nL 126.327437 214.04674 \r\nL 126.483016 214.744842 \r\nL 126.534876 214.558902 \r\nL 126.586735 214.750021 \r\nL 126.638595 203.009852 \r\nL 126.690454 214.616345 \r\nL 126.949752 214.733315 \r\nL 127.001612 213.840332 \r\nL 127.053471 214.54541 \r\nL 127.105331 208.52192 \r\nL 127.15719 214.417011 \r\nL 127.20905 214.744877 \r\nL 127.260909 214.156347 \r\nL 127.312769 214.712562 \r\nL 127.364628 214.751561 \r\nL 127.416488 181.218521 \r\nL 127.468347 210.814316 \r\nL 127.520207 214.645592 \r\nL 127.623926 214.64989 \r\nL 127.675786 201.534432 \r\nL 127.779505 214.67711 \r\nL 127.831364 214.151778 \r\nL 127.883224 213.694571 \r\nL 127.935083 211.403609 \r\nL 127.986943 213.640346 \r\nL 128.038802 214.449707 \r\nL 128.090662 214.244123 \r\nL 128.142521 208.256095 \r\nL 128.194381 214.07014 \r\nL 128.24624 214.058378 \r\nL 128.401819 214.716602 \r\nL 128.453679 214.634928 \r\nL 128.505538 212.401525 \r\nL 128.557398 200.986375 \r\nL 128.712976 214.557767 \r\nL 128.764836 204.109696 \r\nL 128.816695 214.265404 \r\nL 128.920414 214.733431 \r\nL 129.024134 214.34082 \r\nL 129.075993 210.09791 \r\nL 129.127853 214.694691 \r\nL 129.179712 210.150246 \r\nL 129.283431 214.741349 \r\nL 129.335291 214.707293 \r\nL 129.490869 214.652363 \r\nL 129.594588 189.099643 \r\nL 129.698308 214.750567 \r\nL 129.750167 214.253794 \r\nL 129.802027 214.753109 \r\nL 129.853886 214.666882 \r\nL 130.009465 214.713996 \r\nL 130.165043 214.399823 \r\nL 130.216903 212.386056 \r\nL 130.268762 213.846582 \r\nL 130.372482 214.723907 \r\nL 130.424341 214.222857 \r\nL 130.476201 214.658557 \r\nL 130.52806 214.708149 \r\nL 130.57992 214.45349 \r\nL 130.631779 214.554779 \r\nL 130.735498 214.741579 \r\nL 130.787358 214.702464 \r\nL 130.839217 214.755601 \r\nL 130.891077 212.909149 \r\nL 130.942936 213.725771 \r\nL 130.994796 214.456188 \r\nL 131.098515 211.003286 \r\nL 131.150375 214.739793 \r\nL 131.254094 214.690095 \r\nL 131.305953 214.748341 \r\nL 131.357813 214.667807 \r\nL 131.409672 213.440696 \r\nL 131.461532 214.718391 \r\nL 131.565251 214.75348 \r\nL 131.617111 214.753879 \r\nL 131.772689 213.834795 \r\nL 131.928268 214.721791 \r\nL 132.083846 214.732972 \r\nL 132.135706 205.383397 \r\nL 132.187565 208.074737 \r\nL 132.239425 214.750871 \r\nL 132.291285 214.732204 \r\nL 132.343144 213.745908 \r\nL 132.395004 185.488953 \r\nL 132.446863 214.713106 \r\nL 132.498723 214.745247 \r\nL 132.550582 214.114412 \r\nL 132.602442 211.664063 \r\nL 132.654301 213.382488 \r\nL 132.706161 214.747936 \r\nL 132.75802 214.475 \r\nL 132.80988 210.73636 \r\nL 132.861739 214.7427 \r\nL 132.913599 213.638927 \r\nL 132.965459 214.31472 \r\nL 133.017318 214.654808 \r\nL 133.069178 194.396963 \r\nL 133.121037 214.640307 \r\nL 133.172897 214.7242 \r\nL 133.224756 212.469499 \r\nL 133.276616 214.731351 \r\nL 133.432194 214.224688 \r\nL 133.587773 214.727035 \r\nL 133.691492 214.745057 \r\nL 133.743352 175.047117 \r\nL 133.795211 214.754339 \r\nL 133.847071 195.537911 \r\nL 133.89893 214.157286 \r\nL 133.95079 214.218078 \r\nL 134.002649 214.053961 \r\nL 134.106368 214.745475 \r\nL 134.158228 214.577474 \r\nL 134.210087 213.59802 \r\nL 134.261947 214.754797 \r\nL 134.365666 214.720371 \r\nL 134.417526 213.846512 \r\nL 134.469385 214.558014 \r\nL 134.521245 214.454672 \r\nL 134.573104 211.400426 \r\nL 134.624964 211.673256 \r\nL 134.780542 214.729775 \r\nL 134.884261 214.54203 \r\nL 134.936121 214.17341 \r\nL 134.987981 214.704801 \r\nL 135.03984 194.163848 \r\nL 135.0917 214.725255 \r\nL 135.299138 214.585498 \r\nL 135.350997 214.754878 \r\nL 135.402857 208.889811 \r\nL 135.454716 214.730236 \r\nL 135.506576 211.20889 \r\nL 135.558436 214.692632 \r\nL 135.610295 212.201152 \r\nL 135.662155 214.626655 \r\nL 135.714014 214.736636 \r\nL 135.765874 213.804616 \r\nL 135.817733 214.580751 \r\nL 135.869593 214.743523 \r\nL 135.921452 209.267473 \r\nL 135.973312 214.729024 \r\nL 136.077031 209.154404 \r\nL 136.12889 214.685159 \r\nL 136.23261 214.671555 \r\nL 136.336329 214.624017 \r\nL 136.388188 208.777736 \r\nL 136.440048 213.749788 \r\nL 136.491907 214.753611 \r\nL 136.543767 214.318116 \r\nL 136.595626 196.098607 \r\nL 136.647486 210.044936 \r\nL 136.751205 214.685675 \r\nL 136.854924 202.324746 \r\nL 137.010503 214.674229 \r\nL 137.062362 214.694164 \r\nL 137.114222 210.656154 \r\nL 137.166081 213.120684 \r\nL 137.217941 214.747286 \r\nL 137.2698 185.593075 \r\nL 137.32166 209.90399 \r\nL 137.373519 214.737076 \r\nL 137.425379 213.861274 \r\nL 137.477238 213.997816 \r\nL 137.529098 214.755037 \r\nL 137.632817 214.718133 \r\nL 137.736536 214.691451 \r\nL 137.840255 214.723728 \r\nL 137.943974 212.303176 \r\nL 138.099553 214.702642 \r\nL 138.306991 214.705619 \r\nL 138.358851 214.501074 \r\nL 138.41071 214.744656 \r\nL 138.46257 214.752033 \r\nL 138.566289 214.148351 \r\nL 138.618148 209.084808 \r\nL 138.670008 214.624892 \r\nL 138.721867 214.293745 \r\nL 138.773727 186.548493 \r\nL 138.825586 214.605837 \r\nL 138.981165 208.328603 \r\nL 139.084884 214.740443 \r\nL 139.136744 214.473347 \r\nL 139.188603 209.023597 \r\nL 139.240463 212.689462 \r\nL 139.292322 210.654877 \r\nL 139.447901 214.750968 \r\nL 139.55162 214.752218 \r\nL 139.60348 213.867752 \r\nL 139.655339 214.533204 \r\nL 139.707199 214.518802 \r\nL 139.759058 214.753081 \r\nL 139.810918 206.162826 \r\nL 139.862777 214.020946 \r\nL 139.914637 214.642147 \r\nL 139.966496 207.090823 \r\nL 140.018356 214.064411 \r\nL 140.070215 214.754154 \r\nL 140.122075 213.67112 \r\nL 140.173935 214.724514 \r\nL 140.277654 214.745784 \r\nL 140.329513 212.898355 \r\nL 140.381373 213.982867 \r\nL 140.433232 213.23107 \r\nL 140.485092 187.834648 \r\nL 140.536951 206.30895 \r\nL 140.796249 214.730644 \r\nL 140.899968 214.671921 \r\nL 140.951828 214.659849 \r\nL 141.003687 213.491691 \r\nL 141.055547 214.558575 \r\nL 141.107406 201.845069 \r\nL 141.159266 214.671777 \r\nL 141.211125 214.687855 \r\nL 141.262985 211.118062 \r\nL 141.314844 214.692923 \r\nL 141.470423 214.735124 \r\nL 141.522283 213.279376 \r\nL 141.574142 213.367337 \r\nL 141.626002 214.48741 \r\nL 141.677861 213.134793 \r\nL 141.729721 214.718748 \r\nL 141.78158 214.577434 \r\nL 141.83344 212.396315 \r\nL 141.885299 214.732823 \r\nL 141.989018 214.732811 \r\nL 142.092737 196.511393 \r\nL 142.144597 213.038319 \r\nL 142.196457 208.240438 \r\nL 142.248316 197.328963 \r\nL 142.300176 214.748404 \r\nL 142.403895 212.281108 \r\nL 142.455754 210.877795 \r\nL 142.611333 214.692571 \r\nL 142.663192 214.691473 \r\nL 142.715052 210.914781 \r\nL 142.766911 214.677112 \r\nL 142.92249 214.754361 \r\nL 142.97435 214.157076 \r\nL 143.026209 214.725751 \r\nL 143.129928 196.074906 \r\nL 143.233647 214.744123 \r\nL 143.285507 211.143748 \r\nL 143.389226 214.751168 \r\nL 143.441085 214.74712 \r\nL 143.648524 214.706953 \r\nL 143.700383 201.417494 \r\nL 143.752243 214.692165 \r\nL 143.855962 214.551059 \r\nL 143.959681 214.71344 \r\nL 144.0634 214.590725 \r\nL 144.167119 214.745274 \r\nL 144.218979 198.993777 \r\nL 144.374557 214.74079 \r\nL 144.530136 214.723932 \r\nL 144.581995 214.299284 \r\nL 144.633855 207.754484 \r\nL 144.685714 214.693234 \r\nL 144.737574 214.753331 \r\nL 144.789434 214.527232 \r\nL 144.841293 214.633832 \r\nL 144.893153 214.715853 \r\nL 144.945012 212.613943 \r\nL 144.996872 214.663008 \r\nL 145.048731 214.721323 \r\nL 145.100591 214.610776 \r\nL 145.15245 214.752615 \r\nL 145.20431 203.339128 \r\nL 145.256169 212.850507 \r\nL 145.411748 214.709954 \r\nL 145.463608 214.540043 \r\nL 145.619186 214.735211 \r\nL 145.671046 214.295561 \r\nL 145.722905 214.752724 \r\nL 145.774765 214.680447 \r\nL 145.826624 186.52726 \r\nL 145.878484 214.717322 \r\nL 145.930343 214.680254 \r\nL 146.085922 204.560922 \r\nL 146.241501 214.57557 \r\nL 146.29336 201.984094 \r\nL 146.34522 214.751036 \r\nL 146.500798 214.408036 \r\nL 146.552658 214.557552 \r\nL 146.604517 212.586595 \r\nL 146.656377 214.661989 \r\nL 146.708236 201.27985 \r\nL 146.760096 214.583128 \r\nL 146.811956 214.745269 \r\nL 146.863815 214.615754 \r\nL 146.915675 205.374794 \r\nL 146.967534 214.743324 \r\nL 147.019394 214.269717 \r\nL 147.123113 188.872855 \r\nL 147.226832 214.755341 \r\nL 147.278691 214.633056 \r\nL 147.330551 210.388163 \r\nL 147.38241 214.670092 \r\nL 147.48613 214.72446 \r\nL 147.537989 214.561855 \r\nL 147.589849 214.74295 \r\nL 147.641708 214.74767 \r\nL 147.693568 212.43494 \r\nL 147.797287 212.580224 \r\nL 147.901006 214.503915 \r\nL 147.952865 213.556103 \r\nL 148.004725 214.599238 \r\nL 148.160304 214.749259 \r\nL 148.212163 208.044844 \r\nL 148.264023 214.542999 \r\nL 148.315882 213.213685 \r\nL 148.367742 214.637089 \r\nL 148.419601 214.653775 \r\nL 148.471461 214.382046 \r\nL 148.52332 214.720953 \r\nL 148.57518 213.696239 \r\nL 148.627039 214.636759 \r\nL 148.678899 214.614226 \r\nL 148.782618 214.347623 \r\nL 148.938197 214.752102 \r\nL 149.041916 214.619203 \r\nL 149.093775 214.752176 \r\nL 149.249354 214.66914 \r\nL 149.301213 213.789399 \r\nL 149.353073 214.744886 \r\nL 149.456792 214.660516 \r\nL 149.508652 214.749961 \r\nL 149.560511 214.750288 \r\nL 149.612371 213.777693 \r\nL 149.71609 213.900173 \r\nL 149.819809 214.70567 \r\nL 149.871668 214.58354 \r\nL 150.027247 214.735204 \r\nL 150.130966 214.749204 \r\nL 150.182826 213.102856 \r\nL 150.234685 214.750636 \r\nL 150.338404 214.579176 \r\nL 150.390264 203.9662 \r\nL 150.442123 214.681062 \r\nL 150.493983 212.670152 \r\nL 150.545842 213.73848 \r\nL 150.597702 213.722695 \r\nL 150.649561 187.246636 \r\nL 150.701421 214.712251 \r\nL 150.753281 213.026524 \r\nL 150.80514 214.568323 \r\nL 150.857 197.680077 \r\nL 150.908859 214.471667 \r\nL 150.960719 212.532593 \r\nL 151.012578 214.714383 \r\nL 151.168157 213.564473 \r\nL 151.271876 214.749484 \r\nL 151.323735 197.208116 \r\nL 151.375595 214.743178 \r\nL 151.427455 214.742671 \r\nL 151.479314 214.10739 \r\nL 151.531174 214.136942 \r\nL 151.583033 214.600409 \r\nL 151.634893 210.559889 \r\nL 151.686752 214.710575 \r\nL 151.738612 214.317421 \r\nL 151.790471 214.690416 \r\nL 151.842331 214.755822 \r\nL 151.997909 214.125288 \r\nL 152.101629 214.742602 \r\nL 152.153488 213.712234 \r\nL 152.205348 214.629618 \r\nL 152.257207 214.623717 \r\nL 152.309067 212.283991 \r\nL 152.360926 214.732969 \r\nL 152.412786 213.880506 \r\nL 152.464645 214.273583 \r\nL 152.568364 214.714111 \r\nL 152.620224 214.69774 \r\nL 152.672083 214.495845 \r\nL 152.723943 211.176573 \r\nL 152.775803 213.573849 \r\nL 152.879522 214.754383 \r\nL 152.931381 214.740039 \r\nL 152.983241 214.509186 \r\nL 153.0351 212.071058 \r\nL 153.08696 214.748449 \r\nL 153.138819 182.601738 \r\nL 153.190679 214.743147 \r\nL 153.242538 214.111157 \r\nL 153.294398 214.372681 \r\nL 153.398117 214.67558 \r\nL 153.501836 214.705321 \r\nL 153.553696 214.044129 \r\nL 153.605555 214.742592 \r\nL 153.657415 212.554804 \r\nL 153.709274 214.578807 \r\nL 153.761134 214.049843 \r\nL 153.812993 214.627462 \r\nL 153.864853 214.684857 \r\nL 153.968572 213.396195 \r\nL 154.072291 214.747648 \r\nL 154.124151 214.723694 \r\nL 154.22787 214.74461 \r\nL 154.279729 212.383303 \r\nL 154.331589 214.50658 \r\nL 154.383448 214.735336 \r\nL 154.435308 214.707541 \r\nL 154.487167 214.483564 \r\nL 154.539027 214.753264 \r\nL 154.590886 214.335796 \r\nL 154.642746 198.680027 \r\nL 154.694606 214.465651 \r\nL 154.850184 214.747328 \r\nL 154.902044 212.552883 \r\nL 154.953903 214.294412 \r\nL 155.057622 214.753306 \r\nL 155.109482 213.741946 \r\nL 155.161341 214.736448 \r\nL 155.26506 214.165984 \r\nL 155.36878 209.837284 \r\nL 155.472499 214.755436 \r\nL 155.524358 214.052129 \r\nL 155.576218 214.753166 \r\nL 155.628077 214.702603 \r\nL 155.679937 214.406994 \r\nL 155.731796 214.647489 \r\nL 155.783656 214.478723 \r\nL 155.835515 213.783451 \r\nL 155.887375 214.20325 \r\nL 155.939234 214.236299 \r\nL 156.094813 214.743693 \r\nL 156.146673 214.744176 \r\nL 156.198532 213.494315 \r\nL 156.250392 214.730014 \r\nL 156.302251 213.8478 \r\nL 156.354111 214.739773 \r\nL 156.509689 214.083668 \r\nL 156.665268 214.75403 \r\nL 156.717128 214.753908 \r\nL 156.872706 213.757817 \r\nL 156.924566 214.753038 \r\nL 157.028285 214.718247 \r\nL 157.080144 214.732545 \r\nL 157.132004 214.432267 \r\nL 157.183863 214.755401 \r\nL 157.235723 214.609117 \r\nL 157.287582 214.70273 \r\nL 157.339442 214.742327 \r\nL 157.391302 204.417679 \r\nL 157.443161 214.687319 \r\nL 157.495021 214.40862 \r\nL 157.54688 214.619122 \r\nL 157.702459 214.74141 \r\nL 157.806178 214.719269 \r\nL 157.909897 138.578453 \r\nL 158.013616 214.752959 \r\nL 158.065476 214.642704 \r\nL 158.117335 208.994802 \r\nL 158.169195 214.744737 \r\nL 158.272914 214.554501 \r\nL 158.324773 214.7319 \r\nL 158.376633 214.269895 \r\nL 158.428492 214.744634 \r\nL 158.480352 214.753455 \r\nL 158.532211 214.530827 \r\nL 158.584071 214.753105 \r\nL 158.635931 211.284587 \r\nL 158.68779 214.724383 \r\nL 158.73965 214.637101 \r\nL 158.791509 214.744364 \r\nL 158.843369 213.819852 \r\nL 158.895228 214.729514 \r\nL 159.102666 214.310343 \r\nL 159.154526 214.706661 \r\nL 159.206385 202.586306 \r\nL 159.258245 213.319265 \r\nL 159.310105 214.684852 \r\nL 159.361964 214.042991 \r\nL 159.413824 207.247315 \r\nL 159.465683 214.75566 \r\nL 159.621262 212.203253 \r\nL 159.673121 214.700535 \r\nL 159.77684 194.016441 \r\nL 159.932419 214.651087 \r\nL 160.087998 214.754112 \r\nL 160.139857 210.996633 \r\nL 160.191717 214.749237 \r\nL 160.243576 214.671102 \r\nL 160.295436 213.207541 \r\nL 160.347295 214.728788 \r\nL 160.399155 214.172631 \r\nL 160.451014 214.750556 \r\nL 160.502874 203.442004 \r\nL 160.554733 213.056596 \r\nL 160.658453 214.755271 \r\nL 160.710312 212.61387 \r\nL 160.762172 214.059397 \r\nL 160.814031 214.753017 \r\nL 160.91775 212.326889 \r\nL 160.96961 214.743397 \r\nL 161.021469 182.277298 \r\nL 161.073329 213.92766 \r\nL 161.228907 214.754771 \r\nL 161.332627 213.43878 \r\nL 161.384486 207.232423 \r\nL 161.540065 214.723428 \r\nL 161.643784 214.747554 \r\nL 161.799362 214.568104 \r\nL 161.851222 212.163174 \r\nL 161.903081 214.743959 \r\nL 162.006801 214.40759 \r\nL 162.05866 214.564087 \r\nL 162.11052 214.719112 \r\nL 162.162379 214.541079 \r\nL 162.214239 214.492143 \r\nL 162.317958 214.750029 \r\nL 162.369817 214.412503 \r\nL 162.421677 214.752178 \r\nL 162.680975 214.731102 \r\nL 162.732834 212.350422 \r\nL 162.784694 214.641381 \r\nL 162.940272 214.75395 \r\nL 163.095851 214.723121 \r\nL 163.14771 203.06989 \r\nL 163.19957 214.734385 \r\nL 163.303289 213.495331 \r\nL 163.407008 214.744446 \r\nL 163.458868 213.682795 \r\nL 163.510727 214.707137 \r\nL 163.562587 214.439704 \r\nL 163.614446 214.711786 \r\nL 163.925604 214.735928 \r\nL 163.977463 214.741891 \r\nL 164.029323 214.512651 \r\nL 164.081182 213.362838 \r\nL 164.184901 214.749685 \r\nL 164.236761 213.875978 \r\nL 164.28862 202.075261 \r\nL 164.34048 207.404735 \r\nL 164.392339 214.745676 \r\nL 164.496058 214.677176 \r\nL 164.547918 213.397308 \r\nL 164.599778 214.734131 \r\nL 164.651637 214.589462 \r\nL 164.703497 214.650524 \r\nL 164.755356 214.753209 \r\nL 164.910935 211.807263 \r\nL 164.962794 214.752969 \r\nL 165.014654 214.750718 \r\nL 165.066513 211.485572 \r\nL 165.118373 214.389334 \r\nL 165.170232 214.679855 \r\nL 165.222092 206.898028 \r\nL 165.273952 212.610921 \r\nL 165.377671 214.745447 \r\nL 165.48139 212.921575 \r\nL 165.533249 214.754743 \r\nL 165.585109 203.439526 \r\nL 165.636968 214.75511 \r\nL 165.688828 213.43471 \r\nL 165.740687 214.75251 \r\nL 165.792547 202.197508 \r\nL 165.844406 214.016794 \r\nL 165.948126 205.621853 \r\nL 166.051845 214.75095 \r\nL 166.207423 208.97012 \r\nL 166.311142 214.750077 \r\nL 166.363002 211.736845 \r\nL 166.414861 214.752733 \r\nL 166.777878 214.727193 \r\nL 166.881597 214.675118 \r\nL 166.933457 173.417099 \r\nL 166.985316 203.472448 \r\nL 167.244614 214.75032 \r\nL 167.296474 206.11366 \r\nL 167.348333 214.733735 \r\nL 167.400193 214.544214 \r\nL 167.452052 181.793132 \r\nL 167.503912 204.193026 \r\nL 167.555771 214.742778 \r\nL 167.65949 214.70742 \r\nL 167.71135 214.450338 \r\nL 167.763209 214.7526 \r\nL 167.970648 214.727618 \r\nL 168.022507 214.531114 \r\nL 168.074367 209.81416 \r\nL 168.126226 210.41337 \r\nL 168.178086 163.918564 \r\nL 168.229945 214.707273 \r\nL 168.281805 211.150132 \r\nL 168.333664 214.737814 \r\nL 168.385524 214.706304 \r\nL 168.541103 214.485607 \r\nL 168.644822 214.722475 \r\nL 168.696681 206.148436 \r\nL 168.748541 214.621508 \r\nL 168.8004 214.635824 \r\nL 168.85226 209.733433 \r\nL 168.904119 211.624483 \r\nL 169.059698 214.753124 \r\nL 169.111557 209.871528 \r\nL 169.163417 213.265261 \r\nL 169.215277 212.621151 \r\nL 169.267136 210.41801 \r\nL 169.318996 214.755597 \r\nL 169.422715 214.615558 \r\nL 169.578293 214.74649 \r\nL 169.733872 214.695995 \r\nL 169.785731 214.414559 \r\nL 169.837591 214.619611 \r\nL 169.99317 214.749833 \r\nL 170.045029 214.750866 \r\nL 170.200608 207.23036 \r\nL 170.304327 214.717674 \r\nL 170.356186 194.010849 \r\nL 170.408046 200.310631 \r\nL 170.511765 214.751855 \r\nL 170.563625 214.751003 \r\nL 170.667344 214.749703 \r\nL 170.771063 214.102632 \r\nL 170.874782 206.860766 \r\nL 170.978501 214.754125 \r\nL 171.03036 213.568962 \r\nL 171.134079 214.731559 \r\nL 171.185939 191.190535 \r\nL 171.237799 210.491068 \r\nL 171.289658 206.066621 \r\nL 171.341518 214.73902 \r\nL 171.445237 214.337479 \r\nL 171.497096 214.742523 \r\nL 171.548956 214.082758 \r\nL 171.600815 214.733499 \r\nL 171.756394 214.734009 \r\nL 172.17127 214.745397 \r\nL 172.326849 214.671324 \r\nL 172.378708 214.138718 \r\nL 172.430568 214.738147 \r\nL 172.534287 213.903752 \r\nL 172.638006 213.805501 \r\nL 172.689866 214.741581 \r\nL 172.741725 211.851674 \r\nL 172.793585 212.618209 \r\nL 172.845444 214.747031 \r\nL 172.949163 214.46919 \r\nL 173.001023 214.736574 \r\nL 173.052882 214.701473 \r\nL 173.104742 213.554682 \r\nL 173.156602 213.557411 \r\nL 173.260321 214.742846 \r\nL 173.31218 213.584435 \r\nL 173.36404 214.139634 \r\nL 173.467759 214.748491 \r\nL 173.519618 214.690674 \r\nL 173.571478 211.881094 \r\nL 173.623337 214.326965 \r\nL 173.675197 214.445935 \r\nL 173.727056 210.174992 \r\nL 173.778916 214.746553 \r\nL 173.830776 211.04427 \r\nL 173.882635 214.703719 \r\nL 173.934495 214.617461 \r\nL 173.986354 214.684211 \r\nL 174.090073 209.031558 \r\nL 174.193792 214.66387 \r\nL 174.245652 212.482357 \r\nL 174.297511 214.3929 \r\nL 174.40123 214.747442 \r\nL 174.45309 214.382799 \r\nL 174.50495 211.20723 \r\nL 174.556809 174.44812 \r\nL 174.608669 214.472447 \r\nL 174.660528 203.57334 \r\nL 174.712388 214.571529 \r\nL 174.816107 214.746301 \r\nL 174.867966 214.728434 \r\nL 174.919826 210.521289 \r\nL 174.971685 214.271069 \r\nL 175.023545 213.653055 \r\nL 175.075404 214.561529 \r\nL 175.127264 213.406992 \r\nL 175.179124 214.231005 \r\nL 175.282843 214.755782 \r\nL 175.490281 214.293876 \r\nL 175.54214 214.506232 \r\nL 175.594 199.769286 \r\nL 175.645859 214.639779 \r\nL 175.697719 200.691937 \r\nL 175.749578 214.694174 \r\nL 175.853298 213.688121 \r\nL 175.905157 212.408911 \r\nL 175.957017 214.691865 \r\nL 176.008876 202.267218 \r\nL 176.060736 214.752002 \r\nL 176.164455 214.753125 \r\nL 176.268174 214.161107 \r\nL 176.320033 214.649527 \r\nL 176.371893 212.419126 \r\nL 176.423752 163.164525 \r\nL 176.475612 180.983607 \r\nL 176.631191 214.740015 \r\nL 176.68305 214.639488 \r\nL 176.73491 214.75557 \r\nL 176.786769 202.875218 \r\nL 176.838629 214.700892 \r\nL 176.890488 212.060192 \r\nL 176.942348 214.510321 \r\nL 176.994207 214.509991 \r\nL 177.046067 214.197053 \r\nL 177.097927 202.011329 \r\nL 177.149786 214.748687 \r\nL 177.201646 214.734088 \r\nL 177.253505 195.221338 \r\nL 177.305365 214.719555 \r\nL 177.357224 211.987622 \r\nL 177.409084 212.643507 \r\nL 177.564662 214.752046 \r\nL 177.82396 214.653561 \r\nL 177.87582 214.202515 \r\nL 177.927679 212.43893 \r\nL 178.031398 214.73098 \r\nL 178.083258 214.660231 \r\nL 178.238836 214.567229 \r\nL 178.394415 214.734508 \r\nL 178.446275 214.71698 \r\nL 178.498134 214.412953 \r\nL 178.549994 214.549732 \r\nL 178.653713 214.741463 \r\nL 178.705572 213.847001 \r\nL 178.757432 214.748066 \r\nL 178.809291 214.753967 \r\nL 178.96487 214.246812 \r\nL 179.016729 214.694552 \r\nL 179.068589 214.653002 \r\nL 179.120449 213.587117 \r\nL 179.172308 214.419785 \r\nL 179.276027 214.752155 \r\nL 179.327887 214.743556 \r\nL 179.431606 214.691946 \r\nL 179.587184 214.556935 \r\nL 179.690903 214.742726 \r\nL 179.794623 214.267286 \r\nL 179.950201 214.747198 \r\nL 180.157639 214.699772 \r\nL 180.209499 214.471124 \r\nL 180.261358 214.719271 \r\nL 180.313218 214.586281 \r\nL 180.365077 214.753987 \r\nL 180.416937 200.205894 \r\nL 180.468797 205.755596 \r\nL 180.520656 203.66663 \r\nL 180.572516 214.750325 \r\nL 180.676235 214.702867 \r\nL 180.728094 202.682995 \r\nL 180.779954 212.830376 \r\nL 180.831813 209.304181 \r\nL 180.883673 214.725629 \r\nL 181.039251 202.195865 \r\nL 181.19483 214.753617 \r\nL 181.24669 214.129392 \r\nL 181.298549 214.754108 \r\nL 181.350409 214.741253 \r\nL 181.402268 213.547733 \r\nL 181.454128 214.528774 \r\nL 181.557847 214.742042 \r\nL 181.609706 214.716134 \r\nL 181.661566 213.893784 \r\nL 181.713426 214.053874 \r\nL 181.869004 198.903247 \r\nL 181.972723 214.592551 \r\nL 182.024583 214.100731 \r\nL 182.076442 213.68147 \r\nL 182.180161 214.7368 \r\nL 182.232021 209.371116 \r\nL 182.28388 214.706389 \r\nL 182.33574 214.706513 \r\nL 182.439459 214.610692 \r\nL 182.491319 213.48012 \r\nL 182.543178 214.750002 \r\nL 182.595038 214.734485 \r\nL 182.646897 195.284201 \r\nL 182.698757 197.494032 \r\nL 182.854335 214.235239 \r\nL 182.958054 201.168026 \r\nL 183.009914 214.754835 \r\nL 183.061774 190.400093 \r\nL 183.113633 213.90301 \r\nL 183.165493 214.736478 \r\nL 183.269212 210.308942 \r\nL 183.321071 214.748759 \r\nL 183.42479 214.730205 \r\nL 183.47665 213.780816 \r\nL 183.528509 214.742914 \r\nL 183.632228 214.729601 \r\nL 183.684088 197.623352 \r\nL 183.735948 214.712621 \r\nL 183.787807 214.510668 \r\nL 183.839667 209.967438 \r\nL 183.891526 212.305547 \r\nL 183.943386 212.351043 \r\nL 183.995245 214.738391 \r\nL 184.098964 214.651096 \r\nL 184.202683 210.32092 \r\nL 184.306402 214.755719 \r\nL 184.358262 209.013402 \r\nL 184.410122 211.984729 \r\nL 184.513841 214.755164 \r\nL 184.5657 212.859823 \r\nL 184.61756 214.727841 \r\nL 184.669419 214.716616 \r\nL 184.721279 214.237362 \r\nL 184.773138 214.723894 \r\nL 184.824998 214.388136 \r\nL 184.876857 214.731128 \r\nL 185.032436 214.739856 \r\nL 185.084296 207.352004 \r\nL 185.136155 209.724674 \r\nL 185.291734 214.738124 \r\nL 185.343593 180.563349 \r\nL 185.395453 214.69763 \r\nL 185.447312 209.415372 \r\nL 185.499172 214.702813 \r\nL 185.551031 214.311484 \r\nL 185.602891 214.619195 \r\nL 185.75847 214.749983 \r\nL 186.017767 214.730217 \r\nL 186.225205 214.744985 \r\nL 186.277065 214.625034 \r\nL 186.432644 212.437511 \r\nL 186.484503 214.751737 \r\nL 186.588222 214.687901 \r\nL 186.640082 214.529155 \r\nL 186.79566 214.739518 \r\nL 186.951239 214.685253 \r\nL 187.003099 214.051501 \r\nL 187.054958 204.535319 \r\nL 187.106818 214.497015 \r\nL 187.158677 213.794791 \r\nL 187.210537 214.710745 \r\nL 187.262396 213.241599 \r\nL 187.314256 214.585695 \r\nL 187.417975 214.75442 \r\nL 187.469834 214.735512 \r\nL 187.521694 214.267269 \r\nL 187.573553 214.552444 \r\nL 187.625413 214.744041 \r\nL 187.677273 214.56251 \r\nL 187.729132 214.463739 \r\nL 187.780992 212.774172 \r\nL 187.832851 213.461514 \r\nL 187.98843 214.742298 \r\nL 188.040289 214.552713 \r\nL 188.092149 214.735218 \r\nL 188.144008 213.564773 \r\nL 188.195868 214.75028 \r\nL 188.247727 214.735011 \r\nL 188.299587 211.265891 \r\nL 188.351447 214.080227 \r\nL 188.403306 213.384777 \r\nL 188.455166 214.615897 \r\nL 188.507025 212.853464 \r\nL 188.558885 214.65273 \r\nL 188.610744 214.722763 \r\nL 188.662604 214.659931 \r\nL 188.714463 210.793756 \r\nL 188.766323 214.753669 \r\nL 188.818182 214.734268 \r\nL 188.921901 214.251851 \r\nL 188.973761 214.753759 \r\nL 189.07748 214.693268 \r\nL 189.12934 214.719564 \r\nL 189.181199 214.603074 \r\nL 189.233059 214.751365 \r\nL 189.440497 214.716048 \r\nL 189.492356 198.581683 \r\nL 189.544216 214.464713 \r\nL 189.647935 214.717305 \r\nL 189.699795 202.003861 \r\nL 189.751654 214.723094 \r\nL 189.803514 214.574399 \r\nL 189.855373 210.968935 \r\nL 189.907233 214.752726 \r\nL 189.959092 214.733314 \r\nL 190.010952 209.808271 \r\nL 190.062811 214.742859 \r\nL 190.114671 214.709393 \r\nL 190.21839 214.48971 \r\nL 190.373969 214.75544 \r\nL 190.425828 214.736862 \r\nL 190.477688 193.195963 \r\nL 190.529547 214.658521 \r\nL 190.581407 212.761782 \r\nL 190.633266 203.132958 \r\nL 190.685126 214.75253 \r\nL 190.788845 214.333735 \r\nL 190.840704 214.397617 \r\nL 190.892564 214.750465 \r\nL 190.944424 213.411875 \r\nL 190.996283 204.792917 \r\nL 191.048143 214.274627 \r\nL 191.203721 214.743407 \r\nL 191.3593 214.082576 \r\nL 191.411159 214.749097 \r\nL 191.463019 214.730024 \r\nL 191.566738 168.392537 \r\nL 191.670457 214.689495 \r\nL 191.722317 212.037711 \r\nL 191.826036 214.707819 \r\nL 191.877895 211.666827 \r\nL 191.929755 213.66151 \r\nL 191.981614 214.753371 \r\nL 192.033474 214.628342 \r\nL 192.189052 214.558849 \r\nL 192.396491 214.753216 \r\nL 192.44835 213.913966 \r\nL 192.50021 214.751362 \r\nL 192.552069 214.747161 \r\nL 192.603929 207.755891 \r\nL 192.655788 214.74756 \r\nL 192.707648 210.894386 \r\nL 192.759507 214.33768 \r\nL 192.863226 214.747131 \r\nL 192.915086 214.344722 \r\nL 192.966946 214.740512 \r\nL 193.018805 203.901099 \r\nL 193.070665 214.703751 \r\nL 193.174384 212.782567 \r\nL 193.226243 190.268341 \r\nL 193.278103 214.755842 \r\nL 193.329962 213.939254 \r\nL 193.381822 214.735864 \r\nL 193.433681 214.461797 \r\nL 193.485541 214.705663 \r\nL 193.744839 214.748322 \r\nL 193.848558 214.676254 \r\nL 193.900417 213.788525 \r\nL 193.952277 214.753885 \r\nL 194.004136 214.617242 \r\nL 194.055996 214.746797 \r\nL 194.263434 214.715759 \r\nL 194.470872 214.407924 \r\nL 194.522732 214.754235 \r\nL 194.574591 214.729103 \r\nL 194.626451 214.443949 \r\nL 194.67831 214.751985 \r\nL 194.782029 214.745022 \r\nL 194.833889 214.636461 \r\nL 194.885749 214.66964 \r\nL 194.937608 212.319256 \r\nL 194.989468 214.742401 \r\nL 195.041327 214.74933 \r\nL 195.093187 214.55188 \r\nL 195.145046 214.711371 \r\nL 195.196906 214.598181 \r\nL 195.248765 214.754046 \r\nL 195.404344 209.787101 \r\nL 195.559923 214.745263 \r\nL 195.611782 214.591775 \r\nL 195.663642 214.742798 \r\nL 195.715501 214.754065 \r\nL 195.767361 212.403847 \r\nL 195.81922 214.749049 \r\nL 196.078518 214.732522 \r\nL 196.130377 213.460941 \r\nL 196.182237 213.746545 \r\nL 196.285956 214.724593 \r\nL 196.337816 214.271204 \r\nL 196.389675 214.651954 \r\nL 196.441535 198.023327 \r\nL 196.493394 211.89863 \r\nL 196.545254 214.755373 \r\nL 196.648973 214.743086 \r\nL 196.700832 214.751884 \r\nL 196.752692 210.607877 \r\nL 196.804551 214.750949 \r\nL 196.856411 214.748611 \r\nL 196.908271 208.125351 \r\nL 196.96013 214.75149 \r\nL 197.01199 214.754243 \r\nL 197.063849 214.562466 \r\nL 197.115709 213.931198 \r\nL 197.167568 214.754472 \r\nL 197.219428 214.676723 \r\nL 197.323147 214.296848 \r\nL 197.375006 214.708104 \r\nL 197.426866 214.498795 \r\nL 197.530585 214.74631 \r\nL 197.582445 203.227747 \r\nL 197.634304 212.83466 \r\nL 197.686164 214.746937 \r\nL 197.789883 214.668509 \r\nL 197.841742 214.082155 \r\nL 197.893602 214.671469 \r\nL 197.997321 214.721316 \r\nL 198.04918 213.098994 \r\nL 198.152899 214.742017 \r\nL 198.204759 214.733584 \r\nL 198.360338 214.734903 \r\nL 198.464057 214.647191 \r\nL 198.567776 214.721211 \r\nL 198.619635 202.175498 \r\nL 198.671495 213.845272 \r\nL 198.775214 203.32429 \r\nL 198.827073 213.972676 \r\nL 198.878933 186.709661 \r\nL 198.930793 214.592929 \r\nL 198.982652 214.755536 \r\nL 199.034512 214.028919 \r\nL 199.086371 211.566554 \r\nL 199.19009 214.752588 \r\nL 199.24195 213.749688 \r\nL 199.293809 214.755684 \r\nL 199.656826 214.75208 \r\nL 199.708686 213.993526 \r\nL 199.760545 214.646783 \r\nL 199.812405 214.755269 \r\nL 199.864264 214.700218 \r\nL 199.916124 214.712052 \r\nL 199.967983 212.771362 \r\nL 200.019843 214.601737 \r\nL 200.123562 214.704963 \r\nL 200.175422 205.840807 \r\nL 200.227281 214.197591 \r\nL 200.279141 214.749648 \r\nL 200.331 207.729418 \r\nL 200.38286 214.564799 \r\nL 200.434719 214.753976 \r\nL 200.486579 214.54865 \r\nL 200.590298 214.756119 \r\nL 200.642157 208.785958 \r\nL 200.694017 214.729603 \r\nL 200.745876 214.746534 \r\nL 200.797736 180.422504 \r\nL 200.849596 214.726387 \r\nL 200.901455 212.992083 \r\nL 200.953315 214.753035 \r\nL 201.005174 214.740587 \r\nL 201.057034 214.39718 \r\nL 201.108893 214.55494 \r\nL 201.160753 209.287695 \r\nL 201.212612 214.677224 \r\nL 201.264472 213.111324 \r\nL 201.316331 214.704377 \r\nL 201.47191 214.592379 \r\nL 201.52377 214.734034 \r\nL 201.575629 180.396737 \r\nL 201.627489 214.73205 \r\nL 201.679348 214.622309 \r\nL 201.731208 160.258911 \r\nL 201.783067 184.103429 \r\nL 201.834927 214.747819 \r\nL 201.886786 214.618657 \r\nL 201.938646 204.427436 \r\nL 201.990505 214.753982 \r\nL 202.042365 214.728394 \r\nL 202.094224 208.82899 \r\nL 202.146084 213.826503 \r\nL 202.197944 214.715385 \r\nL 202.249803 206.487201 \r\nL 202.301663 213.88222 \r\nL 202.353522 213.973789 \r\nL 202.509101 214.746147 \r\nL 202.61282 214.657679 \r\nL 202.664679 214.018122 \r\nL 202.716539 206.565685 \r\nL 202.768398 214.715343 \r\nL 202.872118 214.755124 \r\nL 202.923977 213.914222 \r\nL 202.975837 214.753544 \r\nL 203.079556 214.71949 \r\nL 203.131415 214.625509 \r\nL 203.183275 214.752026 \r\nL 203.235134 214.755573 \r\nL 203.286994 177.27114 \r\nL 203.338853 214.739166 \r\nL 203.442572 214.750632 \r\nL 203.494432 186.436244 \r\nL 203.598151 214.725722 \r\nL 203.650011 214.644054 \r\nL 203.805589 214.754759 \r\nL 203.857449 196.058419 \r\nL 203.909308 214.684456 \r\nL 203.961168 212.696269 \r\nL 204.013027 214.749394 \r\nL 204.116747 214.678667 \r\nL 204.168606 214.37933 \r\nL 204.220466 214.730767 \r\nL 204.531623 214.755354 \r\nL 204.687201 214.747798 \r\nL 204.84278 214.714478 \r\nL 204.89464 210.949007 \r\nL 204.946499 214.710256 \r\nL 204.998359 214.740363 \r\nL 205.050218 214.417411 \r\nL 205.102078 211.611848 \r\nL 205.153937 214.743125 \r\nL 205.205797 214.75524 \r\nL 205.257656 213.048316 \r\nL 205.309516 214.576126 \r\nL 205.361375 214.754983 \r\nL 205.413235 214.395656 \r\nL 205.465095 202.630631 \r\nL 205.516954 214.735238 \r\nL 205.568814 214.546779 \r\nL 205.620673 212.746647 \r\nL 205.672533 214.733122 \r\nL 205.724392 191.535628 \r\nL 205.776252 214.088658 \r\nL 205.828111 214.656297 \r\nL 205.879971 214.293964 \r\nL 205.93183 214.365552 \r\nL 206.035549 210.249885 \r\nL 206.139269 214.668723 \r\nL 206.191128 214.630495 \r\nL 206.242988 210.857121 \r\nL 206.294847 214.753771 \r\nL 206.346707 214.563792 \r\nL 206.398566 214.739016 \r\nL 206.502285 214.604939 \r\nL 206.554145 214.183791 \r\nL 206.606004 199.544313 \r\nL 206.657864 214.674323 \r\nL 206.709723 214.454384 \r\nL 206.761583 203.114836 \r\nL 206.813443 214.736333 \r\nL 206.865302 214.595254 \r\nL 206.917162 213.95221 \r\nL 207.020881 214.752067 \r\nL 207.07274 214.665688 \r\nL 207.1246 214.700841 \r\nL 207.176459 208.819373 \r\nL 207.228319 214.653704 \r\nL 207.280178 214.506586 \r\nL 207.383897 214.751875 \r\nL 207.435757 213.031591 \r\nL 207.487617 214.7093 \r\nL 207.539476 214.7096 \r\nL 207.591336 214.382966 \r\nL 207.643195 214.71448 \r\nL 207.746914 214.755292 \r\nL 207.798774 209.564501 \r\nL 207.850633 214.741141 \r\nL 207.902493 214.747081 \r\nL 208.006212 190.988443 \r\nL 208.161791 214.752053 \r\nL 208.21365 191.662516 \r\nL 208.26551 214.677625 \r\nL 208.317369 214.24164 \r\nL 208.369229 214.733913 \r\nL 208.421088 214.666603 \r\nL 208.524807 213.016255 \r\nL 208.576667 214.685934 \r\nL 208.628526 214.567038 \r\nL 208.680386 213.926533 \r\nL 208.732246 214.726844 \r\nL 208.835965 214.641088 \r\nL 208.939684 214.598447 \r\nL 208.991543 195.249911 \r\nL 209.043403 214.724789 \r\nL 209.095262 214.032666 \r\nL 209.147122 199.586744 \r\nL 209.198981 213.038069 \r\nL 209.250841 214.742472 \r\nL 209.35456 214.705406 \r\nL 209.40642 183.396629 \r\nL 209.458279 214.747322 \r\nL 209.510139 214.70743 \r\nL 209.613858 214.724438 \r\nL 209.665717 212.240028 \r\nL 209.769436 214.748876 \r\nL 209.821296 214.680432 \r\nL 209.925015 214.729175 \r\nL 210.028734 210.128191 \r\nL 210.080594 195.662973 \r\nL 210.132453 214.679433 \r\nL 210.184313 214.272036 \r\nL 210.236172 200.673574 \r\nL 210.288032 214.584979 \r\nL 210.339891 208.132519 \r\nL 210.391751 214.752146 \r\nL 210.44361 212.607444 \r\nL 210.49547 214.746601 \r\nL 210.651048 214.722669 \r\nL 210.702908 214.716531 \r\nL 210.754768 203.327085 \r\nL 210.806627 214.190812 \r\nL 210.910346 214.733556 \r\nL 210.962206 213.937591 \r\nL 211.014065 214.215043 \r\nL 211.065925 214.144619 \r\nL 211.117784 214.647311 \r\nL 211.169644 213.674683 \r\nL 211.221503 214.724299 \r\nL 211.325222 213.687034 \r\nL 211.377082 214.753671 \r\nL 211.480801 214.716579 \r\nL 211.532661 213.719006 \r\nL 211.58452 214.751213 \r\nL 211.740099 212.830503 \r\nL 211.895677 214.688258 \r\nL 211.947537 190.703848 \r\nL 211.999396 206.462992 \r\nL 212.051256 214.609456 \r\nL 212.154975 214.422905 \r\nL 212.206835 213.587802 \r\nL 212.258694 214.644089 \r\nL 212.362413 214.56434 \r\nL 212.466132 214.749472 \r\nL 212.517992 212.979457 \r\nL 212.569851 214.74987 \r\nL 212.621711 214.748459 \r\nL 212.72543 214.752054 \r\nL 212.77729 208.937192 \r\nL 212.932868 214.748525 \r\nL 212.984728 213.895595 \r\nL 213.036587 214.587352 \r\nL 213.192166 214.754738 \r\nL 213.244025 201.035903 \r\nL 213.295885 209.374281 \r\nL 213.347745 214.586161 \r\nL 213.399604 175.63761 \r\nL 213.451464 204.094564 \r\nL 213.503323 214.747629 \r\nL 213.555183 185.115062 \r\nL 213.607042 204.803811 \r\nL 213.710761 214.671265 \r\nL 213.762621 214.53346 \r\nL 213.918199 214.754414 \r\nL 213.970059 214.716313 \r\nL 214.021919 214.355299 \r\nL 214.073778 214.75456 \r\nL 214.125638 214.634366 \r\nL 214.177497 214.711129 \r\nL 214.229357 188.99935 \r\nL 214.281216 214.69808 \r\nL 214.333076 214.730177 \r\nL 214.384935 213.344316 \r\nL 214.436795 214.739086 \r\nL 214.488654 214.744655 \r\nL 214.592373 214.265307 \r\nL 214.644233 212.000224 \r\nL 214.696093 214.750086 \r\nL 214.747952 214.494173 \r\nL 214.851671 213.233057 \r\nL 214.95539 214.669539 \r\nL 215.00725 213.93889 \r\nL 215.059109 214.68867 \r\nL 215.110969 194.590779 \r\nL 215.162828 214.752966 \r\nL 215.214688 199.305718 \r\nL 215.266547 214.637522 \r\nL 215.318407 214.727986 \r\nL 215.370267 197.106923 \r\nL 215.422126 214.721336 \r\nL 215.473986 203.519024 \r\nL 215.525845 213.184433 \r\nL 215.577705 214.748235 \r\nL 215.629564 214.66657 \r\nL 215.733283 214.167971 \r\nL 215.785143 214.518876 \r\nL 215.837002 214.273371 \r\nL 215.888862 208.430046 \r\nL 215.940721 213.926179 \r\nL 216.0963 214.752352 \r\nL 216.14816 196.997931 \r\nL 216.200019 200.89813 \r\nL 216.355598 214.755362 \r\nL 216.407457 214.700987 \r\nL 216.459317 214.054079 \r\nL 216.511176 214.714285 \r\nL 216.614895 214.749863 \r\nL 216.718615 206.753214 \r\nL 216.770474 214.746365 \r\nL 216.926053 204.186879 \r\nL 216.977912 204.347338 \r\nL 217.029772 214.754719 \r\nL 217.133491 214.723803 \r\nL 217.18535 214.634637 \r\nL 217.23721 214.255652 \r\nL 217.289069 214.695024 \r\nL 217.444648 214.754013 \r\nL 217.496508 214.20836 \r\nL 217.548367 214.70869 \r\nL 217.755805 214.746965 \r\nL 217.807665 214.243675 \r\nL 217.859524 214.700021 \r\nL 217.911384 214.746071 \r\nL 217.963244 209.669827 \r\nL 218.066963 210.094799 \r\nL 218.118822 214.733181 \r\nL 218.222541 214.688225 \r\nL 218.274401 214.74837 \r\nL 218.32626 214.172407 \r\nL 218.37812 214.539866 \r\nL 218.429979 214.673626 \r\nL 218.533698 210.034581 \r\nL 218.585558 214.740008 \r\nL 218.689277 214.698817 \r\nL 218.792996 214.648283 \r\nL 218.844856 214.354834 \r\nL 218.896715 214.748248 \r\nL 219.052294 197.810821 \r\nL 219.207872 213.870745 \r\nL 219.311592 214.752897 \r\nL 219.363451 213.964377 \r\nL 219.415311 214.344739 \r\nL 219.46717 214.677315 \r\nL 219.51903 209.598038 \r\nL 219.570889 180.361278 \r\nL 219.622749 210.089168 \r\nL 219.674608 214.741172 \r\nL 219.778327 214.68453 \r\nL 219.882046 203.223414 \r\nL 219.985766 214.747584 \r\nL 220.037625 214.565225 \r\nL 220.089485 214.401621 \r\nL 220.141344 213.656389 \r\nL 220.245063 214.753652 \r\nL 220.296923 214.750165 \r\nL 220.348782 205.529211 \r\nL 220.400642 208.171814 \r\nL 220.504361 214.644415 \r\nL 220.55622 214.584215 \r\nL 220.60808 214.726951 \r\nL 220.65994 214.678687 \r\nL 220.711799 214.435104 \r\nL 220.763659 210.396201 \r\nL 220.815518 214.745074 \r\nL 220.867378 214.188961 \r\nL 220.919237 214.625444 \r\nL 220.971097 209.350913 \r\nL 221.022956 214.649522 \r\nL 221.074816 214.728393 \r\nL 221.126675 213.62833 \r\nL 221.178535 214.634604 \r\nL 221.230394 214.613425 \r\nL 221.385973 213.778075 \r\nL 221.489692 213.724194 \r\nL 221.541552 214.753904 \r\nL 221.593411 214.752334 \r\nL 221.74899 214.433045 \r\nL 221.800849 213.544662 \r\nL 221.852709 214.311515 \r\nL 221.904568 213.681653 \r\nL 221.956428 210.178631 \r\nL 222.112007 214.742046 \r\nL 222.163866 205.923754 \r\nL 222.215726 214.554385 \r\nL 222.319445 214.751393 \r\nL 222.371304 214.269106 \r\nL 222.423164 214.411109 \r\nL 222.475023 214.736922 \r\nL 222.578743 214.703662 \r\nL 222.630602 213.455815 \r\nL 222.682462 214.716688 \r\nL 222.734321 214.472413 \r\nL 222.786181 214.596305 \r\nL 222.993619 214.645108 \r\nL 223.149197 214.386342 \r\nL 223.252917 209.51572 \r\nL 223.356636 214.753579 \r\nL 223.408495 214.70332 \r\nL 223.460355 211.397285 \r\nL 223.512214 214.646684 \r\nL 223.615933 214.752928 \r\nL 223.771512 214.694623 \r\nL 223.823371 214.584058 \r\nL 223.875231 214.613717 \r\nL 223.927091 205.035553 \r\nL 223.97895 214.755648 \r\nL 224.082669 214.732842 \r\nL 224.186388 212.005651 \r\nL 224.238248 214.603988 \r\nL 224.290107 213.57783 \r\nL 224.341967 212.411549 \r\nL 224.393826 213.531011 \r\nL 224.497545 214.75371 \r\nL 224.549405 214.74094 \r\nL 224.601265 212.257478 \r\nL 224.653124 214.7402 \r\nL 224.704984 214.557997 \r\nL 224.756843 213.721713 \r\nL 224.808703 213.948626 \r\nL 224.964281 214.750558 \r\nL 225.016141 213.836875 \r\nL 225.068 214.612139 \r\nL 225.11986 204.3469 \r\nL 225.171719 208.413829 \r\nL 225.275439 214.742853 \r\nL 225.327298 206.584331 \r\nL 225.379158 214.684194 \r\nL 225.431017 214.007722 \r\nL 225.482877 214.634162 \r\nL 225.534736 214.715166 \r\nL 225.586596 212.686381 \r\nL 225.638455 214.630365 \r\nL 225.742174 214.730843 \r\nL 225.845893 214.250003 \r\nL 225.897753 214.461558 \r\nL 226.053332 214.739431 \r\nL 226.105191 214.747076 \r\nL 226.157051 214.548179 \r\nL 226.20891 214.755348 \r\nL 226.26077 212.739552 \r\nL 226.312629 214.68874 \r\nL 226.364489 214.749157 \r\nL 226.416348 212.737442 \r\nL 226.468208 214.680631 \r\nL 226.571927 214.548745 \r\nL 226.623787 213.617546 \r\nL 226.675646 214.753025 \r\nL 226.779365 214.71959 \r\nL 226.883084 212.646088 \r\nL 226.934944 214.733992 \r\nL 227.038663 214.67975 \r\nL 227.090522 214.746397 \r\nL 227.142382 213.34725 \r\nL 227.194242 214.666694 \r\nL 227.246101 214.708399 \r\nL 227.297961 212.071247 \r\nL 227.34982 214.744816 \r\nL 227.40168 213.818313 \r\nL 227.453539 214.536211 \r\nL 227.505399 214.750731 \r\nL 227.557258 213.890615 \r\nL 227.609118 214.730269 \r\nL 227.660977 214.720621 \r\nL 227.712837 214.523464 \r\nL 227.764696 214.726285 \r\nL 227.920275 214.381218 \r\nL 228.023994 214.461872 \r\nL 228.127713 214.75236 \r\nL 228.179573 214.741095 \r\nL 228.231432 214.749062 \r\nL 228.283292 214.426389 \r\nL 228.335151 214.75117 \r\nL 228.387011 214.712529 \r\nL 228.43887 214.186423 \r\nL 228.49073 214.741797 \r\nL 228.54259 206.411388 \r\nL 228.594449 214.420902 \r\nL 228.698168 214.74536 \r\nL 228.750028 214.594187 \r\nL 228.801887 214.730695 \r\nL 228.853747 212.827555 \r\nL 228.905606 214.265397 \r\nL 229.009325 214.71865 \r\nL 229.113044 213.69591 \r\nL 229.268623 214.750145 \r\nL 229.372342 214.373283 \r\nL 229.424202 214.752318 \r\nL 229.476061 195.978121 \r\nL 229.527921 214.739693 \r\nL 229.57978 214.738989 \r\nL 229.63164 195.988394 \r\nL 229.683499 214.003524 \r\nL 229.994657 214.749248 \r\nL 230.150235 214.700998 \r\nL 230.202095 214.700681 \r\nL 230.253954 214.494303 \r\nL 230.305814 214.729476 \r\nL 230.357673 214.641517 \r\nL 230.409533 214.743641 \r\nL 230.616971 214.738231 \r\nL 230.72069 212.363335 \r\nL 230.77255 214.718633 \r\nL 230.824409 214.512885 \r\nL 230.876269 214.041862 \r\nL 230.928128 214.505775 \r\nL 231.083707 214.746644 \r\nL 231.187426 214.690019 \r\nL 231.239286 214.566382 \r\nL 231.291145 214.753045 \r\nL 231.343005 213.804936 \r\nL 231.394864 214.700682 \r\nL 231.446724 214.101192 \r\nL 231.498583 214.74074 \r\nL 231.602302 214.676706 \r\nL 231.654162 212.382236 \r\nL 231.706021 214.753995 \r\nL 231.757881 214.753022 \r\nL 231.809741 208.118614 \r\nL 231.8616 214.674746 \r\nL 231.91346 213.707714 \r\nL 231.965319 214.755332 \r\nL 232.017179 214.752034 \r\nL 232.069038 212.930519 \r\nL 232.120898 214.723502 \r\nL 232.276476 213.848776 \r\nL 232.380195 214.750043 \r\nL 232.432055 214.45698 \r\nL 232.483915 213.287461 \r\nL 232.535774 205.923848 \r\nL 232.587634 213.198692 \r\nL 232.639493 214.694344 \r\nL 232.691353 214.499603 \r\nL 232.743212 213.735262 \r\nL 232.795072 204.777087 \r\nL 232.846931 214.754159 \r\nL 232.898791 214.40646 \r\nL 232.95065 202.87599 \r\nL 233.00251 214.753531 \r\nL 233.054369 214.62827 \r\nL 233.106229 214.689627 \r\nL 233.209948 214.74669 \r\nL 233.261808 214.312987 \r\nL 233.313667 212.843286 \r\nL 233.365527 214.755072 \r\nL 233.469246 214.74277 \r\nL 233.521105 214.724249 \r\nL 233.572965 214.351452 \r\nL 233.624824 214.754059 \r\nL 233.676684 214.49961 \r\nL 233.728543 214.622942 \r\nL 233.780403 208.223173 \r\nL 233.832263 208.682206 \r\nL 233.884122 214.727082 \r\nL 233.987841 214.352349 \r\nL 234.039701 212.438816 \r\nL 234.195279 214.745523 \r\nL 234.247139 190.583793 \r\nL 234.298998 195.352651 \r\nL 234.402717 214.703167 \r\nL 234.454577 214.640544 \r\nL 234.506437 214.590072 \r\nL 234.558296 214.743564 \r\nL 234.610156 212.722732 \r\nL 234.662015 214.739243 \r\nL 234.713875 214.755535 \r\nL 234.765734 214.59039 \r\nL 234.817594 214.755385 \r\nL 234.869453 212.116543 \r\nL 234.921313 214.198594 \r\nL 235.076891 214.731137 \r\nL 235.128751 214.03899 \r\nL 235.180611 214.738058 \r\nL 235.336189 214.733154 \r\nL 235.388049 214.57878 \r\nL 235.439908 211.67937 \r\nL 235.491768 213.598995 \r\nL 235.595487 214.724262 \r\nL 235.647346 214.63698 \r\nL 235.699206 214.753887 \r\nL 235.802925 214.04657 \r\nL 235.958504 214.700571 \r\nL 236.010363 214.722295 \r\nL 236.062223 214.04305 \r\nL 236.114082 214.245482 \r\nL 236.165942 214.39413 \r\nL 236.217801 210.895586 \r\nL 236.269661 213.192721 \r\nL 236.32152 214.745501 \r\nL 236.37338 211.918219 \r\nL 236.42524 214.749858 \r\nL 236.477099 213.79008 \r\nL 236.528959 210.653407 \r\nL 236.580818 214.755836 \r\nL 236.632678 212.731897 \r\nL 236.684537 214.017272 \r\nL 236.736397 206.073303 \r\nL 236.788256 214.684155 \r\nL 236.840116 214.747614 \r\nL 236.891975 214.518495 \r\nL 237.047554 212.91139 \r\nL 237.099414 205.73734 \r\nL 237.254992 214.739932 \r\nL 237.306852 214.646089 \r\nL 237.410571 214.683289 \r\nL 237.46243 213.508461 \r\nL 237.51429 209.865811 \r\nL 237.566149 214.747224 \r\nL 237.669868 214.693313 \r\nL 237.721728 214.593908 \r\nL 237.877307 214.746015 \r\nL 238.136604 214.690479 \r\nL 238.240323 214.749604 \r\nL 238.344042 214.752453 \r\nL 238.395902 201.987142 \r\nL 238.447762 214.745652 \r\nL 238.551481 214.638189 \r\nL 238.60334 214.739452 \r\nL 238.6552 211.603995 \r\nL 238.707059 214.714989 \r\nL 238.758919 214.749932 \r\nL 238.810778 209.93938 \r\nL 238.862638 211.680347 \r\nL 238.966357 214.717525 \r\nL 239.018216 213.62754 \r\nL 239.070076 214.000643 \r\nL 239.225655 214.70901 \r\nL 239.277514 214.655832 \r\nL 239.329374 214.720157 \r\nL 239.381233 214.741161 \r\nL 239.484952 214.280325 \r\nL 239.536812 214.677889 \r\nL 239.588671 213.456766 \r\nL 239.640531 214.745382 \r\nL 239.899829 214.563489 \r\nL 240.055407 214.756364 \r\nL 240.107267 214.742541 \r\nL 240.159126 214.311347 \r\nL 240.210986 209.185623 \r\nL 240.262845 214.678625 \r\nL 240.314705 210.463222 \r\nL 240.366564 214.75149 \r\nL 240.574003 214.647281 \r\nL 240.625862 214.395042 \r\nL 240.677722 214.746357 \r\nL 240.729581 212.071871 \r\nL 240.781441 214.626196 \r\nL 240.8333 213.602715 \r\nL 240.88516 214.514089 \r\nL 240.937019 214.649981 \r\nL 240.988879 208.871669 \r\nL 241.040739 214.725747 \r\nL 241.092598 214.526947 \r\nL 241.144458 212.03519 \r\nL 241.196317 214.633979 \r\nL 241.248177 213.721165 \r\nL 241.300036 214.715584 \r\nL 241.403755 214.751243 \r\nL 241.455615 213.778283 \r\nL 241.507474 214.727225 \r\nL 241.559334 214.717917 \r\nL 241.663053 214.748651 \r\nL 241.714913 214.071758 \r\nL 241.766772 206.821876 \r\nL 241.818632 214.756041 \r\nL 241.870491 214.634738 \r\nL 241.922351 214.749402 \r\nL 241.97421 214.737308 \r\nL 242.02607 213.800338 \r\nL 242.077929 214.505101 \r\nL 242.181648 214.743766 \r\nL 242.233508 212.520387 \r\nL 242.285367 214.746102 \r\nL 242.337227 214.617192 \r\nL 242.492806 213.240464 \r\nL 242.544665 214.747194 \r\nL 242.648384 214.722029 \r\nL 242.700244 214.72901 \r\nL 242.855822 214.3732 \r\nL 243.011401 214.741282 \r\nL 243.11512 214.690036 \r\nL 243.218839 214.754607 \r\nL 243.270699 206.087355 \r\nL 243.322558 214.283951 \r\nL 243.374418 213.541429 \r\nL 243.529996 214.741921 \r\nL 243.581856 214.724911 \r\nL 243.633715 214.064513 \r\nL 243.685575 214.747988 \r\nL 243.944873 214.755089 \r\nL 243.996732 211.46716 \r\nL 244.048592 214.754129 \r\nL 244.152311 214.416947 \r\nL 244.20417 213.477909 \r\nL 244.25603 214.725274 \r\nL 244.359749 214.6156 \r\nL 244.411609 214.662997 \r\nL 244.463468 213.645498 \r\nL 244.515328 205.570688 \r\nL 244.567187 214.755934 \r\nL 244.619047 205.853579 \r\nL 244.722766 214.70753 \r\nL 244.774625 214.675542 \r\nL 244.982064 214.744702 \r\nL 245.189502 214.484369 \r\nL 245.241361 214.743079 \r\nL 245.293221 214.689427 \r\nL 245.448799 211.594293 \r\nL 245.656238 214.756152 \r\nL 245.811816 214.708835 \r\nL 245.863676 154.808995 \r\nL 245.915535 214.222716 \r\nL 245.967395 214.755781 \r\nL 246.019254 204.268016 \r\nL 246.071114 214.694958 \r\nL 246.226692 214.755575 \r\nL 246.330412 214.752439 \r\nL 246.382271 203.35901 \r\nL 246.434131 214.709308 \r\nL 246.48599 214.748335 \r\nL 246.53785 208.333874 \r\nL 246.589709 213.918063 \r\nL 246.641569 214.748801 \r\nL 246.693428 210.232634 \r\nL 246.745288 214.75263 \r\nL 246.849007 214.566381 \r\nL 246.900866 213.351441 \r\nL 246.952726 214.542186 \r\nL 247.004586 214.752976 \r\nL 247.056445 214.658477 \r\nL 247.108305 214.706719 \r\nL 247.160164 213.350759 \r\nL 247.212024 214.388165 \r\nL 247.315743 214.755469 \r\nL 247.367602 214.665561 \r\nL 247.419462 214.737006 \r\nL 247.471321 181.904788 \r\nL 247.523181 204.280264 \r\nL 247.6269 214.738264 \r\nL 247.67876 214.736016 \r\nL 247.730619 205.096183 \r\nL 247.782479 213.315836 \r\nL 247.834338 213.023261 \r\nL 247.989917 214.754164 \r\nL 248.093636 214.684641 \r\nL 248.197355 212.202094 \r\nL 248.301074 214.725038 \r\nL 248.352934 214.514661 \r\nL 248.404793 214.518446 \r\nL 248.456653 214.736121 \r\nL 248.508512 214.460511 \r\nL 248.560372 214.677827 \r\nL 248.612231 214.683308 \r\nL 248.71595 214.749355 \r\nL 248.76781 212.227001 \r\nL 248.923388 214.746244 \r\nL 249.234546 214.751373 \r\nL 249.338265 214.556369 \r\nL 249.441984 209.370469 \r\nL 249.597563 214.709592 \r\nL 249.753141 214.525185 \r\nL 249.805001 214.754423 \r\nL 249.85686 214.435214 \r\nL 249.90872 214.752311 \r\nL 250.012439 214.719108 \r\nL 250.064298 213.705046 \r\nL 250.116158 214.657638 \r\nL 250.168017 214.739394 \r\nL 250.219877 214.586918 \r\nL 250.271737 210.600138 \r\nL 250.323596 214.594602 \r\nL 250.375456 214.717362 \r\nL 250.427315 213.356961 \r\nL 250.479175 214.748237 \r\nL 250.531034 214.746625 \r\nL 250.634753 210.990599 \r\nL 250.686613 212.265137 \r\nL 250.738472 201.98171 \r\nL 250.790332 212.639372 \r\nL 250.842191 214.754933 \r\nL 250.945911 214.75026 \r\nL 250.99777 214.568628 \r\nL 251.04963 205.298277 \r\nL 251.101489 214.724829 \r\nL 251.153349 214.626332 \r\nL 251.205208 214.744283 \r\nL 251.257068 214.754764 \r\nL 251.308927 213.505882 \r\nL 251.360787 214.636311 \r\nL 251.412646 195.230566 \r\nL 251.464506 214.701468 \r\nL 251.516365 214.752576 \r\nL 251.568225 214.499188 \r\nL 251.671944 186.133247 \r\nL 251.723804 214.752832 \r\nL 251.827523 213.286776 \r\nL 251.879382 214.711873 \r\nL 251.931242 209.248141 \r\nL 251.983101 214.755706 \r\nL 252.034961 214.40453 \r\nL 252.08682 214.743738 \r\nL 252.190539 214.734515 \r\nL 252.242399 214.371816 \r\nL 252.294259 213.070807 \r\nL 252.346118 214.70149 \r\nL 252.397978 212.460017 \r\nL 252.449837 214.743551 \r\nL 252.553556 214.130636 \r\nL 252.605416 214.691091 \r\nL 252.657275 213.492687 \r\nL 252.709135 214.717651 \r\nL 252.760994 213.534477 \r\nL 252.916573 214.754029 \r\nL 253.020292 214.751607 \r\nL 253.072152 204.519507 \r\nL 253.124011 212.633095 \r\nL 253.22773 214.713166 \r\nL 253.27959 214.061142 \r\nL 253.331449 213.44874 \r\nL 253.383309 214.75025 \r\nL 253.435168 209.000074 \r\nL 253.487028 214.525187 \r\nL 253.538887 214.682715 \r\nL 253.590747 214.629441 \r\nL 253.694466 214.685941 \r\nL 253.746326 208.65754 \r\nL 253.901904 214.746175 \r\nL 253.953764 214.752925 \r\nL 254.005623 201.781687 \r\nL 254.057483 214.60125 \r\nL 254.213062 214.744339 \r\nL 254.264921 214.170548 \r\nL 254.316781 214.327717 \r\nL 254.36864 214.745716 \r\nL 254.524219 213.484622 \r\nL 254.576078 214.548918 \r\nL 254.627938 201.518016 \r\nL 254.679797 204.844604 \r\nL 254.731657 214.748591 \r\nL 254.835376 214.503044 \r\nL 254.990955 214.740075 \r\nL 255.042814 214.344651 \r\nL 255.094674 214.674218 \r\nL 255.45769 214.746266 \r\nL 255.50955 214.579738 \r\nL 255.56141 214.610794 \r\nL 255.665129 214.754598 \r\nL 255.716988 213.821979 \r\nL 255.768848 214.498094 \r\nL 255.820707 214.75168 \r\nL 255.872567 213.967998 \r\nL 255.924426 214.457746 \r\nL 255.976286 214.526027 \r\nL 256.028145 214.48636 \r\nL 256.080005 214.10497 \r\nL 256.183724 214.746776 \r\nL 256.235584 214.611443 \r\nL 256.287443 178.812645 \r\nL 256.339303 193.017812 \r\nL 256.391162 214.438334 \r\nL 256.494881 214.346398 \r\nL 256.546741 212.488166 \r\nL 256.702319 214.754016 \r\nL 256.754179 214.68406 \r\nL 256.806038 211.575892 \r\nL 256.857898 214.739015 \r\nL 256.909758 214.755192 \r\nL 256.961617 202.140927 \r\nL 257.013477 214.437435 \r\nL 257.065336 212.742747 \r\nL 257.117196 214.671841 \r\nL 257.169055 214.729882 \r\nL 257.220915 197.733231 \r\nL 257.272774 214.408283 \r\nL 257.324634 214.683935 \r\nL 257.428353 212.576106 \r\nL 257.583932 214.720502 \r\nL 257.687651 214.276561 \r\nL 257.79137 214.687377 \r\nL 257.843229 214.373336 \r\nL 257.895089 214.72697 \r\nL 258.206246 214.753678 \r\nL 258.309965 214.658792 \r\nL 258.361825 214.754912 \r\nL 258.413684 214.204919 \r\nL 258.465544 214.747104 \r\nL 258.569263 214.721337 \r\nL 258.672982 209.609112 \r\nL 258.828561 214.755301 \r\nL 258.984139 214.582146 \r\nL 259.035999 214.514294 \r\nL 259.191577 214.748455 \r\nL 259.243437 214.753236 \r\nL 259.295296 214.30514 \r\nL 259.347156 214.750585 \r\nL 259.450875 214.714913 \r\nL 259.502735 214.355551 \r\nL 259.554594 214.753157 \r\nL 259.658313 214.680479 \r\nL 259.710173 214.533633 \r\nL 259.762032 214.751293 \r\nL 259.865751 214.710044 \r\nL 260.02133 214.694321 \r\nL 260.073189 214.30562 \r\nL 260.125049 214.443339 \r\nL 260.176909 214.644831 \r\nL 260.228768 214.600693 \r\nL 260.280628 213.373558 \r\nL 260.332487 214.751691 \r\nL 260.488066 214.669955 \r\nL 260.539925 187.958921 \r\nL 260.591785 212.661571 \r\nL 260.643644 212.134534 \r\nL 260.695504 214.749666 \r\nL 260.799223 214.7027 \r\nL 260.851083 201.644955 \r\nL 260.902942 214.643569 \r\nL 260.954802 214.747241 \r\nL 261.006661 212.0647 \r\nL 261.058521 213.95959 \r\nL 261.11038 213.403532 \r\nL 261.16224 214.506451 \r\nL 261.265959 214.720146 \r\nL 261.369678 192.910621 \r\nL 261.525257 214.752879 \r\nL 261.577116 214.732212 \r\nL 261.628976 202.571354 \r\nL 261.680835 214.639145 \r\nL 261.784554 214.729709 \r\nL 261.836414 197.53247 \r\nL 261.888273 214.753552 \r\nL 261.940133 214.452964 \r\nL 261.991992 213.014785 \r\nL 262.147571 214.755038 \r\nL 262.199431 214.740686 \r\nL 262.25129 214.515705 \r\nL 262.30315 214.597804 \r\nL 262.355009 211.369203 \r\nL 262.406869 214.662347 \r\nL 262.458728 214.282556 \r\nL 262.510588 214.737889 \r\nL 262.562447 213.880589 \r\nL 262.614307 214.703594 \r\nL 262.666166 214.717344 \r\nL 262.718026 211.311117 \r\nL 262.769885 214.602137 \r\nL 262.925464 214.75537 \r\nL 263.081043 214.697125 \r\nL 263.132902 214.624059 \r\nL 263.236621 214.741513 \r\nL 263.288481 213.919695 \r\nL 263.34034 214.74631 \r\nL 263.3922 214.740725 \r\nL 263.44406 203.359679 \r\nL 263.495919 213.574382 \r\nL 263.651498 214.748856 \r\nL 263.703357 211.287262 \r\nL 263.755217 214.755584 \r\nL 263.807076 214.743999 \r\nL 263.910795 212.938147 \r\nL 264.066374 214.751449 \r\nL 264.221953 214.747864 \r\nL 264.273812 206.111929 \r\nL 264.325672 214.553029 \r\nL 264.377531 214.550615 \r\nL 264.48125 214.675708 \r\nL 264.53311 212.166998 \r\nL 264.584969 214.564383 \r\nL 264.688688 214.755526 \r\nL 264.740548 214.698404 \r\nL 264.792408 214.232113 \r\nL 264.844267 214.612622 \r\nL 264.896127 214.485604 \r\nL 264.947986 214.555353 \r\nL 264.999846 214.67631 \r\nL 265.051705 214.334213 \r\nL 265.103565 214.749023 \r\nL 265.155424 211.953793 \r\nL 265.207284 214.755315 \r\nL 265.259143 214.689889 \r\nL 265.311003 204.771316 \r\nL 265.362862 208.715362 \r\nL 265.414722 214.750514 \r\nL 265.518441 214.707724 \r\nL 265.570301 213.174712 \r\nL 265.62216 214.746632 \r\nL 265.933317 214.734042 \r\nL 266.088896 214.750499 \r\nL 266.244475 213.034767 \r\nL 266.400053 214.740729 \r\nL 266.451913 214.750686 \r\nL 266.503772 214.617968 \r\nL 266.555632 214.73975 \r\nL 266.607491 203.268897 \r\nL 266.659351 212.744999 \r\nL 266.76307 214.755105 \r\nL 266.81493 214.646416 \r\nL 266.866789 195.226571 \r\nL 266.918649 214.752712 \r\nL 266.970508 210.843939 \r\nL 267.022368 214.069416 \r\nL 267.074227 213.535786 \r\nL 267.126087 214.11163 \r\nL 267.177946 214.735377 \r\nL 267.281665 214.664537 \r\nL 267.333525 214.351652 \r\nL 267.385384 214.692793 \r\nL 267.437244 214.390274 \r\nL 267.696542 214.753237 \r\nL 267.85212 204.713163 \r\nL 267.955839 214.727332 \r\nL 268.007699 212.967246 \r\nL 268.059559 213.801339 \r\nL 268.111418 214.755524 \r\nL 268.163278 214.744276 \r\nL 268.215137 213.400079 \r\nL 268.266997 214.702526 \r\nL 268.370716 214.722258 \r\nL 268.422575 213.986298 \r\nL 268.474435 214.271508 \r\nL 268.578154 214.742529 \r\nL 268.630013 212.667318 \r\nL 268.681873 214.727401 \r\nL 268.733733 214.49436 \r\nL 268.785592 214.739745 \r\nL 268.99303 214.682033 \r\nL 269.04489 214.441773 \r\nL 269.148609 214.73504 \r\nL 269.200468 214.221509 \r\nL 269.252328 214.673671 \r\nL 269.304187 206.590246 \r\nL 269.356047 214.725239 \r\nL 269.407907 214.662142 \r\nL 269.459766 211.204419 \r\nL 269.511626 214.637509 \r\nL 269.563485 212.135273 \r\nL 269.615345 214.287719 \r\nL 269.770923 214.754365 \r\nL 269.822783 213.31173 \r\nL 269.874642 214.754016 \r\nL 269.978361 214.746366 \r\nL 270.030221 214.299967 \r\nL 270.082081 214.728888 \r\nL 270.13394 186.090483 \r\nL 270.1858 214.209359 \r\nL 270.237659 214.626285 \r\nL 270.289519 210.642878 \r\nL 270.341378 214.705819 \r\nL 270.548816 214.755045 \r\nL 270.600676 214.673072 \r\nL 270.652535 182.667608 \r\nL 270.704395 214.748693 \r\nL 270.859974 214.725799 \r\nL 270.911833 214.372446 \r\nL 270.963693 214.755892 \r\nL 271.015552 214.754497 \r\nL 271.067412 214.45868 \r\nL 271.119271 209.731222 \r\nL 271.171131 214.607752 \r\nL 271.27485 214.724531 \r\nL 271.326709 209.468675 \r\nL 271.378569 214.599687 \r\nL 271.430429 206.889128 \r\nL 271.482288 214.737493 \r\nL 271.586007 214.59825 \r\nL 271.637867 214.748818 \r\nL 271.689726 188.505008 \r\nL 271.741586 214.332263 \r\nL 271.793445 214.754923 \r\nL 271.897164 214.734087 \r\nL 272.052743 208.471051 \r\nL 272.156462 214.71815 \r\nL 272.208322 214.389365 \r\nL 272.260181 214.612847 \r\nL 272.312041 213.956968 \r\nL 272.41576 214.754494 \r\nL 272.467619 209.914231 \r\nL 272.519479 180.170679 \r\nL 272.623198 214.754173 \r\nL 272.675058 186.970138 \r\nL 272.726917 214.577828 \r\nL 272.778777 214.4852 \r\nL 272.830636 208.62863 \r\nL 272.986215 139.020473 \r\nL 273.141793 214.715996 \r\nL 273.193653 203.084704 \r\nL 273.245512 205.573065 \r\nL 273.297372 214.685407 \r\nL 273.401091 214.01455 \r\nL 273.50481 214.75605 \r\nL 273.55667 214.695446 \r\nL 273.608529 213.837675 \r\nL 273.660389 214.74406 \r\nL 273.712248 214.683667 \r\nL 273.764108 214.290406 \r\nL 273.815967 214.745568 \r\nL 273.971546 214.417871 \r\nL 274.023406 214.754759 \r\nL 274.075265 214.524316 \r\nL 274.127125 214.275711 \r\nL 274.282703 214.755245 \r\nL 274.334563 212.386173 \r\nL 274.386422 212.78154 \r\nL 274.438282 214.390569 \r\nL 274.490141 212.918947 \r\nL 274.542001 210.100236 \r\nL 274.64572 214.753722 \r\nL 274.69758 212.144312 \r\nL 274.749439 214.747842 \r\nL 274.801299 214.744772 \r\nL 274.853158 212.777201 \r\nL 274.905018 214.66401 \r\nL 274.956877 214.754276 \r\nL 275.008737 212.280298 \r\nL 275.060596 213.849047 \r\nL 275.216175 214.720172 \r\nL 275.268034 214.718232 \r\nL 275.319894 214.297457 \r\nL 275.371754 214.684582 \r\nL 275.475473 214.745961 \r\nL 275.579192 214.387934 \r\nL 275.73477 214.629913 \r\nL 275.78663 214.725796 \r\nL 275.838489 214.35044 \r\nL 275.890349 214.503358 \r\nL 275.942208 214.749607 \r\nL 276.045928 208.950698 \r\nL 276.201506 214.735142 \r\nL 276.253366 212.149698 \r\nL 276.305225 214.755151 \r\nL 276.357085 214.656178 \r\nL 276.512663 204.406729 \r\nL 276.564523 214.746955 \r\nL 276.616382 214.736248 \r\nL 276.668242 212.096265 \r\nL 276.720102 213.140451 \r\nL 276.771961 214.671161 \r\nL 276.87568 210.725131 \r\nL 277.031259 214.673652 \r\nL 277.083118 212.435625 \r\nL 277.134978 214.332119 \r\nL 277.238697 214.649924 \r\nL 277.290557 212.911389 \r\nL 277.342416 213.461821 \r\nL 277.497995 214.753359 \r\nL 277.601714 202.664964 \r\nL 277.705433 214.755607 \r\nL 277.757292 210.22767 \r\nL 277.809152 214.704218 \r\nL 277.964731 213.144258 \r\nL 278.01659 214.754148 \r\nL 278.120309 214.744873 \r\nL 278.172169 210.966824 \r\nL 278.224028 214.749561 \r\nL 278.275888 214.721513 \r\nL 278.327747 214.497429 \r\nL 278.379607 214.750013 \r\nL 278.431466 214.730268 \r\nL 278.535185 214.406012 \r\nL 278.587045 214.74243 \r\nL 278.638905 213.977233 \r\nL 278.690764 214.701466 \r\nL 278.794483 214.720907 \r\nL 278.846343 214.661978 \r\nL 278.898202 214.755124 \r\nL 279.10564 214.696066 \r\nL 279.1575 214.461311 \r\nL 279.209359 214.677563 \r\nL 279.313079 214.393103 \r\nL 279.468657 214.748955 \r\nL 279.572376 214.682138 \r\nL 279.624236 214.594186 \r\nL 279.676095 214.629469 \r\nL 279.727955 214.658485 \r\nL 279.831674 214.710607 \r\nL 279.883533 210.51337 \r\nL 279.935393 187.49711 \r\nL 280.090972 214.75499 \r\nL 280.29841 214.648989 \r\nL 280.350269 214.378416 \r\nL 280.402129 199.369707 \r\nL 280.453988 213.661271 \r\nL 280.505848 214.748158 \r\nL 280.609567 214.676333 \r\nL 280.661427 214.735194 \r\nL 280.713286 205.620716 \r\nL 280.765146 214.582749 \r\nL 280.817005 214.750788 \r\nL 280.868865 214.44086 \r\nL 280.920724 214.71958 \r\nL 281.024443 214.738491 \r\nL 281.128162 212.836609 \r\nL 281.231881 213.367504 \r\nL 281.283741 189.192304 \r\nL 281.43932 214.756237 \r\nL 281.491179 211.94466 \r\nL 281.543039 214.670377 \r\nL 281.594898 214.693174 \r\nL 281.698617 214.732809 \r\nL 281.750477 207.465153 \r\nL 281.854196 214.754551 \r\nL 281.906056 214.664146 \r\nL 282.009775 214.752488 \r\nL 282.061634 210.273578 \r\nL 282.113494 214.598659 \r\nL 282.165353 214.212855 \r\nL 282.217213 191.129891 \r\nL 282.269072 214.398532 \r\nL 282.320932 214.749675 \r\nL 282.372791 201.457781 \r\nL 282.424651 214.527399 \r\nL 282.52837 214.738488 \r\nL 282.683949 214.535489 \r\nL 282.787668 214.74918 \r\nL 282.839527 212.281856 \r\nL 282.891387 214.738476 \r\nL 282.943246 214.417401 \r\nL 282.995106 214.712539 \r\nL 283.150684 214.754946 \r\nL 283.306263 214.756003 \r\nL 283.461842 214.657327 \r\nL 283.513701 187.218936 \r\nL 283.565561 214.73277 \r\nL 283.928578 214.749265 \r\nL 284.084156 214.711736 \r\nL 284.136016 209.123 \r\nL 284.187875 214.729234 \r\nL 284.239735 214.28444 \r\nL 284.343454 214.64752 \r\nL 284.395313 201.084827 \r\nL 284.550892 214.751969 \r\nL 284.602752 214.752462 \r\nL 284.654611 213.871918 \r\nL 284.706471 214.755272 \r\nL 285.017628 214.742108 \r\nL 285.276926 214.751818 \r\nL 285.328785 214.671848 \r\nL 285.380645 214.744492 \r\nL 285.795521 214.70742 \r\nL 285.84738 207.899373 \r\nL 285.89924 214.747757 \r\nL 285.9511 214.2827 \r\nL 286.002959 214.74042 \r\nL 286.054819 206.234905 \r\nL 286.106678 214.722775 \r\nL 286.417835 214.658802 \r\nL 286.521555 214.720296 \r\nL 286.573414 214.560214 \r\nL 286.625274 208.894188 \r\nL 286.677133 214.029335 \r\nL 286.728993 214.218653 \r\nL 286.780852 203.25074 \r\nL 286.832712 214.747621 \r\nL 286.884571 211.392772 \r\nL 286.936431 214.729687 \r\nL 287.092009 214.732643 \r\nL 287.143869 213.784268 \r\nL 287.195729 214.751396 \r\nL 287.299448 214.748723 \r\nL 287.351307 214.572428 \r\nL 287.403167 214.727769 \r\nL 287.455026 198.024648 \r\nL 287.506886 206.193384 \r\nL 287.610605 214.751572 \r\nL 287.662464 199.908628 \r\nL 287.714324 214.750161 \r\nL 287.766183 214.633852 \r\nL 287.869903 214.730059 \r\nL 287.921762 211.562913 \r\nL 287.973622 214.716025 \r\nL 288.025481 214.626445 \r\nL 288.077341 204.675078 \r\nL 288.1292 214.658887 \r\nL 288.18106 205.667925 \r\nL 288.336638 214.734958 \r\nL 288.388498 214.698694 \r\nL 288.544077 214.755436 \r\nL 288.595936 213.618166 \r\nL 288.647796 214.752485 \r\nL 288.699655 214.076103 \r\nL 288.751515 214.274963 \r\nL 288.803374 214.634148 \r\nL 288.855234 214.446155 \r\nL 288.907093 202.00297 \r\nL 288.958953 214.720322 \r\nL 289.010812 214.291918 \r\nL 289.062672 214.721218 \r\nL 289.218251 214.675689 \r\nL 289.27011 214.752814 \r\nL 289.32197 213.75656 \r\nL 289.373829 214.754107 \r\nL 289.425689 214.704345 \r\nL 289.477548 214.324005 \r\nL 289.529408 214.755305 \r\nL 289.633127 214.749031 \r\nL 289.684986 214.590032 \r\nL 289.736846 214.63891 \r\nL 289.840565 214.750413 \r\nL 289.892425 212.472135 \r\nL 289.944284 214.652321 \r\nL 290.048003 213.6631 \r\nL 290.099863 214.746991 \r\nL 290.203582 214.74069 \r\nL 290.307301 214.658639 \r\nL 290.35916 200.512175 \r\nL 290.41102 214.755188 \r\nL 290.514739 214.687145 \r\nL 290.566599 213.789214 \r\nL 290.618458 214.748038 \r\nL 290.670318 214.636015 \r\nL 290.722177 214.674441 \r\nL 290.774037 214.689878 \r\nL 290.825896 214.571227 \r\nL 290.877756 213.737361 \r\nL 290.929615 195.949811 \r\nL 290.981475 214.746925 \r\nL 291.033334 212.029594 \r\nL 291.085194 213.599654 \r\nL 291.188913 214.74853 \r\nL 291.240773 214.734952 \r\nL 291.344492 214.679646 \r\nL 291.396351 213.980514 \r\nL 291.448211 214.565256 \r\nL 291.50007 214.696239 \r\nL 291.55193 212.314189 \r\nL 291.603789 214.740119 \r\nL 291.863087 214.692225 \r\nL 291.914947 214.755837 \r\nL 291.966806 212.70015 \r\nL 292.018666 214.548293 \r\nL 292.070525 214.715533 \r\nL 292.122385 213.578907 \r\nL 292.174244 214.728869 \r\nL 292.226104 214.587093 \r\nL 292.277963 214.756141 \r\nL 292.329823 213.930693 \r\nL 292.381682 214.72845 \r\nL 292.433542 208.774524 \r\nL 292.485402 214.349569 \r\nL 292.537261 214.755665 \r\nL 292.64098 214.722743 \r\nL 292.69284 214.16264 \r\nL 292.744699 214.755165 \r\nL 292.796559 197.750228 \r\nL 292.848418 214.684705 \r\nL 292.900278 214.483843 \r\nL 292.952137 212.024088 \r\nL 293.003997 214.28777 \r\nL 293.107716 214.754696 \r\nL 293.159576 214.705738 \r\nL 293.263295 170.273813 \r\nL 293.418873 214.744462 \r\nL 293.470733 213.950934 \r\nL 293.522592 214.753827 \r\nL 293.678171 214.605997 \r\nL 293.73003 214.754017 \r\nL 293.78189 214.606547 \r\nL 293.83375 214.679008 \r\nL 293.885609 209.87013 \r\nL 293.937469 214.738891 \r\nL 294.041188 214.738907 \r\nL 294.093047 207.358035 \r\nL 294.144907 214.74729 \r\nL 294.196766 213.800938 \r\nL 294.248626 214.752503 \r\nL 294.300485 214.680807 \r\nL 294.352345 214.739285 \r\nL 294.456064 214.733059 \r\nL 294.507924 214.528549 \r\nL 294.559783 214.744423 \r\nL 294.663502 214.712674 \r\nL 294.767221 214.472182 \r\nL 294.9228 214.747537 \r\nL 294.974659 214.675492 \r\nL 295.026519 214.745471 \r\nL 295.078378 210.835642 \r\nL 295.130238 214.16076 \r\nL 295.182098 214.645759 \r\nL 295.233957 186.333052 \r\nL 295.285817 208.681262 \r\nL 295.337676 214.745589 \r\nL 295.389536 210.23738 \r\nL 295.545114 214.750277 \r\nL 295.648833 214.742313 \r\nL 295.700693 213.855493 \r\nL 295.752553 214.533427 \r\nL 295.804412 208.309819 \r\nL 295.856272 214.747322 \r\nL 296.01185 214.751228 \r\nL 296.06371 212.642734 \r\nL 296.115569 214.680622 \r\nL 296.219288 214.750523 \r\nL 296.374867 213.468037 \r\nL 296.530446 214.726906 \r\nL 296.582305 214.419349 \r\nL 296.634165 214.654024 \r\nL 296.686024 214.732556 \r\nL 296.789743 210.59748 \r\nL 296.841603 214.695616 \r\nL 296.893462 214.443837 \r\nL 296.945322 213.833184 \r\nL 296.997181 214.745824 \r\nL 297.049041 214.730294 \r\nL 297.100901 213.564703 \r\nL 297.15276 214.690097 \r\nL 297.20462 214.744758 \r\nL 297.256479 211.326606 \r\nL 297.308339 214.754342 \r\nL 297.360198 214.168909 \r\nL 297.412058 214.349999 \r\nL 297.463917 214.754889 \r\nL 297.515777 214.504892 \r\nL 297.567636 213.848823 \r\nL 297.619496 214.741744 \r\nL 297.723215 214.719248 \r\nL 297.775075 207.217158 \r\nL 297.826934 214.593474 \r\nL 297.878794 213.702268 \r\nL 297.930653 214.327501 \r\nL 297.982513 214.165918 \r\nL 298.034372 214.653597 \r\nL 298.086232 213.767408 \r\nL 298.138091 214.537782 \r\nL 298.189951 214.447324 \r\nL 298.24181 214.751964 \r\nL 298.29367 213.605889 \r\nL 298.345529 214.731092 \r\nL 298.449249 214.728445 \r\nL 298.501108 212.794973 \r\nL 298.552968 214.734685 \r\nL 298.708546 214.41244 \r\nL 298.760406 203.459169 \r\nL 298.812265 214.734075 \r\nL 298.864125 213.785917 \r\nL 298.915984 214.664282 \r\nL 298.967844 214.722503 \r\nL 299.019703 214.34415 \r\nL 299.071563 212.988477 \r\nL 299.123423 214.444902 \r\nL 299.175282 213.008697 \r\nL 299.227142 214.73427 \r\nL 299.279001 214.711468 \r\nL 299.330861 214.26075 \r\nL 299.38272 214.650922 \r\nL 299.43458 203.922589 \r\nL 299.486439 214.715055 \r\nL 299.538299 210.498931 \r\nL 299.590158 213.559065 \r\nL 299.642018 213.061541 \r\nL 299.745737 214.755294 \r\nL 299.797597 214.554013 \r\nL 299.849456 214.726737 \r\nL 299.901316 214.433389 \r\nL 299.953175 214.748348 \r\nL 300.005035 214.583415 \r\nL 300.056894 214.738145 \r\nL 300.160613 214.713242 \r\nL 300.264332 214.748545 \r\nL 300.316192 214.414202 \r\nL 300.471771 214.744352 \r\nL 300.627349 214.736044 \r\nL 300.679209 214.367036 \r\nL 300.731068 214.750508 \r\nL 300.886647 214.69268 \r\nL 300.938506 214.528162 \r\nL 300.990366 214.648192 \r\nL 301.145945 214.725116 \r\nL 301.197804 208.523344 \r\nL 301.249664 214.713638 \r\nL 301.301523 214.738281 \r\nL 301.353383 213.625967 \r\nL 301.405242 213.805626 \r\nL 301.457102 214.729082 \r\nL 301.560821 214.660536 \r\nL 301.61268 214.736028 \r\nL 301.7164 212.982846 \r\nL 301.768259 214.736185 \r\nL 301.820119 205.586694 \r\nL 301.871978 207.125047 \r\nL 301.975697 214.74882 \r\nL 302.027557 214.684939 \r\nL 302.183135 213.242766 \r\nL 302.234995 214.736965 \r\nL 302.338714 195.795679 \r\nL 302.390574 214.549058 \r\nL 302.494293 213.006144 \r\nL 302.649871 214.752035 \r\nL 302.80545 213.516481 \r\nL 302.857309 213.911829 \r\nL 302.961028 214.756343 \r\nL 303.012888 211.813399 \r\nL 303.064748 214.472844 \r\nL 303.116607 214.678795 \r\nL 303.168467 213.229423 \r\nL 303.220326 213.956901 \r\nL 303.272186 214.457083 \r\nL 303.324045 210.720497 \r\nL 303.375905 214.52634 \r\nL 303.479624 214.753683 \r\nL 303.531483 214.508973 \r\nL 303.583343 214.647521 \r\nL 303.687062 205.192181 \r\nL 303.790781 214.753791 \r\nL 303.842641 214.503848 \r\nL 303.8945 214.75502 \r\nL 303.94636 214.206589 \r\nL 303.998219 214.75568 \r\nL 304.050079 196.355063 \r\nL 304.101938 214.496985 \r\nL 304.153798 213.099244 \r\nL 304.205657 214.736512 \r\nL 304.257517 202.162742 \r\nL 304.309377 93.608861 \r\nL 304.361236 181.993271 \r\nL 304.516815 214.7202 \r\nL 304.620534 214.753133 \r\nL 304.672393 213.909166 \r\nL 304.724253 206.045424 \r\nL 304.776112 214.753769 \r\nL 304.827972 214.674639 \r\nL 304.879831 214.291573 \r\nL 304.931691 213.056813 \r\nL 304.983551 214.302962 \r\nL 305.03541 197.950143 \r\nL 305.08727 214.704561 \r\nL 305.139129 214.731777 \r\nL 305.190989 203.21193 \r\nL 305.242848 214.751453 \r\nL 305.346567 214.744822 \r\nL 305.398427 214.460938 \r\nL 305.450286 214.651066 \r\nL 305.554005 214.754246 \r\nL 305.605865 214.514216 \r\nL 305.657725 214.755034 \r\nL 305.761444 214.730179 \r\nL 305.813303 204.184398 \r\nL 305.865163 214.730698 \r\nL 305.968882 214.658652 \r\nL 306.020741 213.959336 \r\nL 306.072601 214.744248 \r\nL 306.12446 214.195764 \r\nL 306.17632 214.74438 \r\nL 306.280039 214.740365 \r\nL 306.331899 211.073478 \r\nL 306.383758 214.711341 \r\nL 306.539337 214.754418 \r\nL 306.694915 214.734828 \r\nL 306.850494 207.988681 \r\nL 306.902353 214.755383 \r\nL 306.954213 214.672276 \r\nL 307.006073 213.203536 \r\nL 307.057932 214.419513 \r\nL 307.109792 214.745257 \r\nL 307.161651 207.031414 \r\nL 307.213511 214.748316 \r\nL 307.31723 214.530094 \r\nL 307.369089 214.704167 \r\nL 307.420949 213.774144 \r\nL 307.472808 214.752538 \r\nL 307.628387 214.755511 \r\nL 307.680247 213.036576 \r\nL 307.732106 214.755525 \r\nL 307.783966 214.59671 \r\nL 307.835825 214.649021 \r\nL 307.991404 214.755018 \r\nL 308.302561 214.748066 \r\nL 308.354421 212.59494 \r\nL 308.40628 214.707374 \r\nL 308.45814 214.719962 \r\nL 308.509999 205.695206 \r\nL 308.561859 214.750469 \r\nL 308.665578 211.358904 \r\nL 308.717437 214.75468 \r\nL 308.821156 214.54424 \r\nL 308.873016 196.234718 \r\nL 308.924876 214.745683 \r\nL 309.028595 205.363054 \r\nL 309.184173 214.755607 \r\nL 309.236033 214.739168 \r\nL 309.287892 206.294496 \r\nL 309.339752 214.734952 \r\nL 309.54719 214.737001 \r\nL 309.59905 209.329758 \r\nL 309.650909 214.200344 \r\nL 309.806488 214.749613 \r\nL 309.910207 214.73517 \r\nL 310.065785 210.764938 \r\nL 310.117645 214.754712 \r\nL 310.221364 214.70474 \r\nL 310.273224 173.82984 \r\nL 310.325083 210.151732 \r\nL 310.428802 214.755635 \r\nL 310.480662 214.636823 \r\nL 310.63624 214.751955 \r\nL 310.6881 213.990709 \r\nL 310.739959 214.716445 \r\nL 310.791819 214.688617 \r\nL 310.843678 196.262378 \r\nL 310.895538 214.617595 \r\nL 310.947398 214.674829 \r\nL 311.051117 211.780154 \r\nL 311.102976 214.721608 \r\nL 311.206695 214.434912 \r\nL 311.258555 214.717372 \r\nL 311.310414 213.635559 \r\nL 311.362274 214.152544 \r\nL 311.414133 214.20066 \r\nL 311.465993 214.755692 \r\nL 311.517852 214.724081 \r\nL 311.569712 210.769956 \r\nL 311.621572 212.914513 \r\nL 311.77715 214.751283 \r\nL 311.82901 209.946239 \r\nL 311.880869 190.403581 \r\nL 311.984588 214.755124 \r\nL 312.036448 214.736563 \r\nL 312.243886 214.754522 \r\nL 312.347605 214.718079 \r\nL 312.399465 213.662719 \r\nL 312.451324 214.684715 \r\nL 312.503184 214.566155 \r\nL 312.555043 214.756241 \r\nL 312.606903 213.450215 \r\nL 312.658762 214.585084 \r\nL 312.710622 214.707124 \r\nL 312.762481 214.499217 \r\nL 312.91806 214.749521 \r\nL 313.073639 214.749529 \r\nL 313.229217 214.732863 \r\nL 313.281077 210.835415 \r\nL 313.332936 212.245718 \r\nL 313.384796 214.75611 \r\nL 313.488515 214.741814 \r\nL 313.644094 214.746454 \r\nL 313.695953 212.751635 \r\nL 313.747813 214.720639 \r\nL 313.799672 214.720924 \r\nL 313.851532 212.168463 \r\nL 313.903391 214.564487 \r\nL 314.162689 214.752769 \r\nL 314.266408 214.710444 \r\nL 314.318268 214.642402 \r\nL 314.370127 214.752708 \r\nL 314.421987 214.749063 \r\nL 314.473846 214.33443 \r\nL 314.525706 214.607185 \r\nL 314.629425 214.752299 \r\nL 314.681284 214.472733 \r\nL 314.733144 214.649694 \r\nL 314.836863 162.726598 \r\nL 314.992442 214.750682 \r\nL 315.044301 214.753565 \r\nL 315.096161 214.55928 \r\nL 315.14802 213.494008 \r\nL 315.19988 213.996357 \r\nL 315.251739 213.668971 \r\nL 315.303599 214.738566 \r\nL 315.355458 213.058393 \r\nL 315.407318 213.519599 \r\nL 315.459177 214.586012 \r\nL 315.562897 205.244594 \r\nL 315.614756 209.481227 \r\nL 315.718475 214.750233 \r\nL 315.770335 214.461527 \r\nL 315.874054 214.740295 \r\nL 315.925913 212.063418 \r\nL 315.977773 214.708407 \r\nL 316.133351 214.755942 \r\nL 316.34079 214.624667 \r\nL 316.392649 214.540606 \r\nL 316.548228 214.754115 \r\nL 316.600087 214.751997 \r\nL 316.651947 200.871641 \r\nL 316.703806 208.459531 \r\nL 316.859385 214.745305 \r\nL 316.911245 214.748554 \r\nL 316.963104 206.881359 \r\nL 317.014964 214.738617 \r\nL 317.066823 213.689264 \r\nL 317.118683 196.147048 \r\nL 317.170542 214.349341 \r\nL 317.222402 214.755639 \r\nL 317.274261 214.56019 \r\nL 317.326121 211.763298 \r\nL 317.37798 213.065324 \r\nL 317.42984 214.721182 \r\nL 317.481699 214.032202 \r\nL 317.533559 213.643906 \r\nL 317.689138 214.725073 \r\nL 317.948435 214.651298 \r\nL 318.052154 214.188179 \r\nL 318.104014 210.571839 \r\nL 318.155874 214.389834 \r\nL 318.207733 213.90606 \r\nL 318.259593 214.75395 \r\nL 318.311452 213.321591 \r\nL 318.363312 214.747878 \r\nL 318.415171 214.720984 \r\nL 318.467031 207.246437 \r\nL 318.51889 214.310609 \r\nL 318.57075 214.755316 \r\nL 318.622609 214.74892 \r\nL 318.726328 214.602865 \r\nL 318.778188 211.67486 \r\nL 318.830048 214.75294 \r\nL 319.141205 214.525188 \r\nL 319.244924 214.726092 \r\nL 319.296783 212.871042 \r\nL 319.348643 214.754694 \r\nL 319.452362 214.30884 \r\nL 319.504222 181.279926 \r\nL 319.556081 209.069845 \r\nL 319.763519 214.749104 \r\nL 319.919098 214.2523 \r\nL 320.074676 214.746353 \r\nL 320.126536 186.609922 \r\nL 320.178396 214.344106 \r\nL 320.333974 214.754165 \r\nL 320.437693 211.688375 \r\nL 320.489553 213.721979 \r\nL 320.541412 210.557051 \r\nL 320.645131 214.724511 \r\nL 320.696991 213.801929 \r\nL 320.74885 205.834582 \r\nL 320.80071 208.508522 \r\nL 320.904429 214.75265 \r\nL 320.956289 214.729906 \r\nL 321.008148 214.503319 \r\nL 321.060008 204.236128 \r\nL 321.111867 213.502905 \r\nL 321.215586 214.75184 \r\nL 321.267446 211.216398 \r\nL 321.319305 214.73241 \r\nL 321.526744 214.754907 \r\nL 321.578603 208.977201 \r\nL 321.630463 214.707911 \r\nL 321.837901 214.75617 \r\nL 321.88976 214.756299 \r\nL 321.94162 214.32118 \r\nL 321.993479 214.6481 \r\nL 322.097198 214.756292 \r\nL 322.149058 214.714251 \r\nL 322.252777 214.461531 \r\nL 322.304637 214.75087 \r\nL 322.356496 213.380642 \r\nL 322.408356 214.600151 \r\nL 322.460215 192.80982 \r\nL 322.512075 214.690704 \r\nL 322.563934 214.748895 \r\nL 322.615794 214.584413 \r\nL 322.667653 214.67436 \r\nL 322.771373 214.752996 \r\nL 322.823232 187.787276 \r\nL 322.875092 214.301859 \r\nL 322.978811 214.721501 \r\nL 323.03067 201.348695 \r\nL 323.08253 214.597277 \r\nL 323.134389 212.938216 \r\nL 323.186249 214.71731 \r\nL 323.238108 214.732968 \r\nL 323.341827 214.319622 \r\nL 323.497406 214.756205 \r\nL 323.704844 214.741401 \r\nL 323.756704 214.422629 \r\nL 323.808563 214.443707 \r\nL 323.860423 214.735305 \r\nL 323.964142 214.706458 \r\nL 324.016001 214.731719 \r\nL 324.067861 213.825693 \r\nL 324.119721 188.373345 \r\nL 324.17158 214.756059 \r\nL 324.22344 209.242108 \r\nL 324.275299 214.737966 \r\nL 324.327159 213.552976 \r\nL 324.379018 214.69199 \r\nL 324.430878 214.747025 \r\nL 324.482737 214.493955 \r\nL 324.534597 214.640732 \r\nL 324.690175 214.738995 \r\nL 324.742035 214.72119 \r\nL 324.793895 214.419766 \r\nL 324.845754 214.750248 \r\nL 324.897614 214.745254 \r\nL 325.053192 214.595556 \r\nL 325.105052 212.811688 \r\nL 325.156911 214.751481 \r\nL 325.208771 214.414598 \r\nL 325.26063 214.746663 \r\nL 325.779226 214.671791 \r\nL 325.882945 205.417979 \r\nL 325.986664 214.75381 \r\nL 326.038523 214.673628 \r\nL 326.090383 214.746014 \r\nL 326.142243 213.574449 \r\nL 326.194102 214.755589 \r\nL 326.297821 214.755622 \r\nL 326.349681 200.966709 \r\nL 326.4534 201.345437 \r\nL 326.608978 214.751744 \r\nL 326.660838 213.428528 \r\nL 326.712697 213.8501 \r\nL 326.764557 214.748128 \r\nL 326.868276 214.696763 \r\nL 327.023855 214.755757 \r\nL 327.075714 210.132272 \r\nL 327.127574 214.746704 \r\nL 327.179433 214.750298 \r\nL 327.231293 211.813182 \r\nL 327.283152 214.599 \r\nL 327.335012 214.28383 \r\nL 327.386872 214.681893 \r\nL 327.438731 211.333656 \r\nL 327.490591 214.675098 \r\nL 327.646169 214.334998 \r\nL 327.749888 214.74345 \r\nL 327.801748 214.722403 \r\nL 327.905467 214.748247 \r\nL 328.061046 214.268465 \r\nL 328.112905 214.737227 \r\nL 328.216624 214.671112 \r\nL 328.320343 214.749239 \r\nL 328.372203 214.572437 \r\nL 328.424062 214.684681 \r\nL 328.475922 214.679994 \r\nL 328.527781 214.127865 \r\nL 328.579641 214.69508 \r\nL 328.6315 214.731164 \r\nL 328.73522 213.607726 \r\nL 328.890798 214.756007 \r\nL 328.942658 214.743177 \r\nL 328.994517 214.704181 \r\nL 329.046377 214.283589 \r\nL 329.098236 214.690794 \r\nL 329.150096 214.583142 \r\nL 329.201955 214.736244 \r\nL 329.253815 214.277219 \r\nL 329.305674 211.284481 \r\nL 329.357534 214.67504 \r\nL 329.409394 214.7403 \r\nL 329.461253 212.707402 \r\nL 329.513113 214.419469 \r\nL 329.564972 214.754263 \r\nL 329.616832 212.950379 \r\nL 329.668691 213.577935 \r\nL 329.82427 214.754632 \r\nL 329.876129 214.740301 \r\nL 329.927989 211.643651 \r\nL 329.979848 214.725722 \r\nL 330.031708 214.732411 \r\nL 330.187287 206.896667 \r\nL 330.342865 214.736673 \r\nL 330.550303 211.498763 \r\nL 330.654022 214.755339 \r\nL 330.705882 214.68224 \r\nL 330.757742 213.238111 \r\nL 330.809601 214.342635 \r\nL 330.91332 214.753704 \r\nL 331.068899 214.498117 \r\nL 331.120758 214.74835 \r\nL 331.172618 214.669084 \r\nL 331.224477 213.173597 \r\nL 331.276337 214.731863 \r\nL 331.380056 214.752449 \r\nL 331.431916 214.625648 \r\nL 331.483775 170.830296 \r\nL 331.535635 214.623653 \r\nL 331.639354 214.749278 \r\nL 331.743073 211.645188 \r\nL 331.846792 214.743382 \r\nL 331.898651 214.243815 \r\nL 332.002371 214.753714 \r\nL 332.05423 214.72006 \r\nL 332.157949 214.520129 \r\nL 332.209809 214.59683 \r\nL 332.261668 213.736056 \r\nL 332.417247 214.751324 \r\nL 332.469106 214.753432 \r\nL 332.520966 213.198191 \r\nL 332.572825 214.675632 \r\nL 332.676545 214.716893 \r\nL 332.728404 214.657616 \r\nL 332.780264 214.754562 \r\nL 332.883983 214.743186 \r\nL 332.935842 203.612162 \r\nL 332.987702 212.412988 \r\nL 333.039561 214.753677 \r\nL 333.091421 214.723777 \r\nL 333.14328 209.899224 \r\nL 333.19514 214.671003 \r\nL 333.246999 212.421745 \r\nL 333.350719 212.558221 \r\nL 333.402578 212.003832 \r\nL 333.558157 214.75145 \r\nL 333.610016 212.848899 \r\nL 333.661876 214.732685 \r\nL 333.713735 214.650447 \r\nL 333.765595 212.151635 \r\nL 333.817454 212.997837 \r\nL 333.869314 214.715426 \r\nL 333.973033 214.705986 \r\nL 334.024893 214.480619 \r\nL 334.076752 214.727122 \r\nL 334.128612 214.628879 \r\nL 334.180471 212.481064 \r\nL 334.232331 214.634536 \r\nL 334.28419 214.742755 \r\nL 334.33605 213.670926 \r\nL 334.387909 214.708492 \r\nL 334.491628 214.7516 \r\nL 334.543488 214.281169 \r\nL 334.595347 214.308527 \r\nL 334.647207 214.669859 \r\nL 334.802786 211.681111 \r\nL 334.854645 214.729067 \r\nL 334.958364 214.672013 \r\nL 335.062083 214.752969 \r\nL 335.113943 214.286258 \r\nL 335.165802 214.704315 \r\nL 335.217662 214.734098 \r\nL 335.269521 147.893953 \r\nL 335.321381 212.372982 \r\nL 335.373241 198.717484 \r\nL 335.4251 213.774403 \r\nL 335.47696 214.756015 \r\nL 335.580679 214.748037 \r\nL 335.632538 214.756075 \r\nL 335.684398 212.461533 \r\nL 335.736257 214.630953 \r\nL 335.788117 214.569652 \r\nL 335.839976 214.722349 \r\nL 335.891836 213.60624 \r\nL 335.943695 214.754382 \r\nL 335.995555 214.753358 \r\nL 336.047415 214.510368 \r\nL 336.099274 197.250416 \r\nL 336.151134 207.041042 \r\nL 336.306712 214.692432 \r\nL 336.358572 214.663053 \r\nL 336.410431 213.555919 \r\nL 336.462291 214.731073 \r\nL 336.669729 214.750124 \r\nL 336.721589 213.838207 \r\nL 336.773448 214.754659 \r\nL 336.877167 214.752692 \r\nL 336.929027 214.62104 \r\nL 336.980886 214.722842 \r\nL 337.084605 214.747561 \r\nL 337.240184 214.735647 \r\nL 337.395763 212.766806 \r\nL 337.499482 214.753397 \r\nL 337.551341 214.693726 \r\nL 337.603201 213.919279 \r\nL 337.65506 214.219311 \r\nL 337.70692 170.579885 \r\nL 337.758779 206.564669 \r\nL 337.810639 197.443566 \r\nL 337.966218 214.715135 \r\nL 338.069937 214.711865 \r\nL 338.121796 213.105916 \r\nL 338.173656 214.736709 \r\nL 338.225515 213.414005 \r\nL 338.277375 214.622553 \r\nL 338.329234 214.578012 \r\nL 338.381094 212.629516 \r\nL 338.432953 214.744599 \r\nL 338.484813 214.460079 \r\nL 338.536672 214.735456 \r\nL 338.744111 214.696299 \r\nL 338.79597 174.930915 \r\nL 338.84783 196.857948 \r\nL 338.951549 214.753638 \r\nL 339.003408 214.69757 \r\nL 339.107127 201.348978 \r\nL 339.158987 214.753426 \r\nL 339.210846 214.729688 \r\nL 339.262706 181.304015 \r\nL 339.314566 214.70496 \r\nL 339.522004 214.535838 \r\nL 339.573863 214.310569 \r\nL 339.677582 214.75254 \r\nL 339.729442 214.74367 \r\nL 339.781301 214.745909 \r\nL 339.833161 213.581468 \r\nL 339.88502 214.721447 \r\nL 339.98874 214.752476 \r\nL 340.040599 211.462563 \r\nL 340.092459 214.745709 \r\nL 340.144318 214.678903 \r\nL 340.144318 214.678903 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 20.5625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 355.3625 224.64 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 7.2 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p00332668e4\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 376.138683 263.63625\" width=\"376.138683pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-11-28T19:22:45.052544</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 263.63625 \r\nL 376.138683 263.63625 \r\nL 376.138683 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 364.903125 239.758125 \r\nL 364.903125 22.318125 \r\nL 30.103125 22.318125 \r\nz\r\n\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma96ceba61e\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(42.140057 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"107.499782\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(97.956032 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"169.678257\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(160.134507 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"231.856733\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(222.312983 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"294.035208\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(284.491458 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"356.213683\" xlink:href=\"#ma96ceba61e\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(343.488683 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mf7be002f29\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf7be002f29\" y=\"229.875114\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 233.674332)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf7be002f29\" y=\"183.531095\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 187.330314)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf7be002f29\" y=\"137.187077\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 140.986296)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf7be002f29\" y=\"90.843059\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 94.642278)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf7be002f29\" y=\"44.499041\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 48.29826)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pcefd6b7ade)\" d=\"M 45.321307 214.941794 \r\nL 45.632199 229.849108 \r\nL 45.943092 229.169923 \r\nL 46.253984 229.76727 \r\nL 46.564876 226.401113 \r\nL 46.875769 229.389073 \r\nL 47.186661 229.841642 \r\nL 47.497553 229.587962 \r\nL 47.808446 229.868263 \r\nL 48.430231 229.833281 \r\nL 48.741123 228.631758 \r\nL 49.362908 229.641695 \r\nL 49.6738 229.668011 \r\nL 49.984692 229.863858 \r\nL 50.295585 229.52989 \r\nL 50.606477 229.773547 \r\nL 50.91737 226.395842 \r\nL 51.228262 229.825059 \r\nL 51.539154 229.741507 \r\nL 51.850047 229.32081 \r\nL 52.160939 229.616281 \r\nL 52.471831 228.582419 \r\nL 52.782724 186.168622 \r\nL 53.093616 229.449285 \r\nL 53.404509 229.726502 \r\nL 53.715401 229.700043 \r\nL 54.026293 222.226229 \r\nL 54.337186 227.315132 \r\nL 54.648078 227.224507 \r\nL 54.95897 229.85087 \r\nL 55.269863 226.758446 \r\nL 55.580755 216.091499 \r\nL 55.891648 229.855317 \r\nL 56.513432 229.775947 \r\nL 56.824325 228.742527 \r\nL 57.135217 172.423518 \r\nL 57.446109 229.796025 \r\nL 57.757002 180.8926 \r\nL 58.067894 228.500086 \r\nL 58.378787 229.66195 \r\nL 58.689679 215.983236 \r\nL 59.000571 229.433291 \r\nL 59.311464 228.950392 \r\nL 59.622356 229.562078 \r\nL 60.244141 229.861158 \r\nL 60.555033 229.774219 \r\nL 60.865926 213.922126 \r\nL 61.48771 229.644973 \r\nL 61.798603 220.897632 \r\nL 62.109495 229.296202 \r\nL 62.420388 32.201761 \r\nL 62.73128 106.118662 \r\nL 63.042172 229.68654 \r\nL 63.353065 229.62938 \r\nL 63.663957 228.027098 \r\nL 63.974849 223.07759 \r\nL 64.285742 228.56884 \r\nL 64.596634 228.681043 \r\nL 64.907527 229.796554 \r\nL 65.218419 229.838986 \r\nL 65.840204 229.261533 \r\nL 66.151096 229.844738 \r\nL 66.461988 210.19323 \r\nL 66.772881 229.76668 \r\nL 67.083773 229.840429 \r\nL 67.394666 229.37789 \r\nL 67.705558 162.762515 \r\nL 68.01645 229.685898 \r\nL 68.327343 229.722623 \r\nL 68.638235 181.794901 \r\nL 68.949127 220.0208 \r\nL 69.26002 229.746851 \r\nL 69.570912 228.775098 \r\nL 69.881805 229.851534 \r\nL 70.192697 229.8462 \r\nL 70.503589 229.093213 \r\nL 70.814482 227.388779 \r\nL 71.125374 220.557459 \r\nL 71.436266 229.855506 \r\nL 71.747159 228.527559 \r\nL 72.058051 229.454989 \r\nL 72.368944 229.597072 \r\nL 72.679836 229.288629 \r\nL 72.990728 227.940571 \r\nL 73.301621 229.066039 \r\nL 73.612513 228.939936 \r\nL 73.923405 229.75755 \r\nL 74.234298 188.864721 \r\nL 74.54519 219.803001 \r\nL 74.856083 229.751981 \r\nL 75.166975 226.334437 \r\nL 75.477867 229.849029 \r\nL 75.78876 227.543095 \r\nL 76.099652 181.746551 \r\nL 76.410544 229.718682 \r\nL 76.721437 229.5616 \r\nL 77.343222 229.780339 \r\nL 77.654114 212.113622 \r\nL 77.965006 228.484612 \r\nL 78.275899 229.803671 \r\nL 78.586791 225.502562 \r\nL 78.897683 229.86113 \r\nL 79.208576 225.278311 \r\nL 79.519468 229.85841 \r\nL 79.830361 229.543413 \r\nL 80.141253 228.653784 \r\nL 80.452145 225.474796 \r\nL 80.763038 229.778504 \r\nL 81.07393 174.581513 \r\nL 81.384822 225.475178 \r\nL 81.695715 229.733592 \r\nL 82.006607 228.06365 \r\nL 82.3175 229.836255 \r\nL 82.628392 229.450585 \r\nL 82.939284 229.85869 \r\nL 83.250177 229.734455 \r\nL 83.561069 229.871095 \r\nL 83.871961 114.854944 \r\nL 84.182854 229.463981 \r\nL 84.493746 229.729821 \r\nL 85.115531 229.870221 \r\nL 85.426423 229.682075 \r\nL 85.737316 229.664091 \r\nL 86.048208 229.829 \r\nL 86.3591 227.649561 \r\nL 86.669993 229.851648 \r\nL 86.980885 229.716158 \r\nL 87.291778 221.473833 \r\nL 87.60267 229.714632 \r\nL 87.913562 229.709578 \r\nL 88.224455 228.983564 \r\nL 88.535347 229.846945 \r\nL 88.846239 229.847552 \r\nL 89.157132 213.360482 \r\nL 89.468024 229.862736 \r\nL 89.778917 219.483045 \r\nL 90.089809 229.570565 \r\nL 90.711594 224.723918 \r\nL 91.022486 228.349404 \r\nL 91.333379 229.738844 \r\nL 91.644271 229.635252 \r\nL 91.955163 216.056308 \r\nL 92.266056 229.431073 \r\nL 92.576948 193.944474 \r\nL 92.88784 229.040525 \r\nL 93.198733 229.733563 \r\nL 93.509625 229.799163 \r\nL 93.820518 229.332635 \r\nL 94.13141 229.681325 \r\nL 95.374979 229.837112 \r\nL 95.685872 228.819097 \r\nL 95.996764 226.965556 \r\nL 96.307657 229.690944 \r\nL 96.618549 225.24625 \r\nL 96.929441 229.805016 \r\nL 97.240334 179.7847 \r\nL 97.551226 229.865084 \r\nL 97.862118 228.977967 \r\nL 98.173011 229.808566 \r\nL 98.483903 225.564718 \r\nL 98.794796 228.492914 \r\nL 99.105688 229.786079 \r\nL 99.727473 229.868805 \r\nL 100.038365 229.750726 \r\nL 100.971042 229.722921 \r\nL 101.281935 223.117404 \r\nL 101.592827 229.499653 \r\nL 101.903719 229.572894 \r\nL 102.214612 229.365958 \r\nL 102.525504 229.851305 \r\nL 102.836396 228.452359 \r\nL 103.147289 229.866634 \r\nL 103.458181 219.412272 \r\nL 104.079966 229.793325 \r\nL 104.390858 137.715784 \r\nL 104.701751 229.68673 \r\nL 105.012643 227.709333 \r\nL 105.323535 198.824075 \r\nL 105.634428 229.612893 \r\nL 105.94532 218.336328 \r\nL 106.256213 187.099386 \r\nL 106.567105 229.822398 \r\nL 106.877997 229.383673 \r\nL 107.18889 229.837483 \r\nL 107.499782 229.212429 \r\nL 107.810674 228.958743 \r\nL 108.121567 229.773864 \r\nL 108.432459 229.594568 \r\nL 108.743352 229.841035 \r\nL 109.054244 228.934384 \r\nL 109.365136 229.720176 \r\nL 109.986921 229.548381 \r\nL 110.297813 228.644754 \r\nL 110.608706 229.510675 \r\nL 110.919598 229.855689 \r\nL 111.230491 229.807598 \r\nL 111.541383 229.238514 \r\nL 111.852275 223.792684 \r\nL 112.163168 229.516886 \r\nL 112.47406 229.713833 \r\nL 112.784952 228.906685 \r\nL 113.095845 229.805368 \r\nL 113.406737 229.474765 \r\nL 113.71763 152.556403 \r\nL 114.028522 228.860492 \r\nL 114.339414 229.32174 \r\nL 114.650307 227.298701 \r\nL 114.961199 229.179909 \r\nL 115.272091 229.735372 \r\nL 115.582984 228.490244 \r\nL 115.893876 229.812221 \r\nL 116.204769 225.499091 \r\nL 116.515661 136.506896 \r\nL 116.826553 229.692922 \r\nL 117.137446 229.833462 \r\nL 117.448338 228.817808 \r\nL 117.75923 226.016813 \r\nL 118.070123 219.164689 \r\nL 118.381015 229.641679 \r\nL 118.691908 217.967699 \r\nL 119.0028 229.096086 \r\nL 119.313692 229.4267 \r\nL 119.624585 223.758949 \r\nL 119.935477 227.734964 \r\nL 120.246369 226.897961 \r\nL 120.557262 229.599937 \r\nL 120.868154 228.830945 \r\nL 121.179047 229.826065 \r\nL 121.489939 229.758634 \r\nL 122.111724 162.011206 \r\nL 122.422616 229.579618 \r\nL 122.733509 168.756533 \r\nL 123.044401 228.240383 \r\nL 123.355293 185.377984 \r\nL 123.666186 196.871758 \r\nL 123.977078 229.736781 \r\nL 124.28797 210.621155 \r\nL 124.598863 229.556324 \r\nL 124.909755 225.905326 \r\nL 125.53154 227.707198 \r\nL 125.842432 229.662981 \r\nL 126.153325 229.787343 \r\nL 126.775109 229.091242 \r\nL 127.086002 229.777765 \r\nL 127.396894 209.985062 \r\nL 127.707787 229.659657 \r\nL 128.018679 229.820485 \r\nL 128.329571 229.741218 \r\nL 128.640464 223.034953 \r\nL 128.951356 229.522309 \r\nL 129.262248 229.45296 \r\nL 129.573141 221.831675 \r\nL 129.884033 219.10543 \r\nL 130.194926 229.246486 \r\nL 130.505818 228.170699 \r\nL 130.81671 229.777456 \r\nL 131.127603 229.722189 \r\nL 131.749387 227.378446 \r\nL 132.06028 229.32463 \r\nL 132.371172 229.821047 \r\nL 132.682065 228.764371 \r\nL 132.992957 228.827621 \r\nL 133.303849 227.574394 \r\nL 133.614742 229.790804 \r\nL 133.925634 229.716609 \r\nL 134.236526 229.185139 \r\nL 134.547419 223.570893 \r\nL 134.858311 229.783779 \r\nL 135.169204 223.611855 \r\nL 135.480096 228.292633 \r\nL 135.790988 229.741976 \r\nL 136.101881 229.136358 \r\nL 136.412773 229.809457 \r\nL 136.723665 229.469103 \r\nL 137.034558 56.229211 \r\nL 137.34545 229.769809 \r\nL 137.656343 229.581548 \r\nL 137.967235 229.563859 \r\nL 138.278127 229.120772 \r\nL 138.58902 226.825037 \r\nL 138.899912 229.71595 \r\nL 139.210804 229.778776 \r\nL 139.521697 228.823458 \r\nL 139.832589 229.840575 \r\nL 140.143482 177.799701 \r\nL 140.454374 229.695482 \r\nL 140.765266 228.964882 \r\nL 141.076159 229.589111 \r\nL 141.387051 229.783845 \r\nL 141.697943 229.473641 \r\nL 142.008836 225.340903 \r\nL 142.319728 228.779403 \r\nL 142.630621 229.818054 \r\nL 142.941513 228.238464 \r\nL 143.252405 229.763946 \r\nL 144.495975 229.862352 \r\nL 144.806867 187.509994 \r\nL 145.11776 229.021886 \r\nL 145.428652 229.412411 \r\nL 145.739544 229.321378 \r\nL 146.050437 229.847351 \r\nL 146.672221 229.730081 \r\nL 146.983114 229.27171 \r\nL 147.294006 204.553307 \r\nL 147.604899 229.831357 \r\nL 147.915791 229.730791 \r\nL 148.226683 149.613885 \r\nL 148.537576 229.68152 \r\nL 149.15936 229.60959 \r\nL 149.470253 229.839949 \r\nL 149.781145 229.743355 \r\nL 150.092038 227.798403 \r\nL 150.40293 229.807596 \r\nL 150.713822 228.82264 \r\nL 151.024715 227.28328 \r\nL 151.335607 229.759896 \r\nL 151.6465 228.929694 \r\nL 151.957392 228.686248 \r\nL 152.268284 229.793049 \r\nL 152.579177 229.414287 \r\nL 152.890069 228.188244 \r\nL 153.200961 229.844622 \r\nL 153.511854 225.894625 \r\nL 153.822746 229.088873 \r\nL 154.133639 229.718037 \r\nL 154.444531 229.56628 \r\nL 154.755423 229.803258 \r\nL 155.066316 229.502583 \r\nL 155.377208 229.712769 \r\nL 155.6881 229.703468 \r\nL 155.998993 229.261873 \r\nL 156.309885 229.677482 \r\nL 156.620778 229.787661 \r\nL 156.93167 226.447289 \r\nL 157.242562 229.633412 \r\nL 157.553455 228.262607 \r\nL 157.864347 229.693389 \r\nL 158.175239 224.419674 \r\nL 158.486132 229.850788 \r\nL 158.797024 229.296977 \r\nL 159.107917 229.770087 \r\nL 159.418809 176.844982 \r\nL 159.729701 228.440122 \r\nL 160.040594 229.647905 \r\nL 160.351486 229.844016 \r\nL 161.284163 229.851086 \r\nL 161.595056 229.610914 \r\nL 161.905948 229.621218 \r\nL 162.527733 228.600138 \r\nL 162.838625 228.950115 \r\nL 163.149517 229.705914 \r\nL 163.46041 229.769903 \r\nL 163.771302 225.53127 \r\nL 164.082195 229.77243 \r\nL 164.393087 224.492065 \r\nL 164.703979 208.125804 \r\nL 165.014872 229.719532 \r\nL 165.325764 89.370213 \r\nL 165.636656 229.537776 \r\nL 165.947549 206.175543 \r\nL 166.258441 222.657569 \r\nL 166.569334 228.496755 \r\nL 166.880226 229.027261 \r\nL 167.191118 206.818417 \r\nL 167.502011 229.865488 \r\nL 167.812903 229.699945 \r\nL 168.123795 229.856439 \r\nL 168.434688 228.298434 \r\nL 168.74558 229.456597 \r\nL 169.056473 229.839082 \r\nL 169.367365 229.733847 \r\nL 169.678257 229.869914 \r\nL 169.98915 229.661075 \r\nL 170.300042 229.792452 \r\nL 170.610934 228.056015 \r\nL 170.921827 229.611874 \r\nL 171.232719 229.594521 \r\nL 171.543612 229.821508 \r\nL 171.854504 229.870625 \r\nL 172.165396 229.400773 \r\nL 172.476289 229.676927 \r\nL 172.787181 229.098712 \r\nL 173.098073 229.862661 \r\nL 173.408966 229.836589 \r\nL 173.719858 229.290642 \r\nL 174.030751 229.829412 \r\nL 174.341643 229.672937 \r\nL 174.652535 160.935904 \r\nL 174.963428 229.550578 \r\nL 175.27432 229.760606 \r\nL 175.585212 229.19409 \r\nL 175.896105 222.989987 \r\nL 176.206997 229.275676 \r\nL 176.828782 229.829301 \r\nL 177.139674 219.348435 \r\nL 177.450567 212.80916 \r\nL 177.761459 229.864573 \r\nL 178.383244 229.648557 \r\nL 178.694136 229.218658 \r\nL 179.005029 225.721339 \r\nL 179.315921 229.763928 \r\nL 179.626813 208.349715 \r\nL 179.937706 229.363581 \r\nL 180.248598 229.728718 \r\nL 180.55949 227.729718 \r\nL 180.870383 229.658884 \r\nL 181.181275 229.608355 \r\nL 181.80306 229.769022 \r\nL 182.113952 229.862799 \r\nL 182.424845 229.701748 \r\nL 182.735737 222.231944 \r\nL 183.04663 224.109986 \r\nL 183.357522 229.788052 \r\nL 183.668414 220.338269 \r\nL 183.979307 229.593885 \r\nL 184.290199 123.568665 \r\nL 184.911984 229.735708 \r\nL 185.222876 229.827631 \r\nL 185.533769 221.638371 \r\nL 185.844661 228.850333 \r\nL 186.155553 229.725391 \r\nL 186.466446 229.530885 \r\nL 186.777338 229.812741 \r\nL 187.399123 229.799669 \r\nL 187.710015 228.956986 \r\nL 188.020908 229.830678 \r\nL 188.3318 223.001357 \r\nL 188.642692 229.779355 \r\nL 188.953585 229.844613 \r\nL 189.264477 229.779731 \r\nL 189.575369 104.712007 \r\nL 189.886262 229.586675 \r\nL 190.197154 229.683637 \r\nL 190.508047 225.651025 \r\nL 190.818939 228.417037 \r\nL 191.129831 229.603857 \r\nL 191.440724 229.279842 \r\nL 191.751616 229.835405 \r\nL 192.062508 229.82969 \r\nL 192.373401 229.660504 \r\nL 192.684293 227.015991 \r\nL 192.995186 183.458729 \r\nL 193.306078 229.867062 \r\nL 193.61697 229.500523 \r\nL 193.927863 228.344498 \r\nL 194.238755 228.565111 \r\nL 194.549647 229.824742 \r\nL 194.86054 229.642824 \r\nL 195.171432 228.557333 \r\nL 195.482325 229.22499 \r\nL 195.793217 228.596458 \r\nL 196.104109 144.884035 \r\nL 196.415002 220.117927 \r\nL 196.725894 229.797182 \r\nL 197.036786 222.076605 \r\nL 197.347679 229.872921 \r\nL 197.658571 229.074379 \r\nL 197.969464 142.685264 \r\nL 198.280356 229.711351 \r\nL 198.591248 229.796797 \r\nL 198.902141 229.458856 \r\nL 199.213033 229.826686 \r\nL 199.523925 196.422063 \r\nL 199.834818 229.733504 \r\nL 200.14571 229.84354 \r\nL 200.456603 229.41167 \r\nL 200.767495 229.859698 \r\nL 201.078387 229.383517 \r\nL 201.38928 229.845664 \r\nL 201.700172 229.660192 \r\nL 202.011064 229.840973 \r\nL 202.632849 229.646453 \r\nL 202.943742 214.497558 \r\nL 203.254634 229.018081 \r\nL 203.565526 229.852341 \r\nL 203.876419 222.612281 \r\nL 204.187311 229.85121 \r\nL 205.119988 229.836779 \r\nL 205.430881 229.860132 \r\nL 205.741773 168.219752 \r\nL 206.052665 229.447713 \r\nL 206.363558 229.709763 \r\nL 206.985342 229.857217 \r\nL 207.607127 229.455198 \r\nL 207.91802 229.85617 \r\nL 208.228912 227.711201 \r\nL 208.539804 229.859005 \r\nL 208.850697 229.775912 \r\nL 209.161589 180.84214 \r\nL 209.472481 229.848648 \r\nL 209.783374 229.766363 \r\nL 210.094266 216.353825 \r\nL 210.405159 229.867556 \r\nL 211.337836 229.867901 \r\nL 211.648728 223.573443 \r\nL 211.95962 229.401486 \r\nL 212.270513 227.403512 \r\nL 212.892298 201.839136 \r\nL 213.20319 229.80489 \r\nL 213.514082 229.509703 \r\nL 213.824975 220.117184 \r\nL 214.135867 229.723161 \r\nL 214.44676 224.624417 \r\nL 214.757652 229.561459 \r\nL 215.068544 229.813682 \r\nL 215.379437 229.8271 \r\nL 215.690329 228.299298 \r\nL 216.001221 229.768164 \r\nL 216.312114 229.699901 \r\nL 216.623006 229.864197 \r\nL 216.933899 229.753715 \r\nL 217.244791 229.799034 \r\nL 217.555683 229.030742 \r\nL 217.866576 226.174251 \r\nL 218.177468 229.822636 \r\nL 218.48836 227.720844 \r\nL 218.799253 229.867698 \r\nL 219.110145 209.834251 \r\nL 219.421038 229.870197 \r\nL 220.042822 229.732889 \r\nL 220.353715 227.123304 \r\nL 220.664607 229.754162 \r\nL 221.597284 229.859888 \r\nL 222.840854 229.756038 \r\nL 223.151746 229.50335 \r\nL 223.462638 229.697485 \r\nL 223.773531 229.021058 \r\nL 224.084423 229.701497 \r\nL 224.395316 229.824803 \r\nL 224.706208 228.626654 \r\nL 225.0171 229.867114 \r\nL 225.327993 200.505909 \r\nL 225.638885 226.627032 \r\nL 225.949777 229.809745 \r\nL 226.26067 198.115219 \r\nL 226.571562 229.767993 \r\nL 226.882455 228.541685 \r\nL 227.193347 206.364371 \r\nL 227.504239 229.450368 \r\nL 227.815132 227.440314 \r\nL 228.126024 227.537572 \r\nL 228.436916 229.869385 \r\nL 228.747809 228.840985 \r\nL 229.058701 229.86789 \r\nL 229.369594 224.473749 \r\nL 229.680486 229.373764 \r\nL 229.991378 229.868599 \r\nL 231.234948 229.855325 \r\nL 231.54584 219.980243 \r\nL 231.856733 229.799639 \r\nL 232.167625 229.730488 \r\nL 232.78941 229.874212 \r\nL 233.100302 229.211223 \r\nL 233.411194 229.77362 \r\nL 233.722087 229.083235 \r\nL 234.032979 229.871602 \r\nL 234.965656 229.812094 \r\nL 235.276549 207.145533 \r\nL 235.587441 202.052784 \r\nL 235.898333 229.819401 \r\nL 236.520118 229.799516 \r\nL 236.831011 223.864819 \r\nL 237.141903 229.569707 \r\nL 237.452795 228.860811 \r\nL 237.763688 229.751004 \r\nL 238.07458 221.42226 \r\nL 238.385472 220.218248 \r\nL 238.696365 229.836882 \r\nL 239.007257 229.697998 \r\nL 239.31815 229.821117 \r\nL 239.629042 229.738756 \r\nL 239.939934 203.282124 \r\nL 240.250827 229.860151 \r\nL 240.561719 195.663337 \r\nL 240.872611 228.642486 \r\nL 241.183504 229.848728 \r\nL 241.494396 155.8822 \r\nL 241.805289 229.831489 \r\nL 242.116181 229.539707 \r\nL 242.427073 229.849837 \r\nL 243.35975 229.831143 \r\nL 243.670643 216.875051 \r\nL 243.981535 195.087987 \r\nL 244.292428 229.830844 \r\nL 244.60332 228.417323 \r\nL 244.914212 229.809557 \r\nL 245.225105 133.672288 \r\nL 245.535997 139.886193 \r\nL 245.84689 229.860286 \r\nL 246.157782 229.79074 \r\nL 246.468674 213.098374 \r\nL 246.779567 218.207939 \r\nL 247.090459 228.664378 \r\nL 247.401351 229.608381 \r\nL 247.712244 229.867459 \r\nL 248.023136 229.868899 \r\nL 248.334029 229.295448 \r\nL 248.644921 229.718491 \r\nL 248.955813 229.861038 \r\nL 249.266706 226.219974 \r\nL 249.577598 229.74994 \r\nL 249.88849 229.872721 \r\nL 250.199383 229.849121 \r\nL 250.510275 113.351794 \r\nL 250.821168 229.810091 \r\nL 251.13206 229.848746 \r\nL 251.442952 190.976794 \r\nL 251.753845 228.440418 \r\nL 252.064737 229.847053 \r\nL 252.375629 229.719747 \r\nL 252.686522 229.841978 \r\nL 252.997414 229.838253 \r\nL 253.308307 229.496674 \r\nL 253.619199 229.584949 \r\nL 253.930091 208.430686 \r\nL 254.240984 229.869827 \r\nL 254.862768 229.696203 \r\nL 255.173661 229.817435 \r\nL 255.484553 229.714065 \r\nL 255.795446 228.695468 \r\nL 256.106338 229.79007 \r\nL 256.41723 229.424497 \r\nL 256.728123 227.368724 \r\nL 257.039015 149.167038 \r\nL 257.349907 216.506626 \r\nL 257.6608 229.85529 \r\nL 257.971692 216.946485 \r\nL 258.282585 229.87347 \r\nL 258.593477 229.216696 \r\nL 258.904369 186.365071 \r\nL 259.215262 229.836054 \r\nL 259.526154 229.844142 \r\nL 259.837046 229.712561 \r\nL 260.147939 229.856233 \r\nL 260.458831 187.685712 \r\nL 260.769724 229.180261 \r\nL 261.080616 229.851346 \r\nL 261.391508 222.70014 \r\nL 261.702401 229.86875 \r\nL 262.013293 228.745987 \r\nL 262.324185 229.82577 \r\nL 262.635078 229.715281 \r\nL 262.94597 229.864531 \r\nL 263.256863 229.634894 \r\nL 263.567755 229.82631 \r\nL 263.878647 210.077867 \r\nL 264.18954 229.648122 \r\nL 264.500432 229.842896 \r\nL 264.811324 226.64175 \r\nL 265.122217 229.87068 \r\nL 265.433109 229.689552 \r\nL 266.054894 229.735788 \r\nL 266.365786 229.849821 \r\nL 266.676679 154.912316 \r\nL 266.987571 229.795466 \r\nL 267.920248 229.848516 \r\nL 268.231141 229.638767 \r\nL 268.852925 229.867466 \r\nL 269.163818 226.686973 \r\nL 269.47471 229.870418 \r\nL 269.785602 229.61353 \r\nL 270.096495 211.420076 \r\nL 270.407387 229.541381 \r\nL 270.71828 229.855748 \r\nL 271.029172 219.384084 \r\nL 271.340064 229.871184 \r\nL 271.650957 229.853697 \r\nL 271.961849 219.771524 \r\nL 272.272741 229.861597 \r\nL 272.583634 217.213062 \r\nL 272.894526 229.773422 \r\nL 273.205419 228.743712 \r\nL 273.827203 224.627805 \r\nL 274.138096 229.710094 \r\nL 274.448988 229.58787 \r\nL 274.759881 214.353993 \r\nL 275.070773 229.730465 \r\nL 275.381665 215.165193 \r\nL 275.692558 229.55279 \r\nL 276.00345 229.831536 \r\nL 276.314342 229.85648 \r\nL 276.625235 229.519639 \r\nL 276.936127 229.8481 \r\nL 277.24702 229.765334 \r\nL 277.868804 229.860099 \r\nL 278.179697 229.857738 \r\nL 278.490589 228.574831 \r\nL 278.801481 229.496901 \r\nL 279.112374 229.863886 \r\nL 279.423266 222.978132 \r\nL 279.734159 229.871029 \r\nL 280.045051 192.244101 \r\nL 280.355943 229.87269 \r\nL 280.666836 228.04838 \r\nL 280.977728 229.717448 \r\nL 281.28862 225.357716 \r\nL 281.599513 229.855078 \r\nL 283.775759 229.783504 \r\nL 284.086652 222.273786 \r\nL 284.397544 229.768144 \r\nL 284.708437 229.679244 \r\nL 285.019329 229.71589 \r\nL 285.330221 229.868239 \r\nL 285.641114 229.494639 \r\nL 285.952006 229.868892 \r\nL 286.262898 191.557457 \r\nL 286.573791 228.777655 \r\nL 286.884683 229.856953 \r\nL 287.195576 220.796365 \r\nL 287.506468 229.813919 \r\nL 287.81736 227.218772 \r\nL 288.128253 191.590633 \r\nL 288.439145 229.805482 \r\nL 288.750037 201.370382 \r\nL 289.06093 225.92022 \r\nL 289.371822 229.866195 \r\nL 289.682715 229.720765 \r\nL 289.993607 229.87096 \r\nL 290.304499 227.107234 \r\nL 290.615392 229.422297 \r\nL 290.926284 229.862779 \r\nL 291.548069 229.872331 \r\nL 291.858961 229.594988 \r\nL 292.169854 229.829111 \r\nL 292.480746 222.914852 \r\nL 292.791638 229.837459 \r\nL 293.102531 229.64884 \r\nL 293.413423 229.842561 \r\nL 293.724315 229.874489 \r\nL 294.035208 229.600958 \r\nL 294.656993 229.467785 \r\nL 294.967885 229.867345 \r\nL 295.900562 229.747758 \r\nL 296.211454 228.968358 \r\nL 296.522347 167.222921 \r\nL 296.833239 229.835631 \r\nL 297.455024 229.672046 \r\nL 297.765916 224.513516 \r\nL 298.076809 229.721191 \r\nL 298.387701 229.720301 \r\nL 298.698593 229.835849 \r\nL 299.009486 224.091626 \r\nL 299.320378 215.965108 \r\nL 299.631271 229.786836 \r\nL 299.942163 229.596207 \r\nL 300.253055 229.832894 \r\nL 300.563948 229.650175 \r\nL 300.87484 223.88807 \r\nL 301.185732 229.854704 \r\nL 301.496625 192.057869 \r\nL 301.807517 229.371254 \r\nL 302.11841 229.86082 \r\nL 302.429302 195.698919 \r\nL 302.740194 229.836689 \r\nL 303.051087 229.421858 \r\nL 303.361979 229.812927 \r\nL 304.294656 229.844252 \r\nL 304.605549 208.997443 \r\nL 304.916441 159.002951 \r\nL 305.227333 229.84473 \r\nL 305.538226 223.467531 \r\nL 305.849118 229.839362 \r\nL 306.160011 192.095229 \r\nL 306.470903 203.774364 \r\nL 306.781795 229.850697 \r\nL 307.092688 229.806692 \r\nL 307.40358 224.945911 \r\nL 308.025365 229.578519 \r\nL 308.336257 229.807169 \r\nL 308.958042 229.871481 \r\nL 309.268934 228.995655 \r\nL 309.579827 229.766822 \r\nL 309.890719 229.857672 \r\nL 310.201611 211.320833 \r\nL 310.512504 229.791712 \r\nL 311.134289 229.860937 \r\nL 311.445181 117.522315 \r\nL 311.756073 229.78046 \r\nL 312.066966 229.845046 \r\nL 312.377858 214.754673 \r\nL 312.68875 228.108081 \r\nL 312.999643 229.842553 \r\nL 313.310535 229.702741 \r\nL 313.621428 229.85791 \r\nL 313.93232 229.848737 \r\nL 314.554105 229.542472 \r\nL 314.864997 198.482394 \r\nL 315.175889 229.871122 \r\nL 315.797674 229.650249 \r\nL 316.108567 229.791552 \r\nL 316.419459 229.717597 \r\nL 316.730351 229.467552 \r\nL 317.041244 229.809418 \r\nL 317.352136 228.752843 \r\nL 317.663028 228.279344 \r\nL 317.973921 165.89042 \r\nL 318.284813 212.84045 \r\nL 318.595706 229.862589 \r\nL 318.906598 214.310858 \r\nL 319.21749 229.867756 \r\nL 319.528383 229.642827 \r\nL 319.839275 158.988607 \r\nL 320.150167 229.821155 \r\nL 321.082845 229.867145 \r\nL 321.393737 219.93308 \r\nL 321.704629 227.65163 \r\nL 322.015522 229.867904 \r\nL 322.326414 228.573607 \r\nL 322.637306 229.871909 \r\nL 322.948199 225.190515 \r\nL 323.259091 229.823702 \r\nL 323.569984 229.740142 \r\nL 323.880876 229.867631 \r\nL 324.502661 229.790532 \r\nL 324.813553 220.723748 \r\nL 325.124445 229.856059 \r\nL 325.435338 229.861054 \r\nL 325.74623 227.838033 \r\nL 326.057123 229.871088 \r\nL 326.9898 229.554173 \r\nL 327.300692 229.860597 \r\nL 327.611584 196.151369 \r\nL 327.922477 229.710763 \r\nL 328.544262 229.867728 \r\nL 329.787831 229.863075 \r\nL 330.098723 224.537594 \r\nL 330.409616 229.868191 \r\nL 330.720508 229.737167 \r\nL 331.031401 201.597761 \r\nL 331.342293 229.798014 \r\nL 331.653185 229.858838 \r\nL 331.964078 229.256335 \r\nL 332.27497 229.870328 \r\nL 332.585862 229.839738 \r\nL 332.896755 227.01538 \r\nL 333.207647 229.86216 \r\nL 333.51854 223.911554 \r\nL 333.829432 229.752067 \r\nL 334.140324 229.787456 \r\nL 334.451217 217.514868 \r\nL 334.762109 226.889007 \r\nL 335.073001 229.837511 \r\nL 335.383894 229.52639 \r\nL 335.694786 222.652779 \r\nL 336.005679 229.828166 \r\nL 336.316571 224.844598 \r\nL 336.627463 229.713617 \r\nL 336.938356 229.856981 \r\nL 337.249248 229.848099 \r\nL 337.560141 229.474364 \r\nL 337.871033 229.856332 \r\nL 338.181925 229.773119 \r\nL 338.80371 229.856639 \r\nL 339.114602 229.850005 \r\nL 339.425495 229.297456 \r\nL 340.04728 229.869127 \r\nL 340.358172 226.52129 \r\nL 340.669064 229.870532 \r\nL 340.979957 197.254843 \r\nL 341.290849 229.867998 \r\nL 341.601741 228.572583 \r\nL 341.912634 229.830727 \r\nL 342.223526 221.408536 \r\nL 342.534419 229.760061 \r\nL 343.156203 229.836105 \r\nL 343.777988 229.858483 \r\nL 344.08888 229.775392 \r\nL 344.710665 229.855538 \r\nL 345.021558 228.343915 \r\nL 345.33245 229.808556 \r\nL 345.954235 229.759315 \r\nL 346.265127 229.846543 \r\nL 346.576019 229.532698 \r\nL 346.886912 229.854645 \r\nL 347.197804 172.024925 \r\nL 347.508697 227.775212 \r\nL 347.819589 229.866334 \r\nL 348.130481 213.107551 \r\nL 348.441374 229.860478 \r\nL 348.752266 225.009459 \r\nL 349.063158 208.204405 \r\nL 349.374051 229.796747 \r\nL 349.684943 215.355166 \r\nL 349.684943 215.355166 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 30.103125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 239.758125 \r\nL 364.903125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 364.903125 239.758125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 22.318125 \r\nL 364.903125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- Validation_loss -->\r\n    <g style=\"fill:#ffffff;\" transform=\"translate(153.285 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 1831 0 \r\nL 50 4666 \r\nL 709 4666 \r\nL 2188 738 \r\nL 3669 4666 \r\nL 4325 4666 \r\nL 2547 0 \r\nL 1831 0 \r\nz\r\n\" id=\"DejaVuSans-56\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2906 2969 \r\nL 2906 4863 \r\nL 3481 4863 \r\nL 3481 0 \r\nL 2906 0 \r\nL 2906 525 \r\nQ 2725 213 2448 61 \r\nQ 2172 -91 1784 -91 \r\nQ 1150 -91 751 415 \r\nQ 353 922 353 1747 \r\nQ 353 2572 751 3078 \r\nQ 1150 3584 1784 3584 \r\nQ 2172 3584 2448 3432 \r\nQ 2725 3281 2906 2969 \r\nz\r\nM 947 1747 \r\nQ 947 1113 1208 752 \r\nQ 1469 391 1925 391 \r\nQ 2381 391 2643 752 \r\nQ 2906 1113 2906 1747 \r\nQ 2906 2381 2643 2742 \r\nQ 2381 3103 1925 3103 \r\nQ 1469 3103 1208 2742 \r\nQ 947 2381 947 1747 \r\nz\r\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3263 -1063 \r\nL 3263 -1509 \r\nL -63 -1509 \r\nL -63 -1063 \r\nL 3263 -1063 \r\nz\r\n\" id=\"DejaVuSans-5f\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-56\"/>\r\n     <use x=\"60.658203\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"121.9375\" xlink:href=\"#DejaVuSans-6c\"/>\r\n     <use x=\"149.720703\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"177.503906\" xlink:href=\"#DejaVuSans-64\"/>\r\n     <use x=\"240.980469\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"302.259766\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"341.46875\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"369.251953\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"430.433594\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     <use x=\"493.8125\" xlink:href=\"#DejaVuSans-5f\"/>\r\n     <use x=\"543.8125\" xlink:href=\"#DejaVuSans-6c\"/>\r\n     <use x=\"571.595703\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"632.777344\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"684.876953\" xlink:href=\"#DejaVuSans-73\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pcefd6b7ade\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-11-28T19:22:45.179178</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m79ea2d2ea5\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(43.732244 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"89.729261\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(81.777699 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"127.774716\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(119.823153 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"165.82017\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(157.868608 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"203.865625\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(195.914063 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"241.91108\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(233.959517 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"279.956534\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(272.004972 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"318.001989\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(310.050426 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"356.047443\" xlink:href=\"#m79ea2d2ea5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(348.095881 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m24eb51281e\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"202.192882\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.01 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 205.992101)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-31\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"177.149273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.02 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 180.948492)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"152.105664\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.03 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 155.904883)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"127.062055\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.04 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 130.861274)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"102.018446\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.05 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 105.817665)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"76.974837\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.06 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 80.774056)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"51.931228\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.07 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 55.730447)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m24eb51281e\" y=\"26.887619\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 0.08 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 30.686838)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p94f54117b0)\" d=\"M 51.683807 110.50371 \r\nL 127.774716 95.520943 \r\nL 203.865625 196.499314 \r\nL 279.956534 214.756364 \r\nL 356.047443 186.238437 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_19\">\r\n    <path clip-path=\"url(#p94f54117b0)\" d=\"M 51.683807 104.716742 \r\nL 127.774716 168.259622 \r\nL 203.865625 174.098919 \r\nL 279.956534 17.083636 \r\nL 356.047443 111.848544 \r\n\" style=\"fill:none;stroke:#feffb3;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 43.465625 44.55625 \r\nL 245.009375 44.55625 \r\nQ 247.009375 44.55625 247.009375 42.55625 \r\nL 247.009375 14.2 \r\nQ 247.009375 12.2 245.009375 12.2 \r\nL 43.465625 12.2 \r\nQ 41.465625 12.2 41.465625 14.2 \r\nL 41.465625 42.55625 \r\nQ 41.465625 44.55625 43.465625 44.55625 \r\nz\r\n\" style=\"opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 45.465625 20.298437 \r\nL 65.465625 20.298437 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\"/>\r\n    <g id=\"text_18\">\r\n     <!-- Average Train loss per epoch -->\r\n     <g style=\"fill:#ffffff;\" transform=\"translate(73.465625 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 191 3500 \r\nL 800 3500 \r\nL 1894 563 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2284 0 \r\nL 1503 0 \r\nL 191 3500 \r\nz\r\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2906 1791 \r\nQ 2906 2416 2648 2759 \r\nQ 2391 3103 1925 3103 \r\nQ 1463 3103 1205 2759 \r\nQ 947 2416 947 1791 \r\nQ 947 1169 1205 825 \r\nQ 1463 481 1925 481 \r\nQ 2391 481 2648 825 \r\nQ 2906 1169 2906 1791 \r\nz\r\nM 3481 434 \r\nQ 3481 -459 3084 -895 \r\nQ 2688 -1331 1869 -1331 \r\nQ 1566 -1331 1297 -1286 \r\nQ 1028 -1241 775 -1147 \r\nL 775 -588 \r\nQ 1028 -725 1275 -790 \r\nQ 1522 -856 1778 -856 \r\nQ 2344 -856 2625 -561 \r\nQ 2906 -266 2906 331 \r\nL 2906 616 \r\nQ 2728 306 2450 153 \r\nQ 2172 0 1784 0 \r\nQ 1141 0 747 490 \r\nQ 353 981 353 1791 \r\nQ 353 2603 747 3093 \r\nQ 1141 3584 1784 3584 \r\nQ 2172 3584 2450 3431 \r\nQ 2728 3278 2906 2969 \r\nL 2906 3500 \r\nL 3481 3500 \r\nL 3481 434 \r\nz\r\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\r\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M -19 4666 \r\nL 3928 4666 \r\nL 3928 4134 \r\nL 2272 4134 \r\nL 2272 0 \r\nL 1638 0 \r\nL 1638 4134 \r\nL -19 4134 \r\nL -19 4666 \r\nz\r\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1159 525 \r\nL 1159 -1331 \r\nL 581 -1331 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nz\r\nM 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"62.533203\" xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"121.712891\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"183.236328\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"224.349609\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"285.628906\" xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"349.105469\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"410.628906\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"442.416016\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"488.75\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"529.863281\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"591.142578\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"618.925781\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"682.304688\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"714.091797\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"741.875\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"803.056641\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"855.15625\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"907.255859\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"939.042969\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"1002.519531\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"1064.042969\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"1105.15625\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"1136.943359\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"1198.466797\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"1261.943359\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"1323.125\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"1378.105469\" xlink:href=\"#DejaVuSans-68\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_22\">\r\n     <path d=\"M 45.465625 34.976562 \r\nL 65.465625 34.976562 \r\n\" style=\"fill:none;stroke:#feffb3;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_23\"/>\r\n    <g id=\"text_19\">\r\n     <!-- Average Validation loss per epoch -->\r\n     <g style=\"fill:#ffffff;\" transform=\"translate(73.465625 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 1831 0 \r\nL 50 4666 \r\nL 709 4666 \r\nL 2188 738 \r\nL 3669 4666 \r\nL 4325 4666 \r\nL 2547 0 \r\nL 1831 0 \r\nz\r\n\" id=\"DejaVuSans-56\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2906 2969 \r\nL 2906 4863 \r\nL 3481 4863 \r\nL 3481 0 \r\nL 2906 0 \r\nL 2906 525 \r\nQ 2725 213 2448 61 \r\nQ 2172 -91 1784 -91 \r\nQ 1150 -91 751 415 \r\nQ 353 922 353 1747 \r\nQ 353 2572 751 3078 \r\nQ 1150 3584 1784 3584 \r\nQ 2172 3584 2448 3432 \r\nQ 2725 3281 2906 2969 \r\nz\r\nM 947 1747 \r\nQ 947 1113 1208 752 \r\nQ 1469 391 1925 391 \r\nQ 2381 391 2643 752 \r\nQ 2906 1113 2906 1747 \r\nQ 2906 2381 2643 2742 \r\nQ 2381 3103 1925 3103 \r\nQ 1469 3103 1208 2742 \r\nQ 947 2381 947 1747 \r\nz\r\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"62.533203\" xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"121.712891\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"183.236328\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"224.349609\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"285.628906\" xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"349.105469\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"410.628906\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"442.416016\" xlink:href=\"#DejaVuSans-56\"/>\r\n      <use x=\"503.074219\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"564.353516\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"592.136719\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"619.919922\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"683.396484\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"744.675781\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"783.884766\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"811.667969\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"872.849609\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"936.228516\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"968.015625\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"995.798828\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"1056.980469\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"1109.080078\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"1161.179688\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"1192.966797\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"1256.443359\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"1317.966797\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"1359.080078\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"1390.867188\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"1452.390625\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"1515.867188\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"1577.048828\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"1632.029297\" xlink:href=\"#DejaVuSans-68\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p94f54117b0\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 384.828125 263.63625\" width=\"384.828125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-11-28T19:22:45.317837</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 263.63625 \r\nL 384.828125 263.63625 \r\nL 384.828125 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 42.828125 239.758125 \r\nL 377.628125 239.758125 \r\nL 377.628125 22.318125 \r\nL 42.828125 22.318125 \r\nz\r\n\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m27a1172365\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"58.046307\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(50.094744 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"96.091761\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(88.140199 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"134.137216\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(126.185653 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"172.18267\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(164.231108 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"210.228125\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(202.276563 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"248.27358\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(240.322017 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"286.319034\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(278.367472 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"324.364489\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3.5 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(316.412926 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"362.409943\" xlink:href=\"#m27a1172365\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(354.458381 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mcda34fa51f\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"233.6626\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.950 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 237.461819)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"201.325543\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.955 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 205.124761)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"168.988485\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.960 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 172.787704)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"136.651428\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.965 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 140.450647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"104.314371\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.970 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 108.11359)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"71.977314\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.975 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 75.776533)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mcda34fa51f\" y=\"39.640257\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.980 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 43.439475)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p98f652dd40)\" d=\"M 58.046307 229.874489 \r\nL 134.137216 65.261963 \r\nL 210.228125 84.547144 \r\nL 286.319034 32.201761 \r\nL 362.409943 39.778138 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 42.828125 239.758125 \r\nL 42.828125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 377.628125 239.758125 \r\nL 377.628125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 42.828125 239.758125 \r\nL 377.628125 239.758125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 42.828125 22.318125 \r\nL 377.628125 22.318125 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- Correct % per epoch of validation -->\r\n    <g style=\"fill:#ffffff;\" transform=\"translate(109.530313 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 4122 4306 \r\nL 4122 3641 \r\nQ 3803 3938 3442 4084 \r\nQ 3081 4231 2675 4231 \r\nQ 1875 4231 1450 3742 \r\nQ 1025 3253 1025 2328 \r\nQ 1025 1406 1450 917 \r\nQ 1875 428 2675 428 \r\nQ 3081 428 3442 575 \r\nQ 3803 722 4122 1019 \r\nL 4122 359 \r\nQ 3791 134 3420 21 \r\nQ 3050 -91 2638 -91 \r\nQ 1578 -91 968 557 \r\nQ 359 1206 359 2328 \r\nQ 359 3453 968 4101 \r\nQ 1578 4750 2638 4750 \r\nQ 3056 4750 3426 4639 \r\nQ 3797 4528 4122 4306 \r\nz\r\n\" id=\"DejaVuSans-43\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 4653 2053 \r\nQ 4381 2053 4226 1822 \r\nQ 4072 1591 4072 1178 \r\nQ 4072 772 4226 539 \r\nQ 4381 306 4653 306 \r\nQ 4919 306 5073 539 \r\nQ 5228 772 5228 1178 \r\nQ 5228 1588 5073 1820 \r\nQ 4919 2053 4653 2053 \r\nz\r\nM 4653 2450 \r\nQ 5147 2450 5437 2106 \r\nQ 5728 1763 5728 1178 \r\nQ 5728 594 5436 251 \r\nQ 5144 -91 4653 -91 \r\nQ 4153 -91 3862 251 \r\nQ 3572 594 3572 1178 \r\nQ 3572 1766 3864 2108 \r\nQ 4156 2450 4653 2450 \r\nz\r\nM 1428 4353 \r\nQ 1159 4353 1004 4120 \r\nQ 850 3888 850 3481 \r\nQ 850 3069 1003 2837 \r\nQ 1156 2606 1428 2606 \r\nQ 1700 2606 1854 2837 \r\nQ 2009 3069 2009 3481 \r\nQ 2009 3884 1853 4118 \r\nQ 1697 4353 1428 4353 \r\nz\r\nM 4250 4750 \r\nL 4750 4750 \r\nL 1831 -91 \r\nL 1331 -91 \r\nL 4250 4750 \r\nz\r\nM 1428 4750 \r\nQ 1922 4750 2215 4408 \r\nQ 2509 4066 2509 3481 \r\nQ 2509 2891 2217 2550 \r\nQ 1925 2209 1428 2209 \r\nQ 931 2209 642 2551 \r\nQ 353 2894 353 3481 \r\nQ 353 4063 643 4406 \r\nQ 934 4750 1428 4750 \r\nz\r\n\" id=\"DejaVuSans-25\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1159 525 \r\nL 1159 -1331 \r\nL 581 -1331 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nz\r\nM 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2375 4863 \r\nL 2375 4384 \r\nL 1825 4384 \r\nQ 1516 4384 1395 4259 \r\nQ 1275 4134 1275 3809 \r\nL 1275 3500 \r\nL 2222 3500 \r\nL 2222 3053 \r\nL 1275 3053 \r\nL 1275 0 \r\nL 697 0 \r\nL 697 3053 \r\nL 147 3053 \r\nL 147 3500 \r\nL 697 3500 \r\nL 697 3744 \r\nQ 697 4328 969 4595 \r\nQ 1241 4863 1831 4863 \r\nL 2375 4863 \r\nz\r\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 191 3500 \r\nL 800 3500 \r\nL 1894 563 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2284 0 \r\nL 1503 0 \r\nL 191 3500 \r\nz\r\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2906 2969 \r\nL 2906 4863 \r\nL 3481 4863 \r\nL 3481 0 \r\nL 2906 0 \r\nL 2906 525 \r\nQ 2725 213 2448 61 \r\nQ 2172 -91 1784 -91 \r\nQ 1150 -91 751 415 \r\nQ 353 922 353 1747 \r\nQ 353 2572 751 3078 \r\nQ 1150 3584 1784 3584 \r\nQ 2172 3584 2448 3432 \r\nQ 2725 3281 2906 2969 \r\nz\r\nM 947 1747 \r\nQ 947 1113 1208 752 \r\nQ 1469 391 1925 391 \r\nQ 2381 391 2643 752 \r\nQ 2906 1113 2906 1747 \r\nQ 2906 2381 2643 2742 \r\nQ 2381 3103 1925 3103 \r\nQ 1469 3103 1208 2742 \r\nQ 947 2381 947 1747 \r\nz\r\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-43\"/>\r\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-72\"/>\r\n     <use x=\"170.369141\" xlink:href=\"#DejaVuSans-72\"/>\r\n     <use x=\"209.232422\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"270.755859\" xlink:href=\"#DejaVuSans-63\"/>\r\n     <use x=\"325.736328\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"364.945312\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"396.732422\" xlink:href=\"#DejaVuSans-25\"/>\r\n     <use x=\"491.751953\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"523.539062\" xlink:href=\"#DejaVuSans-70\"/>\r\n     <use x=\"587.015625\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"648.539062\" xlink:href=\"#DejaVuSans-72\"/>\r\n     <use x=\"689.652344\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"721.439453\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"782.962891\" xlink:href=\"#DejaVuSans-70\"/>\r\n     <use x=\"846.439453\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"907.621094\" xlink:href=\"#DejaVuSans-63\"/>\r\n     <use x=\"962.601562\" xlink:href=\"#DejaVuSans-68\"/>\r\n     <use x=\"1025.980469\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"1057.767578\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"1118.949219\" xlink:href=\"#DejaVuSans-66\"/>\r\n     <use x=\"1154.154297\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"1185.941406\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"1245.121094\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"1306.400391\" xlink:href=\"#DejaVuSans-6c\"/>\r\n     <use x=\"1334.183594\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"1361.966797\" xlink:href=\"#DejaVuSans-64\"/>\r\n     <use x=\"1425.443359\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"1486.722656\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"1525.931641\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"1553.714844\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"1614.896484\" xlink:href=\"#DejaVuSans-6e\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p98f652dd40\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"42.828125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the metrics\n",
    "plt.plot(train_losses)\n",
    "plt.show()  \n",
    "\n",
    "plt.plot(validation_losses)\n",
    "plt.title(\"Validation_loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot([_.cpu() for _ in epoch_train_losses],label=\"Average Train loss per epoch\")\n",
    "plt.plot([_.cpu() for _ in epoch_val_losses],label=\"Average Validation loss per epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_correct_epoch_perc)\n",
    "plt.title(\"Correct % per epoch of validation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0006\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0015 | Current Mini-batch loss: 0.0027\n",
      "Testing | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0010 | Current Mini-batch loss: 0.0012\n",
      "Testing | Average Loss: 0.0005 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0003 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.2236 | Current Mini-batch loss: 0.4469\n",
      "Testing | Average Loss: 0.1118 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0931 | Current Mini-batch loss: 0.0743\n",
      "Testing | Average Loss: 0.0465 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0233 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0119 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0063 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.1517 | Current Mini-batch loss: 0.2970\n",
      "Testing | Average Loss: 0.1104 | Current Mini-batch loss: 0.0691\n",
      "Testing | Average Loss: 0.0553 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0306 | Current Mini-batch loss: 0.0059\n",
      "Testing | Average Loss: 0.0156 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0079 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.1610 | Current Mini-batch loss: 0.3140\n",
      "Testing | Average Loss: 0.0807 | Current Mini-batch loss: 0.0005\n",
      "Testing | Average Loss: 0.0405 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0255 | Current Mini-batch loss: 0.0105\n",
      "Testing | Average Loss: 0.0136 | Current Mini-batch loss: 0.0017\n",
      "Testing | Average Loss: 0.0381 | Current Mini-batch loss: 0.0625\n",
      "Testing | Average Loss: 0.0934 | Current Mini-batch loss: 0.1487\n",
      "Testing | Average Loss: 0.0494 | Current Mini-batch loss: 0.0053\n",
      "Testing | Average Loss: 0.0247 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0124 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0066 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.0033 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0026 | Current Mini-batch loss: 0.0018\n",
      "Testing | Average Loss: 0.0016 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0009 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0017 | Current Mini-batch loss: 0.0026\n",
      "Testing | Average Loss: 0.0010 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0018 | Current Mini-batch loss: 0.0026\n",
      "Testing | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0016 | Current Mini-batch loss: 0.0023\n",
      "Testing | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0005 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0003 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.1593 | Current Mini-batch loss: 0.3183\n",
      "Testing | Average Loss: 0.0798 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0400 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.1084 | Current Mini-batch loss: 0.1768\n",
      "Testing | Average Loss: 0.0543 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0274 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0137 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0069 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0050 | Current Mini-batch loss: 0.0030\n",
      "Testing | Average Loss: 0.0032 | Current Mini-batch loss: 0.0014\n",
      "Testing | Average Loss: 0.0017 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0011 | Current Mini-batch loss: 0.0006\n",
      "Testing | Average Loss: 0.0010 | Current Mini-batch loss: 0.0009\n",
      "Testing | Average Loss: 0.0006 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.2485 | Current Mini-batch loss: 0.4964\n",
      "Testing | Average Loss: 0.1243 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0626 | Current Mini-batch loss: 0.0010\n",
      "Testing | Average Loss: 0.0323 | Current Mini-batch loss: 0.0020\n",
      "Testing | Average Loss: 0.0886 | Current Mini-batch loss: 0.1450\n",
      "Testing | Average Loss: 0.7354 | Current Mini-batch loss: 1.3822\n",
      "Testing | Average Loss: 0.3678 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.2376 | Current Mini-batch loss: 0.1074\n",
      "Testing | Average Loss: 0.1188 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0595 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0448 | Current Mini-batch loss: 0.0302\n",
      "Testing | Average Loss: 0.0306 | Current Mini-batch loss: 0.0163\n",
      "Testing | Average Loss: 0.0202 | Current Mini-batch loss: 0.0099\n",
      "Testing | Average Loss: 0.0102 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0055 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.1667 | Current Mini-batch loss: 0.3279\n",
      "Testing | Average Loss: 0.0834 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0444 | Current Mini-batch loss: 0.0054\n",
      "Testing | Average Loss: 0.0222 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0114 | Current Mini-batch loss: 0.0005\n",
      "Testing | Average Loss: 0.0060 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0030 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0016 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0008 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0007 | Current Mini-batch loss: 0.0005\n",
      "Testing | Average Loss: 0.0020 | Current Mini-batch loss: 0.0034\n",
      "Testing | Average Loss: 0.0011 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0007 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0003 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0001 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0758 | Current Mini-batch loss: 0.1514\n",
      "Testing | Average Loss: 0.0380 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0254 | Current Mini-batch loss: 0.0128\n",
      "Testing | Average Loss: 0.0129 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.2479 | Current Mini-batch loss: 0.4828\n",
      "Testing | Average Loss: 0.1242 | Current Mini-batch loss: 0.0006\n",
      "Testing | Average Loss: 0.0692 | Current Mini-batch loss: 0.0142\n",
      "Testing | Average Loss: 0.0346 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0177 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0090 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.5697 | Current Mini-batch loss: 1.1304\n",
      "Testing | Average Loss: 0.2855 | Current Mini-batch loss: 0.0012\n",
      "Testing | Average Loss: 0.1519 | Current Mini-batch loss: 0.0184\n",
      "Testing | Average Loss: 0.0760 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0381 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0194 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.0097 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0052 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0027 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0024 | Current Mini-batch loss: 0.0022\n",
      "Testing | Average Loss: 0.0015 | Current Mini-batch loss: 0.0006\n",
      "Testing | Average Loss: 0.0018 | Current Mini-batch loss: 0.0021\n",
      "Testing | Average Loss: 0.0009 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0007 | Current Mini-batch loss: 0.0005\n",
      "Testing | Average Loss: 0.0005 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0062 | Current Mini-batch loss: 0.0118\n",
      "Testing | Average Loss: 0.0032 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0016 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0057 | Current Mini-batch loss: 0.0097\n",
      "Testing | Average Loss: 0.0032 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.0027 | Current Mini-batch loss: 0.0022\n",
      "Testing | Average Loss: 0.0014 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0008 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0019 | Current Mini-batch loss: 0.0034\n",
      "Testing | Average Loss: 0.0010 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0005 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0005 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.1298 | Current Mini-batch loss: 0.2593\n",
      "Testing | Average Loss: 0.0825 | Current Mini-batch loss: 0.0352\n",
      "Testing | Average Loss: 0.0413 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.3343 | Current Mini-batch loss: 0.6272\n",
      "Testing | Average Loss: 0.1673 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0837 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0425 | Current Mini-batch loss: 0.0014\n",
      "Testing | Average Loss: 0.0213 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0108 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0091 | Current Mini-batch loss: 0.0074\n",
      "Testing | Average Loss: 0.0046 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0023 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0018 | Current Mini-batch loss: 0.0013\n",
      "Testing | Average Loss: 0.0010 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.1490 | Current Mini-batch loss: 0.2971\n",
      "Testing | Average Loss: 0.0746 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0373 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0275 | Current Mini-batch loss: 0.0177\n",
      "Testing | Average Loss: 0.0138 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0069 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0266 | Current Mini-batch loss: 0.0462\n",
      "Testing | Average Loss: 0.0171 | Current Mini-batch loss: 0.0077\n",
      "Testing | Average Loss: 0.0086 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0043 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0032 | Current Mini-batch loss: 0.0021\n",
      "Testing | Average Loss: 0.0030 | Current Mini-batch loss: 0.0028\n",
      "Testing | Average Loss: 0.0030 | Current Mini-batch loss: 0.0031\n",
      "Testing | Average Loss: 0.0019 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0055 | Current Mini-batch loss: 0.0091\n",
      "Testing | Average Loss: 0.0028 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0017 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0009 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0006 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0004 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0002 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0015 | Current Mini-batch loss: 0.0028\n",
      "Testing | Average Loss: 0.0764 | Current Mini-batch loss: 0.1514\n",
      "Testing | Average Loss: 0.0391 | Current Mini-batch loss: 0.0018\n",
      "Testing | Average Loss: 0.0196 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0427 | Current Mini-batch loss: 0.0658\n",
      "Testing | Average Loss: 0.0214 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0108 | Current Mini-batch loss: 0.0002\n",
      "Testing | Average Loss: 0.0054 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.1018 | Current Mini-batch loss: 0.1981\n",
      "Testing | Average Loss: 0.0513 | Current Mini-batch loss: 0.0008\n",
      "Testing | Average Loss: 0.0259 | Current Mini-batch loss: 0.0004\n",
      "Testing | Average Loss: 0.0129 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0116 | Current Mini-batch loss: 0.0102\n",
      "Testing | Average Loss: 0.0058 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.1766 | Current Mini-batch loss: 0.3473\n",
      "Testing | Average Loss: 0.0886 | Current Mini-batch loss: 0.0007\n",
      "Testing | Average Loss: 0.0443 | Current Mini-batch loss: 0.0001\n",
      "Testing | Average Loss: 0.0363 | Current Mini-batch loss: 0.0282\n",
      "Testing | Average Loss: 0.0192 | Current Mini-batch loss: 0.0021\n",
      "Testing | Average Loss: 0.0185 | Current Mini-batch loss: 0.0177\n",
      "Testing | Average Loss: 0.0094 | Current Mini-batch loss: 0.0003\n",
      "Testing | Average Loss: 0.0047 | Current Mini-batch loss: 0.0000\n",
      "Testing | Average Loss: 0.0024 | Current Mini-batch loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "avg_loss = None\n",
    "\n",
    "correct = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in test_loader:\n",
    "    x,y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # 1. Get the prediction\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "    \n",
    "        # 2. Compute the loss\n",
    "        loss = loss_fn(pred,y)\n",
    "    \n",
    "    # 3. Get num correct\n",
    "    _correct = (torch.argmax(pred, dim=1) == y).sum()\n",
    "    correct += _correct.detach()\n",
    "    # Log metrics\n",
    "    loss_detached = loss.detach()\n",
    "    \n",
    "    test_losses.append(loss_detached.cpu())\n",
    "    \n",
    "    if avg_loss is not None:\n",
    "        avg_loss = (avg_loss + loss_detached) / 2.0\n",
    "    else:\n",
    "        avg_loss = loss_detached\n",
    "        \n",
    "    print(f\"Testing | Average Loss: {avg_loss:.4f} | Current Mini-batch loss: {loss_detached:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing | Average Loss: 0.0023826146498322487 | Correct 1544/1566 or 98.59513854980469%\n",
      "Difference in Train loss and Test loss: 0.013988050632178783\n"
     ]
    }
   ],
   "source": [
    "print(f\"Testing | Average Loss: {avg_loss} | Correct {correct}/{len(test)} or {(correct/len(test))*100}%\")\n",
    "last_average_loss = epoch_train_losses[-1].item()\n",
    "\n",
    "print(f\"Difference in Train loss and Test loss: {abs(avg_loss -last_average_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 374.232889 248.518125\" width=\"374.232889pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-11-28T19:22:50.552974</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 374.232889 248.518125 \r\nL 374.232889 0 \r\nL 0 0 \r\nz\r\n\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me3f0eac7fb\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(42.140057 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"84.342286\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 25 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(77.979786 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"123.363265\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 50 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(117.000765 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"162.384244\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 75 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(156.021744 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"201.405223\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 100 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(191.861473 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"240.426202\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 125 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(230.882452 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"279.447181\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 150 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(269.903431 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"318.46816\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 175 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(308.92441 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"357.489139\" xlink:href=\"#me3f0eac7fb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 200 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(347.945389 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m51d2a04039\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"214.757014\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 218.556232)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"186.155214\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.2 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 189.954433)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"157.553415\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.4 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 161.352634)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"128.951616\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.6 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 132.750834)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"100.349816\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.8 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 104.149035)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"71.748017\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1.0 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 75.547236)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"43.146218\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 1.2 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 46.945437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51d2a04039\" y=\"14.544418\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.4 -->\r\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 18.343637)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p2bf138ae40)\" d=\"M 45.321307 214.741611 \r\nL 54.686342 214.71129 \r\nL 56.247181 214.368824 \r\nL 57.80802 214.746341 \r\nL 59.368859 214.584829 \r\nL 60.929698 214.753911 \r\nL 64.051377 214.737301 \r\nL 65.612216 150.851295 \r\nL 67.173055 214.739904 \r\nL 68.733894 204.128854 \r\nL 70.294733 214.754846 \r\nL 74.977251 214.643734 \r\nL 76.53809 172.279332 \r\nL 78.098929 204.869568 \r\nL 79.659768 214.740599 \r\nL 81.220608 213.910277 \r\nL 82.781447 214.659893 \r\nL 84.342286 214.740408 \r\nL 85.903125 169.848211 \r\nL 87.463964 214.686874 \r\nL 89.024803 214.725964 \r\nL 90.585642 213.252989 \r\nL 92.146482 214.512361 \r\nL 93.707321 205.815032 \r\nL 95.26816 193.494118 \r\nL 96.828999 213.994254 \r\nL 98.389838 214.751642 \r\nL 103.072356 214.742039 \r\nL 104.633195 214.493421 \r\nL 107.754873 214.747756 \r\nL 109.315712 214.387981 \r\nL 110.876552 214.720945 \r\nL 112.437391 214.386705 \r\nL 113.99823 214.755064 \r\nL 115.559069 214.424181 \r\nL 117.119908 214.74949 \r\nL 120.241587 214.746613 \r\nL 121.802426 169.242559 \r\nL 123.363265 214.70474 \r\nL 124.924104 214.735694 \r\nL 126.484943 189.471174 \r\nL 128.045782 214.724347 \r\nL 132.7283 214.752841 \r\nL 134.289139 214.322087 \r\nL 137.410817 214.730772 \r\nL 140.532496 214.626529 \r\nL 142.093335 214.73295 \r\nL 143.654174 143.771005 \r\nL 145.215013 214.752839 \r\nL 148.336691 214.468559 \r\nL 149.897531 194.023037 \r\nL 151.45837 17.083636 \r\nL 153.019209 214.743068 \r\nL 154.580048 199.394338 \r\nL 156.140887 214.755341 \r\nL 157.701726 214.741142 \r\nL 159.262566 210.443326 \r\nL 160.823405 212.426044 \r\nL 162.384244 213.346627 \r\nL 163.945083 214.73741 \r\nL 165.505922 214.639488 \r\nL 167.066761 167.867655 \r\nL 168.627601 214.743066 \r\nL 170.18844 213.978635 \r\nL 171.749279 214.75012 \r\nL 174.870957 214.659707 \r\nL 177.992635 214.738028 \r\nL 181.114314 214.688043 \r\nL 182.675153 214.272567 \r\nL 184.235992 214.732287 \r\nL 196.722705 214.749047 \r\nL 198.283545 193.100591 \r\nL 199.844384 214.73239 \r\nL 201.405223 212.92376 \r\nL 202.966062 214.699385 \r\nL 204.526901 145.707649 \r\nL 206.08774 214.677015 \r\nL 207.64858 212.725 \r\nL 209.209419 214.748175 \r\nL 210.770258 214.657671 \r\nL 212.331097 214.705897 \r\nL 213.891936 53.094776 \r\nL 215.452775 214.58796 \r\nL 217.013615 212.121856 \r\nL 218.574454 214.746064 \r\nL 226.378649 214.734695 \r\nL 227.939489 214.44799 \r\nL 229.500328 214.673995 \r\nL 231.061167 214.459742 \r\nL 232.622006 214.742095 \r\nL 235.743684 214.719826 \r\nL 237.304524 213.06287 \r\nL 238.865363 214.727026 \r\nL 240.426202 214.749728 \r\nL 241.987041 213.369203 \r\nL 243.54788 214.642733 \r\nL 245.108719 214.438649 \r\nL 246.669559 214.744403 \r\nL 249.791237 214.748574 \r\nL 251.352076 214.275824 \r\nL 252.912915 214.753359 \r\nL 257.595433 214.753483 \r\nL 259.156272 177.673605 \r\nL 260.717111 209.719986 \r\nL 262.27795 214.752257 \r\nL 263.838789 125.056025 \r\nL 265.399628 214.717203 \r\nL 266.960468 214.748985 \r\nL 268.521307 214.553752 \r\nL 270.082146 214.749588 \r\nL 271.642985 214.717481 \r\nL 273.203824 213.705513 \r\nL 274.764663 214.745513 \r\nL 276.325503 214.740033 \r\nL 277.886342 214.565357 \r\nL 279.447181 214.742777 \r\nL 281.00802 172.267002 \r\nL 282.568859 214.738745 \r\nL 284.129698 214.753804 \r\nL 285.690538 212.219004 \r\nL 287.251377 214.751612 \r\nL 288.812216 214.748066 \r\nL 290.373055 208.149545 \r\nL 291.933894 213.660799 \r\nL 293.494733 214.738034 \r\nL 295.055573 214.748621 \r\nL 296.616412 214.454699 \r\nL 299.73809 214.318697 \r\nL 301.298929 214.654131 \r\nL 302.859768 213.457256 \r\nL 304.420608 214.752997 \r\nL 305.981447 214.660959 \r\nL 307.542286 214.756364 \r\nL 310.663964 214.729628 \r\nL 313.785642 214.738443 \r\nL 315.346482 214.361933 \r\nL 316.907321 193.104126 \r\nL 318.46816 214.493749 \r\nL 320.028999 214.747781 \r\nL 321.589838 205.351016 \r\nL 323.150677 214.750871 \r\nL 326.272356 214.752811 \r\nL 327.833195 186.426494 \r\nL 329.394034 214.648426 \r\nL 332.515712 214.754497 \r\nL 334.076552 213.302479 \r\nL 335.637391 214.747868 \r\nL 337.19823 165.086367 \r\nL 338.759069 214.661238 \r\nL 340.319908 214.748957 \r\nL 341.880747 210.726768 \r\nL 343.441587 214.450376 \r\nL 345.002426 212.219095 \r\nL 346.563265 214.71881 \r\nL 349.684943 214.74734 \r\nL 349.684943 214.74734 \r\n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p2bf138ae40\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"./models/model5-5_SGD_98-6_5epochs_025LR_8BS.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
